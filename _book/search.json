[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Informações Legais e Declaração de Uso de Inteligência Artificial",
    "section": "",
    "text": "Informações Legais e Declaração de Uso de Inteligência Artificial",
    "crumbs": [
      "Informações Legais e Declaração de Uso de Inteligência Artificial"
    ]
  },
  {
    "objectID": "index.html#direitos-autorais-e-uso-da-obra",
    "href": "index.html#direitos-autorais-e-uso-da-obra",
    "title": "Informações Legais e Declaração de Uso de Inteligência Artificial",
    "section": "Direitos Autorais e Uso da Obra",
    "text": "Direitos Autorais e Uso da Obra\n© 2026 — Os autores.\nTodos os direitos reservados. Esta obra é protegida pela legislação brasileira de direitos autorais (Lei nº 9.610/1998). É vedada a reprodução, distribuição, armazenamento ou transmissão total ou parcial deste livro, por qualquer meio ou processo, eletrônico ou mecânico, sem autorização expressa e por escrito dos autores, salvo nos casos permitidos pela legislação vigente para fins exclusivamente acadêmicos e não comerciais, com a devida citação da fonte.\nA utilização do material em ambientes de ensino é permitida desde que preservada a integridade do conteúdo, mencionada a autoria e respeitados os princípios de uso responsável da informação científica. A reprodução para fins comerciais, bem como a modificação substancial do conteúdo sem autorização, constitui violação de direitos autorais.",
    "crumbs": [
      "Informações Legais e Declaração de Uso de Inteligência Artificial"
    ]
  },
  {
    "objectID": "index.html#declaração-sobre-o-uso-de-ferramentas-de-inteligência-artificial",
    "href": "index.html#declaração-sobre-o-uso-de-ferramentas-de-inteligência-artificial",
    "title": "Informações Legais e Declaração de Uso de Inteligência Artificial",
    "section": "Declaração sobre o Uso de Ferramentas de Inteligência Artificial",
    "text": "Declaração sobre o Uso de Ferramentas de Inteligência Artificial\nDurante a elaboração deste livro foram utilizadas ferramentas de Inteligência Artificial, em especial sistemas de apoio à escrita e organização textual, como recurso complementar no processo de produção acadêmica.\nO uso de IA teve caráter estritamente assistivo, não substituindo o desenvolvimento conceitual, a formulação matemática, a interpretação estatística nem as decisões pedagógicas da obra. Todo conteúdo foi cuidadosamente revisado, validado e adaptado pelos autores, que assumem integral responsabilidade científica e intelectual pelo texto final.\nNão foram delegadas à IA decisões metodológicas, inferenciais ou conclusões analíticas. A elaboração teórica, a seleção de exemplos, a validação matemática e a coerência didática resultam da experiência acadêmica e da atuação dos autores na área de Modelos de Regressão.\nReafirmamos que o uso responsável de tecnologias emergentes deve sempre preservar o rigor científico, a integridade acadêmica e o protagonismo intelectual humano.",
    "crumbs": [
      "Informações Legais e Declaração de Uso de Inteligência Artificial"
    ]
  },
  {
    "objectID": "prefacio.html",
    "href": "prefacio.html",
    "title": "Prefácio",
    "section": "",
    "text": "Este livro resulta da experiência acumulada no ensino, na orientação de estudantes e no desenvolvimento de pesquisas em modelos de regressão. Somos docentes pesquisadores do Departamento de Estatística e Matemática Aplicada da Universidade Federal do Ceará (DEMA/UFC), com atuação contínua em regressão linear, modelagem estatística e inferência. Integramos o Laboratório de Inovação em Estatística — StatLab/UFC (https://statlab.quarto.pub/) e o Grupo de Pesquisa Modelos de Regressão e Aplicações do CNPq (http://dgp.cnpq.br/dgp/espelhogrupo/6344659534806942), espaços nos quais articulamos ensino, pesquisa e aplicação a problemas reais.\nO material tem como base as disciplinas Modelos de Regressão I (CC0290) e Modelagem Estatística (CC0452), momentos centrais na formação do estudante, em que probabilidade, inferência e álgebra matricial se integram em uma estrutura unificada de modelagem. Ao longo dos anos, consolidamos a convicção de que ensinar regressão exige equilíbrio entre rigor teórico e clareza interpretativa.\nO livro desenvolve o Modelo de Regressão Linear Simples e o Modelo de Regressão Linear Múltipla de forma progressiva, enfatizando interpretação, hipóteses, propriedades dos estimadores, inferência clássica e diagnóstico. A implementação é realizada integralmente no R, utilizado como ambiente de experimentação conceitual e não apenas como ferramenta computacional.\nOs fundamentos matemáticos mais densos são apresentados em apêndices específicos, preservando o fluxo didático do texto principal e permitindo diferentes níveis de aprofundamento. Acreditamos que a regressão deve ser ensinada como modelo estatístico, com atenção às suas suposições, limitações e implicações práticas.\nEsperamos que este livro contribua para a formação de profissionais capazes de utilizar modelos de regressão com rigor técnico, pensamento estatístico crítico e responsabilidade analítica.",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introdução e Panorama dos Modelos de Regressão",
    "section": "",
    "text": "1.1 A centralidade da regressão\nEste livro é dedicado aos estudos de Modelos de Regressão, desde a formulação clássica até extensões modernas. O material apoia as disciplinas Modelos de Regressão I (CC0290) e Modelagem Estatística (CC0452), ambas com implementação no software R. Ao longo do semestre, o estudante terá contato tanto com a fundamentação matemática e estatística quanto com aplicações práticas em diferentes áreas, utilizando ferramentas computacionais para explorar dados reais.\nO curso está estruturado de forma progressiva: parte-se da regressão linear simples, como porta de entrada ao raciocínio de modelagem, avança-se para a regressão linear múltipla, inclusão de variáveis categóricas e técnicas de seleção de variáveis e análise de diagnóstico. Tópicos adicionais como modelos lineares generalizados (MLGs), extensões não lineares e métodos de regularização também serão apresentados.\nMais do que aprender procedimentos técnicos, o objetivo é desenvolver a capacidade de interpretar resultados, avaliar a adequação dos modelos e comunicar conclusões de forma clara. A ênfase está tanto na teoria quanto na prática, de modo que o estudante seja capaz de aplicar a modelagem estatística em contextos multidisciplinares.\nA análise de regressão ocupa posição central na Estatística e, em especial, na Econometria. Como afirma Hoffmann (2016):\nEssa afirmação reflete o fato de que praticamente toda modelagem econométrica, seja para estimar elasticidades, avaliar políticas públicas, medir impactos ou testar teorias, passa, de alguma forma, por um modelo de regressão.\nMas essa centralidade não é exclusiva da economia. Na saúde, a regressão mede risco e associações; na engenharia, modela desempenho; nas ciências ambientais, estima impactos; nas ciências sociais, investiga relações estruturais; na ciência de dados, permanece como ferramenta interpretável diante de modelos mais complexos.\nA regressão tornou-se, portanto, uma linguagem universal para responder a uma pergunta fundamental:\nEssa pergunta é simples. A resposta exige matemática, probabilidade, inferência e interpretação.",
    "crumbs": [
      "Parte I — Modelagem",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução e Panorama dos Modelos de Regressão</span>"
    ]
  },
  {
    "objectID": "intro.html#a-centralidade-da-regressão",
    "href": "intro.html#a-centralidade-da-regressão",
    "title": "1  Introdução e Panorama dos Modelos de Regressão",
    "section": "",
    "text": "“A análise de regressão é o método mais importante da econometria.”\n\n\n\n\n\nComo varia uma quantidade quando outra varia?",
    "crumbs": [
      "Parte I — Modelagem",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução e Panorama dos Modelos de Regressão</span>"
    ]
  },
  {
    "objectID": "intro.html#objetivos-do-livro",
    "href": "intro.html#objetivos-do-livro",
    "title": "1  Introdução e Panorama dos Modelos de Regressão",
    "section": "1.2 Objetivos do livro",
    "text": "1.2 Objetivos do livro\n\nContextualizar historicamente os modelos de regressão;\n\nCompreender a lógica da modelagem estatística: componente sistemático, componente aleatório e relação entre estes;\n\nApresentar e discutir os principais modelos abordados na disciplina;\n\nConectar a teoria com aplicações em economia, saúde, engenharia, ciências sociais e ambientais;\n\nDiscutir potenciais e limitações de cada abordagem, reconhecendo as hipóteses subjacentes;\n\nDesenvolver a capacidade de usar ferramentas computacionais para ajuste, diagnóstico e interpretação de modelos.\n\nEste material é concebidao como uma jornada pela família dos modelos de regressão. Partimos de um problema simples, como relacionar uma variável resposta a uma variável explicativa, e avançamos gradualmente até modelos capazes de lidar com múltiplos fatores, variáveis categóricas, dados de contagem, proporções e situações em que as hipóteses clássicas do modelo deixam de ser válidas.\nA ideia central é que, ao final do semestre, o estudante seja capaz de compreender não apenas como ajustar um modelo, mas também quando e por que usá-lo, avaliando sua adequação e reconhecendo seus limites.",
    "crumbs": [
      "Parte I — Modelagem",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução e Panorama dos Modelos de Regressão</span>"
    ]
  },
  {
    "objectID": "intro.html#breve-história-da-regressão",
    "href": "intro.html#breve-história-da-regressão",
    "title": "1  Introdução e Panorama dos Modelos de Regressão",
    "section": "1.3 Breve História da Regressão",
    "text": "1.3 Breve História da Regressão\nA regressão, como hoje conhecemos, é fruto de mais de um século de evolução teórica e prática. Seu ponto de partida remonta ao século XIX, quando estudos empíricos sobre hereditariedade começaram a revelar regularidades que poderiam ser descritas matematicamente. Desde então, a regressão deixou de ser apenas uma curiosidade biológica e tornou-se um dos pilares da estatística moderna, sustentando análises nas mais diversas áreas.\n\n1.3.1 Francis Galton (1822–1911)\n\n\n\nFrancis Galton\n\n\nA história da regressão começa com uma inquietação simples, quase doméstica. No final do século XIX, Francis Galton observava famílias inglesas e fazia uma pergunta que parecia trivial, mas que mudaria a estatística para sempre: filhos de pais muito altos serão igualmente altos?\nAo analisar dados de estaturas familiares, Galton percebeu algo intrigante: filhos de pais muito altos tendiam a ser altos, mas não tanto quanto seus pais; filhos de pais muito baixos tendiam a ser baixos, mas não tão baixos quanto seus progenitores. Havia uma força invisível puxando as medidas extremas de volta ao centro. Em 1886 (Galton (1886b); Galton (1886a)), ele chamou esse fenômeno de regression toward mediocrity.\nSem perceber completamente, Galton havia introduzido quatro pilares da regressão moderna:\numa variável resposta, uma variável explicativa, uma média condicional e uma variabilidade em torno dessa média.\nA matemática ainda era rudimentar. Mas a ideia estava lançada.\n\n\n1.3.2 Karl Pearson (1857–1936)\n\n\n\nKarl Pearson\n\n\nSe Galton teve a intuição, Karl Pearson deu forma matemática ao fenômeno. Discípulo e colaborador de Galton, Pearson transformou observações empíricas em estrutura formal. Desenvolveu o coeficiente de correlação linear, sistematizou métodos de ajuste e ajudou a fundar o primeiro departamento de estatística do mundo, no University College London. Criou também a revista Biometrika, marco na consolidação da estatística como disciplina científica.\nA regressão deixava de ser apenas uma regularidade observada em dados biológicos. Tornava-se parte da teoria da probabilidade e da estatística matemática (Pearson e Lee (1903)).\nO que antes era descrição começava a se tornar método.\n\n\n1.3.3 Ronald A. Fisher (1890–1962)\n\n\n\nRonald Fisher\n\n\nNas décadas seguintes, a estatística enfrentava um novo desafio: não bastava ajustar curvas; era preciso decidir. Ronald A. Fisher foi o arquiteto dessa virada. Na década de 1920, incorporou fundamentos de inferência à regressão e redefiniu o papel da estatística na ciência.\nCom Fisher surgem a análise de variância (ANOVA), o método da máxima verossimilhança e os princípios modernos de planejamento experimental. A regressão passa a permitir testar hipóteses, construir intervalos de confiança e quantificar incertezas.\nA técnica deixa de ser apenas descritiva. Torna-se ferramenta de decisão científica.\n\n\n1.3.4 Jerzy Neyman (1894–1981) & Egon Pearson (1895–1980)\n\n\n\nJerzy Neyman & Egon Pearson\n\n\nA década de 1930 marca outro salto conceitual. Jerzy Neyman e Egon Pearson, filho de Karl Pearson, estruturam formalmente os testes de hipóteses. Definem os erros do tipo I e tipo II, introduzem critérios de decisão e consolidam o conceito de intervalo de confiança.\nA regressão, agora, não apenas estima relações: ela fornece regras claras para aceitar ou rejeitar hipóteses. A incerteza passa a ser quantificada de maneira sistemática.\nO método ganha rigor lógico.\n\n\n1.3.5 John Tukey (1915–2000)\n\n\n\nJohn Tukey\n\n\nMas a estatística não evolui apenas por formalização. Na década de 1950, John Tukey propõe algo radical: antes de testar, é preciso explorar. Defende que os dados devem ser interrogados visualmente. Populariza gráficos de resíduos, diagnósticos e técnicas exploratórias.\nA regressão passa a ser acompanhada por perguntas práticas:\nos pressupostos fazem sentido? há pontos influentes? o modelo realmente representa os dados?\nA estatística recupera o diálogo com a realidade empírica.\n\n\n1.3.6 Peter McCullagh (1952–) & John Nelder (1934–2010)\n\n\n\nPeter McCullagh & John Nelder\n\n\nCom o avanço da computação nas décadas de 1960 e 1970, a regressão amplia suas fronteiras. A álgebra matricial viabiliza modelos com múltiplos preditores, e a necessidade de analisar dados não normais torna-se evidente.\nEm 1983, McCullagh e Nelder publicam Generalized Linear Models, obra que sistematiza os Modelos Lineares Generalizados. A regressão deixa de ser restrita a variáveis contínuas normalmente distribuídas. Passa a modelar contagens, proporções, tempos de ocorrência.\nA ideia que nasceu da altura de pais e filhos expande-se para praticamente todos os campos científicos.\nA regressão não surgiu pronta. Foi construída por inquietações, formalizações, rupturas e expansões. Cada geração acrescentou uma camada: intuição, estrutura, inferência, decisão, diagnóstico e generalização.\nO que começou como uma pergunta sobre estatura tornou-se uma das ferramentas centrais da ciência moderna.",
    "crumbs": [
      "Parte I — Modelagem",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução e Panorama dos Modelos de Regressão</span>"
    ]
  },
  {
    "objectID": "intro.html#o-contexto-histórico-eugenia",
    "href": "intro.html#o-contexto-histórico-eugenia",
    "title": "1  Introdução e Panorama dos Modelos de Regressão",
    "section": "1.4 O contexto histórico: eugenia",
    "text": "1.4 O contexto histórico: eugenia\nÉ necessário reconhecer que parte do desenvolvimento inicial da estatística ocorreu em um contexto marcado por ideias eugenistas. Francis Galton cunhou o termo “eugenia” em 1883. A eugenia defendia o “melhoramento” da raça humana por meio de seleção artificial.\nKarl Pearson foi defensor ativo dessas ideias e dirigiu o laboratório Francis Galton para a Eugenia Nacional na University College London. Ronald Fisher também expressou posições eugenistas em seus escritos.\nAs ideias eugenistas foram posteriormente utilizadas para justificar políticas discriminatórias e violações graves de direitos humanos no século XX. Após a Segunda Guerra Mundial, consolidou-se um consenso internacional de condenação à eugenia como ideologia racista e incompatível com princípios éticos fundamentais.\nEssa contextualização histórica não diminui a importância dos avanços metodológicos produzidos por esses autores. Pelo contrário, reforça a necessidade de compreender que métodos estatísticos são ferramentas cujo uso exige responsabilidade ética.\nA regressão é instrumento científico poderoso. O modo como é utilizada depende do pesquisador.",
    "crumbs": [
      "Parte I — Modelagem",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução e Panorama dos Modelos de Regressão</span>"
    ]
  },
  {
    "objectID": "intro.html#consolidação-da-regressão-como-pilar-da-estatística-moderna",
    "href": "intro.html#consolidação-da-regressão-como-pilar-da-estatística-moderna",
    "title": "1  Introdução e Panorama dos Modelos de Regressão",
    "section": "1.5 Consolidação da regressão como pilar da Estatística moderna",
    "text": "1.5 Consolidação da regressão como pilar da Estatística moderna\nA trajetória da regressão pode ser compreendida como uma sequência evolutiva:\n\nObservação empírica da regressão à média (Galton);\n\nFormalização matemática e correlação (Pearson);\n\nInferência e planejamento experimental (Fisher);\n\nEstrutura formal de testes de hipóteses (Neyman-Pearson);\n\nDiagnóstico e análise exploratória (Tukey);\n\nGeneralização para modelos lineares generalizados (McCullagh e Nelder);\n\nRegularização moderna (Ridge e LASSO);\n\nIntegração com ciência de dados e aprendizagem de máquina.\n\nApesar das transformações metodológicas, a pergunta central permanece:\n\nQual é a relação média entre uma variável resposta e seus preditores?",
    "crumbs": [
      "Parte I — Modelagem",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução e Panorama dos Modelos de Regressão</span>"
    ]
  },
  {
    "objectID": "intro.html#panorama-dos-modelos-de-regressão",
    "href": "intro.html#panorama-dos-modelos-de-regressão",
    "title": "1  Introdução e Panorama dos Modelos de Regressão",
    "section": "1.6 Panorama dos Modelos de Regressão",
    "text": "1.6 Panorama dos Modelos de Regressão\nTendo em mente os objetivos, estruturas e suposições discutidos anteriormente, é possível visualizar o panorama dos modelos de regressão que compõem esta disciplina. A ideia é seguir uma trajetória progressiva: partir de modelos simples e intuitivos, que permitem enxergar diretamente a relação entre duas variáveis, e avançar gradualmente para estruturas mais sofisticadas, capazes de lidar com múltiplos fatores, respostas não normais e bases de dados complexas.\nO percurso do curso está organizado em quatro blocos principais:\n\nFundamentos — regressão linear simples (MRLS), que introduz a lógica de média condicional, estimação por mínimos quadrados e análise de resíduos.\n\nModelos com múltiplos fatores — regressão linear múltipla (MRLM), inclusão de variáveis categóricas e interpretação de efeitos parciais.\n\nValidação e escolha de modelos — métodos de diagnóstico, análise de resíduos, critérios de seleção de variáveis e regularização.\n\nExtensões — modelos lineares generalizados (GLMs), regressão polinomial, modelos não lineares e métodos modernos de regularização (Ridge e LASSO), fundamentais na era dos grandes volumes de dados.\n\nEm cada etapa, o modelo será apresentado de forma integrada: sua fundamentação teórica, as suposições que o tornam válido, exemplos de interpretação prática e aplicações reais em áreas como economia, saúde, engenharia, ciências ambientais e ciência de dados.\nAssim, o panorama da disciplina funciona como um mapa de navegação: consolidamos primeiro os alicerces da regressão, depois exploramos aplicações mais complexas e, por fim, avançamos até modelos modernos que dialogam diretamente com os desafios atuais da análise de dados.\n\n\n\n\nGalton, Francis. 1886a. “Family likeness in stature”. Proceedings of the Royal Society of London 40: 42–72.\n\n\n———. 1886b. “Regression towards mediocrity in hereditary stature”. Journal of the Anthropological Institute 15: 246–63.\n\n\nHoffmann, Rodolfo. 2016. Análise de Regressão: Uma Introdução à Econometria. 5º ed. Portal de Livros Abertos da USP. https://doi.org/10.11606/9788592105709.\n\n\nPearson, Karl, e Alice Lee. 1903. “On the laws of inheritance”. Biometrika 2: 357–462.",
    "crumbs": [
      "Parte I — Modelagem",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução e Panorama dos Modelos de Regressão</span>"
    ]
  },
  {
    "objectID": "modelagem.html",
    "href": "modelagem.html",
    "title": "2  Modelagem Estatística e Regressão",
    "section": "",
    "text": "2.1 Introdução à Modelagem\nAntes de estudarmos a regressão em si, é importante entender que ela faz parte de um campo mais amplo chamado modelagem matemática/estatística. Modelar é o ato de construir representações formais de fenômenos reais com o objetivo de descrevê-los, explicá-los, prever seu comportamento ou orientar decisões.\nEm termos conceituais, modelar significa responder à seguinte pergunta:\nDe forma geral, a modelagem matemática/estatística consiste na tentativa de traduzir um problema do mundo real em termos matemáticos ou estatísticos, estruturando hipóteses, equações e relações que permitam analisar e responder à pergunta proposta. Ela está presente em diversas áreas, como física, química, biologia, economia, engenharia e ciências sociais.\nÉ fundamental compreender que um modelo não é a realidade. Ele é uma aproximação útil. Todo modelo envolve simplificações, suposições e escolhas: quais variáveis considerar, quais ignorar, que tipo de relação assumir, qual grau de precisão é necessário. Assim, modelar é sempre um exercício de equilíbrio entre realismo e simplicidade.\nAlguns exemplos de fenômenos que podem ser descritos por modelos matemáticos incluem:\n- Crescimento populacional;\n- Reações químicas;\n- Sistemas mecânicos e eletrônicos;\n- Previsão do clima;\n- Dinâmica do tráfego e da logística;\n- Estratégias econômicas e financeiras;\n- Mudanças ambientais e climáticas.",
    "crumbs": [
      "Parte I — Modelagem",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelagem Estatística e Regressão</span>"
    ]
  },
  {
    "objectID": "modelagem.html#introdução-à-modelagem",
    "href": "modelagem.html#introdução-à-modelagem",
    "title": "2  Modelagem Estatística e Regressão",
    "section": "",
    "text": "Como posso representar, de maneira estruturada, um fenômeno complexo do mundo real por meio de variáveis e relações formais?\n\n\n\n\n\n2.1.1 Modelo matemático versus modelo estatístico\nUma distinção conceitual importante é a diferença entre:\n\nModelo matemático (determinístico): descreve uma relação funcional exata entre variáveis. Para cada valor de entrada, existe um único valor de saída.\n\nModelo estatístico (estocástico): descreve uma relação média ou probabilística, reconhecendo a presença de variabilidade não explicada pelas variáveis observadas.\n\nNo modelo determinístico, escreve-se algo como:\n\\[\nY = 2X\n\\]\nAqui, se \\(X=5\\), então necessariamente \\(Y=10\\). Não há variação possível.\nJá em um modelo estatístico, reconhecemos que fenômenos reais sofrem influência de fatores não observados, erros de mensuração, flutuações naturais ou variáveis omitidas. Assim, escreve-se:\n\\[\nY = 2X + \\varepsilon,\n\\]\nem que \\(\\varepsilon\\) representa a componente aleatória (ruído).\nA diferença entre essas duas estruturas pode ser visualizada na simulação abaixo.\n\n\n\n\n\n\n\n\nFigura 2.1: Modelo matemático (determinístico) versus modelo estatístico (com ruído): no primeiro, a relação é funcional; no segundo, observa-se uma tendência média com variabilidade aleatória.\n\n\n\n\n\nObserve que, no modelo estatístico, não buscamos uma igualdade exata, mas uma tendência média em torno da qual os dados se distribuem.\nEssa diferença é central para compreender a regressão.\n\n\n2.1.2 Classificações didáticas de modelos\nDe forma didática, modelos matemáticos/estatísticos podem ser classificados em:\n\nDeterminísticos ou estocásticos (estatísticos)\nDependendo se o acaso está explicitamente presente na formulação.\nDiscretos ou contínuos\nDependendo da natureza das variáveis envolvidas. Contagens são discretas; altura, peso e temperatura são contínuos.\nDinâmicos ou estáticos\nDependendo se o tempo é incorporado explicitamente na estrutura do modelo.\n\nEssas classificações são discutidas em livros clássicos de modelagem matemática, como Meerschaert (2013) e Giordano, Fox, e Horton (2013). Elas ajudam a organizar o tipo de pergunta que estamos fazendo e a estrutura matemática adequada para respondê-la.\n\n\n2.1.3 O ciclo da modelagem\nO processo de modelagem geralmente envolve as seguintes etapas:\n\nFormular a pergunta em termos matemáticos.\n\nSelecionar uma abordagem de modelagem.\n\nConstruir o modelo com base nas variáveis e hipóteses do problema.\n\nResolver o modelo matemático (ou ajustá-lo aos dados, no caso estatístico).\n\nInterpretar a solução em termos do fenômeno real.\n\nEsse ciclo é iterativo: muitas vezes o modelo precisa ser ajustado ou refinado conforme novas informações surgem. Ao confrontar o modelo com dados, podem surgir questões como:\n\nA forma funcional escolhida faz sentido?\nVariáveis importantes foram omitidas?\nO comportamento dos resíduos está de acordo com as hipóteses?\nO modelo mantém desempenho em novos dados?\n\nModelagem não é um procedimento linear; é um processo de aproximações sucessivas.\nAlém disso, três elementos costumam acompanhar o ciclo de modelagem:\n\nEstimação ou calibração: ajuste de parâmetros com base em dados observados.\nValidação: verificação da qualidade do ajuste.\nAnálise de sensibilidade: avaliação do impacto de pequenas mudanças nas hipóteses ou nos dados.\n\n\n\n2.1.4 Exemplos de modelagem\n\nProblema de otimização: determinar o momento ideal de venda de um animal de criação, considerando ganho de peso, custo de manutenção e preço de mercado.\n\nProblema de crescimento populacional: prever a evolução da população de um país ou espécie a partir de dados censitários.\n\nDentro desse panorama mais amplo, a regressão ocupa um papel específico: ela é uma técnica estatística voltada para modelar a média condicional de uma variável resposta em função de variáveis explicativas. Em outras palavras, ela formaliza matematicamente a ideia de que parte da variação observada pode ser explicada sistematicamente, enquanto outra parte permanece como ruído.\nModelagem é, portanto, a linguagem que conecta teoria, dados e inferência. A regressão é uma de suas expressões mais importantes.\n\n2.1.4.1 Exemplos de modelagem em diferentes áreas\nA modelagem matemática e estatística é utilizada para descrever e prever o comportamento de fenômenos em uma grande variedade de contextos. Alguns exemplos ilustrativos incluem:\n\nEpidemiologia: modelos SIR (Susceptíveis–Infectados–Recuperados) para descrever a propagação de doenças infecciosas ao longo do tempo.\nEngenharia de Pesca: relação entre esforço de pesca (dias de mar, número de embarcações) e o estoque pesqueiro disponível.\nCiência de Dados: previsão da demanda de energia elétrica a partir de temperatura, hora do dia e perfil de consumo.\nEconomia: modelos de oferta e demanda que relacionam preços e quantidades em equilíbrio de mercado.\nClimatologia: simulações que conectam emissão de gases de efeito estufa, temperatura média global e regimes de precipitação.\nEducação: análise da relação entre investimento em ensino e desempenho em exames padronizados.\nOceanografia: modelos que descrevem correntes marinhas em função de gradientes de temperatura e salinidade.\n\nEsses exemplos mostram que modelagem é uma linguagem comum em ciência e tecnologia. A regressão estatística é uma forma particular de modelagem que foca em quantificar relações entre variáveis observáveis, buscando separar o sinal (estrutura determinística) do ruído (componente aleatória).",
    "crumbs": [
      "Parte I — Modelagem",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelagem Estatística e Regressão</span>"
    ]
  },
  {
    "objectID": "modelagem.html#introduçao-à-regressão",
    "href": "modelagem.html#introduçao-à-regressão",
    "title": "2  Modelagem Estatística e Regressão",
    "section": "2.2 Introduçao à Regressão",
    "text": "2.2 Introduçao à Regressão\nRegressão é um modelo matemático-estatístico que busca relacionar uma variável resposta (\\(Y\\)) com uma ou mais variáveis explicativas. De forma conceitual, a regressão parte da ideia de que o comportamento médio de \\(Y\\) pode ser descrito condicionalmente às variáveis explicativas \\(X\\).\nEm termos mais precisos, o que a regressão modela não é simplesmente \\(Y\\), mas a média condicional:\n\\[\nE(Y \\mid X)\n\\]\nEssa perspectiva é central nos textos clássicos de regressão e econometria (Charnet et al. (2008); Hoffmann (2006); Gujarati (2006)). O objetivo não é afirmar que \\(X\\) determina exatamente \\(Y\\), mas que existe uma estrutura sistemática média associada às variáveis explicativas.\nA ideia fundamental é que a variação observada em um fenômeno pode ser decomposta em duas partes:\n\nUma estrutura determinística (ou componente sistemática), que representa o sinal da relação entre as variáveis;\nUma componente aleatória, representada pelo ruído \\(\\varepsilon\\), que captura variações não explicadas pelo modelo.\n\nEssa decomposição aparece de forma recorrente na literatura de regressão aplicada (Draper e Smith (1998); Kutner et al. (2005); Montgomery, Peck, e Vining (2021)) como a base conceitual do modelo linear.\nDe forma geral, existem duas representações usuais para essa ideia. - Modelo Aditivo (o mais utilizado):\n\\[\nY = \\mu(X) + \\varepsilon\n\\]\n\nModelo Multiplicativo (útil em contextos onde a variabilidade é proporcional ao nível médio):\n\n\\[\nY = \\mu(X) \\cdot \\varepsilon\n\\]\ncom:\n\n\\(mu(X)\\) → parte determinística (estrutura média);\n\\(\\varepsilon\\) → componente aleatória (ruído), que pode ser aditivo ou multiplicativo.\n\nO sinal \\(\\mu(X)\\) também é denominado de função de regressão.\nNo modelo aditivo clássico, assume-se que\n\\[\nE(\\varepsilon \\mid X) = 0,\n\\]\nisto é, o erro não carrega informação sistemática adicional sobre \\(Y\\) além da já contida em \\(X\\). Essa condição garante que \\(E(Y \\mid X) = \\mu(X)\\), permitindo interpretar \\(\\mu(X)\\) como a média condicional de \\(Y\\) dado \\(X\\). Essa hipótese é central para a interpretação dos coeficientes como efeitos médios condicionais (Hoffmann (2006); Gujarati (2006)).\nNo caso do modelo multiplicativo, a condição análoga é \\[\nE(\\varepsilon \\mid X) = 1.\n\\]\nNesse caso, temos \\(E(Y \\mid X) = \\mu(X)\\,E(\\varepsilon \\mid X) = \\mu(X)\\), de modo que \\(\\mu(X)\\) continua representando a média condicional de \\(Y\\) dado \\(X\\).\nA diferença fundamental entre as duas formulações é estrutural. No modelo aditivo, o erro representa um deslocamento absoluto em torno da média condicional: a variação aleatória soma-se ao valor esperado, produzindo oscilações cuja magnitude, em princípio, não depende do nível médio. Já no modelo multiplicativo, o erro atua como um fator proporcional, ampliando ou reduzindo o valor médio de acordo com sua própria intensidade. Nesse caso, a variabilidade está intrinsecamente ligada ao nível esperado da variável resposta.\nEssa distinção tem implicações relevantes. Em estruturas aditivas, concebe-se a variabilidade como relativamente independente da escala do fenômeno; em estruturas multiplicativas, a dispersão tende a crescer ou decrescer proporcionalmente ao valor médio. Fenômenos econômicos, biológicos e ambientais frequentemente apresentam esse comportamento proporcional, por exemplo, renda, produção, crescimento populacional ou biomassa, o que torna a formulação multiplicativa conceitualmente mais adequada em muitos contextos.\nA escolha entre uma estrutura aditiva ou multiplicativa envolve considerações teóricas e empíricas. Modelos econômicos frequentemente sugerem relações proporcionais, enquanto a inspeção gráfica dos dados pode revelar padrões de variância crescente. Assim, a definição da forma funcional e da natureza da variação depende da teoria utilizada, da escala de mensuração das variáveis e do comportamento empírico observado nos dados (Gujarati (2006); Hoffmann (2006)).\nNo caso do modelo aditivo clássico, a hipótese de que o erro não carrega informação sistemática adicional além da contida nas variáveis explicativas é central para a interpretação dos coeficientes como efeitos médios condicionais. Essa perspectiva, segundo a qual a regressão modela a média condicional e não valores individuais, constitui um dos pilares da regressão aplicada e da econometria tradicional (Hoffmann (2006); Gujarati (2006)).\n\n2.2.1 Interpretação conceitual: sinal e ruído\nA regressão parte do reconhecimento de que fenômenos reais são influenciados por múltiplos fatores, muitos dos quais não são observáveis ou mensuráveis. Assim, mesmo que exista uma relação estrutural entre \\(X\\) e \\(Y\\), os dados observados não estarão perfeitamente alinhados sobre uma curva.\nO modelo assume que:\n\\[\nY = \\text{sinal} + \\text{ruído}\n\\]\nO sinal representa a parte explicável pelas variáveis incluídas no modelo; o ruído representa:\n\nvariáveis omitidas,\nerros de mensuração,\nflutuações naturais,\nfatores aleatórios não controlados.\n\nEssa separação entre componente sistemática e erro é um dos pilares da regressão linear clássica (Draper e Smith (1998)).\n\n2.2.1.1 Exemplo ilustrativo: Sinal + Ruído\nA seguir, simulamos um conjunto de dados artificiais para ilustrar a lógica central da regressão: um sinal determinístico mais um componente de ruído.\nConsidere a relação:\n\\[\nY = 3 + 2X + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, 5^2),\n\\]\ncom \\(X \\in \\{ 1, 2, \\dots, 30\\}\\). A reta \\(Y = 3 + 2X\\) representa o sinal verdadeiro, enquanto os pontos observados \\((X, Y)\\) incluem a variabilidade aleatória do ruído \\(\\varepsilon\\).\nO gráfico abaixo mostra a dispersão dos dados simulados juntamente com a curva verdadeira. Essa visualização reforça a ideia de que a regressão busca recuperar a estrutura determinística do fenômeno em meio à aleatoriedade introduzida pelos ruídos (erros ou fonte de variação).\n\n\n\n\n\n\n\n\n\n\n\n2.2.1.2 Exemplo ilustrativo: Sinal + Ruído (homoscedástico vs heteroscedástico)\nNeste exemplo, comparamos dois cenários de regressão linear que diferem apenas na dispersão dos ruídos:\n\nHomoscedástico: a variância dos erros é constante em todos os valores de \\(X\\). Nesse caso, a nuvem de pontos se distribui de forma aproximadamente uniforme em torno da reta verdadeira, independentemente da posição no eixo \\(X\\).\n\nHeteroscedástico: a variância cresce com \\(X\\). Assim, para valores pequenos de \\(X\\) os pontos estão mais concentrados, enquanto para valores grandes de \\(X\\) a dispersão aumenta.\n\nEsse contraste é fundamental: se a homoscedasticidade não for respeitada, os estimadores de mínimos quadrados continuam não viesados, mas deixam de ser eficientes (ou seja, não são os de menor variância). Isso motiva o uso de métodos alternativos, como os mínimos quadrados ponderados (WLS) ou transformações nos dados.\n\n\n\n\n\nSinal + ruído: (esq.) variância constante; (dir.) variância crescente (heteroscedasticidade).\n\n\n\n\n\n\n2.2.1.3 Exemplo ilustrativo: Possíbildiades de relações ente X e Y\nObserve que nem toda relação entre duas variáveis é linear e forte. Considere algums possibilidades comuns:\n\n(a) Sem correlação: não existe padrão claro entre \\(X\\) e \\(Y\\); conhecer \\(X\\) não ajuda a prever \\(Y\\).\n\n(b) Correlação linear fraca: existe uma tendência positiva, mas com grande dispersão ao redor da reta.\n\n(c) Correlação linear forte: os pontos seguem de perto uma tendência linear; \\(X\\) explica grande parte da variabilidade em \\(Y\\).\n\n(d) Relação não linear: \\(X\\) e \\(Y\\) se relacionam, mas a forma não é bem descrita por uma reta (por exemplo, uma parábola).\n\nEsse exemplo mostra que a correlação linear é apenas um caso particular dentro de uma variedade de possíveis dependências entre variáveis. Por isso, ao analisar dados, é sempre importante, quando possível, visualizar os diagramas de dispersão antes de ajustar um modelo, evitando conclusões equivocadas sobre linearidade.\n\n\n\n\n\nCorrelação entre X e Y: (a) nenhuma, (b) linear fraca, (c) linear forte, (d) não linear.\n\n\n\n\n\n\n2.2.1.4 Exemplo ilustrativo: Relação não linear entre \\(X\\) e \\(Y\\)\nAté agora, discutimos a regressão sob a perspectiva de relações lineares. No entanto, nem todo fenômeno real apresenta comportamento aproximadamente linear. Em muitos contextos, como crescimento biológico, processos de absorção, liberação acumulada de substâncias ou resposta a doses, a relação entre as variáveis pode apresentar curvatura, saturação ou pontos de inflexão.\nNeste exemplo, construiremos artificialmente uma base de dados cujo sinal verdadeiro não é linear. O comportamento adotado será do tipo crescimento com saturação: inicialmente, pequenos aumentos em \\(X\\) produzem grandes aumentos em \\(Y\\); à medida que \\(X\\) cresce, o efeito marginal diminui e a curva tende a estabilizar.\nEssa estrutura pode ser representada genericamente por uma função do tipo:\n\\[\nY = f(X) + \\varepsilon\n\\]\nem que \\(f(X)\\) é não linear e \\(\\varepsilon\\) representa o componente aleatório.\nO objetivo aqui é didático: comparar dois cenários sobre os mesmos dados observados:\n\nAjustar um modelo linear, mesmo sabendo que o sinal não é linear;\nComparar esse ajuste com a curva verdadeira que gerou os dados.\n\nEssa comparação permite visualizar um ponto conceitual central da modelagem:\n\nUm modelo pode estar corretamente estimado sob suas hipóteses e, ainda assim, estar incorretamente especificado.\n\nOu seja, o problema pode não estar na estimação, mas na forma funcional escolhida.\nAo observar os gráficos, procure refletir:\n\nO modelo linear captura adequadamente o padrão médio?\nHá evidências visuais de curvatura?\nO erro parece sistemático ao longo de \\(X\\)?\nO que aconteceria com os resíduos nesse caso?\n\nEsse tipo de análise visual é uma etapa importante antes da formalização do modelo. A regressão linear é uma ferramenta poderosa, mas sua adequação depende da coerência entre a estrutura assumida e o comportamento real dos dados.\n\n\n\n\n\n\n\n\nFigura 2.2: Exemplo com sinal não linear: (esq.) ajuste linear; (dir.) sinal verdadeiro que gerou os dados.\n\n\n\n\n\n\n\n\n2.2.2 Objetivos e Estruturas do Modelo da Regressão\nDepois de explorar exemplos visuais de associação entre variáveis, é importante consolidar uma visão mais conceitual sobre a regressão. A regressão não é simplesmente uma técnica de ajuste de curvas; ela é uma estrutura formal para modelar a relação média entre variáveis observáveis.\nEm termos conceituais, a regressão procura compreender como o comportamento médio de \\(Y\\) varia em função de \\(X\\). Isto é, ela organiza a seguinte pergunta:\n\nComo a variável resposta se comporta, em média, quando as variáveis explicativas variam?\n\n\n2.2.2.1 Objetivos centrais\nA regressão pode ser compreendida a partir de três grandes finalidades:\n\nExplicar\nIdentificar quais fatores estão associados à variável resposta e quantificar a magnitude dessas associações.\nPredizer\nUtilizar o modelo ajustado para estimar valores futuros ou não observados de \\(Y\\).\nControlar\nAvaliar o efeito de uma variável mantendo as demais constantes, permitindo interpretações condicionais.\n\nEsses objetivos aparecem de maneira combinada na prática. Em econometria, frequentemente busca-se compreender relações estruturais entre variáveis macroeconômicas (Gujarati (2006); Hoffmann (2006)). Em engenharia e estatística aplicada, pode haver maior ênfase na capacidade preditiva do modelo (Montgomery, Peck, e Vining (2021)). Em estudos científicos em geral, explicar e prever caminham juntos.\n\n\n2.2.2.2 Observações conceituais importantes\nAntes de avançarmos para a formalização do modelo linear simples, duas observações merecem destaque.\n1. Correlação significativa e ajuste linear\nEm geral, quando existe correlação linear significativa entre variável explicativa e variável resposta, é razoável esperar que o Modelo de Regressão Linear Simples produza um ajuste satisfatório. Isso ocorre porque a correlação mede o grau de associação linear entre duas variáveis. Se essa associação é forte, a reta ajustada tende a capturar uma parcela substancial da variabilidade observada.\nEntretanto, correlação elevada não garante que o modelo esteja corretamente especificado. Pode haver curvatura, variância não constante ou outros padrões estruturais não capturados por uma reta. A inspeção gráfica continua sendo essencial.\n2. Associação estatística não implica causa e efeito\nUma relação estatística entre variáveis não implica automaticamente causalidade.Mesmo que um coeficiente estimado seja estatisticamente significativo, isso não significa que uma variável cause a outra. A interpretação causal:\n\nnão pode se basear apenas na amostra considerada;\ndeve estar apoiada em teoria ou conhecimento empírico;\npode exigir desenho experimental ou hipóteses estruturais adicionais.\n\nExemplos clássicos ilustram essa distinção:\n\nDespesas de consumo pessoal e renda pessoal disponível — aqui, a teoria econômica sustenta a direção causal.\nGanho de peso e consumo de calorias — a evidência empírica e experimental apoia a interpretação causal.\n\nSem essa base teórica ou experimental, a regressão revela associação, não mecanismo causal.\n\n\n2.2.2.3 Duas características fundamentais do modelo de regressão\nDo ponto de vista probabilístico, um modelo de regressão possui duas características essenciais:\n\nPara cada nível fixado de \\(X\\), existe uma distribuição de probabilidade de \\(Y\\).\nAs médias dessas distribuições variam de forma sistemática com \\(X\\).\n\nEssa segunda característica é o coração da regressão: modelar como a média de \\(Y\\) se altera quando \\(X\\) varia.\nVisualmente, isso significa que, para cada valor de \\(X\\), não há um único valor possível de \\(Y\\), mas sim uma distribuição de valores possíveis. O modelo descreve o comportamento médio dessas distribuições.\n\n\n\n\n\nComo as observações são geradas em regressão linear.\n\n\n\n\nEssa perspectiva probabilística será formalizada no próximo capítulo, quando introduzirmos o Modelo de Regressão Linear Simples (MRLS).\n\n\n2.2.2.4 Estrutura conceitual da regressão\nIndependentemente da área, a regressão parte de uma ideia fundamental:\n\\[\nY = \\text{parte sistemática} + \\text{parte não explicada}\n\\]\nA parte sistemática representa o padrão médio associado às variáveis explicativas. A parte não explicada representa variações adicionais que não são capturadas pelo modelo, seja por fatores não observados, limitações de mensuração ou variabilidade inerente ao fenômeno.\nA regressão organiza essa decomposição de maneira formal e mensurável. Ela não elimina a variabilidade; ela a estrutura.\n\n\n2.2.2.5 Alguns exemplos que mostram a importância prática\nA utilidade da regressão torna-se mais clara quando observamos aplicações concretas.\n\nEconomia\nModelos de regressão são usados para analisar como variáveis como taxa de juros, câmbio e nível de consumo se associam à inflação. O objetivo pode ser compreender o mecanismo econômico (explicação) ou projetar cenários futuros (previsão).\nSaúde\nEm estudos clínicos, a regressão permite avaliar a relação entre tratamento e resposta terapêutica, controlando por idade, sexo ou comorbidades. Aqui, a regressão organiza a comparação entre grupos e ajuda a quantificar diferenças médias.\nEngenharia\nNa modelagem da resistência de materiais, regressões relacionam tensão aplicada e deformação observada, permitindo prever limites operacionais.\nEsportes\nPode-se modelar o desempenho de uma equipe em função de variáveis como investimento, tempo de posse de bola ou eficiência ofensiva, identificando padrões associados ao resultado final.\nPesca e aquicultura\nRelações entre esforço de pesca e biomassa capturada, ou entre tempo de cultivo e ganho de peso, podem ser analisadas por regressão para apoiar decisões produtivas.\nPolíticas públicas\nAvaliações de impacto utilizam regressão para investigar como programas sociais se associam a indicadores como renda, escolaridade ou emprego.\n\nEsses exemplos mostram que regressão não é apenas um instrumento matemático; é uma ferramenta de organização do raciocínio quantitativo em contextos reais.\n\n\n2.2.2.6 Potenciais e limites\nA regressão possui algumas virtudes que explicam sua ampla utilização:\n\nPermite quantificar efeitos médios;\nOferece interpretação relativamente direta dos coeficientes;\nEstrutura a análise de dados de forma sistemática;\nServe como base para extensões mais sofisticadas.\n\nAo mesmo tempo, é importante reconhecer que:\n\nA qualidade da conclusão depende da qualidade dos dados;\nA forma funcional escolhida influencia os resultados;\nModelos podem ser mal especificados;\nAssociação estatística não é automaticamente causalidade.\n\nReconhecer essas dimensões faz parte da maturidade estatística.\nA regressão é, portanto, uma ferramenta que conecta teoria, dados e decisão. Ela organiza a variabilidade observada em uma estrutura interpretável e mensurável.\nNo próximo capítulo, iniciaremos o estudo formal do Modelo de Regressão Linear Simples (MRLS), que constitui o ponto de partida para compreender, com rigor matemático, como essa estrutura é estimada e quais propriedades possui.\n\n\n\n\nCharnet, Reinaldo, Carlos Alberto Freire, Eliane M. R. Charnet, e Helio Bonvino. 2008. Análise de Modelos de Regressão Linear com Aplicações. 2º ed. Campinas: EDUNICAMP.\n\n\nDraper, Norman R., e Harry Smith. 1998. Applied Regression Analysis. 3º ed. New York: John Wiley & Sons.\n\n\nGiordano, Frank R., William P. Fox, e Steven B. Horton. 2013. A First Course in Mathematical Modeling. Cengage Learning.\n\n\nGujarati, Damodar N. 2006. Econometria Básica. 4º ed. Rio de Janeiro: Elsevier Campus.\n\n\nHoffmann, Rodolfo. 2006. Análise de Regressão: Uma Introdução à Econometria. 2º ed. São Paulo: Hucitec.\n\n\nKutner, Michael H., Christopher J. Nachtsheim, John Neter, e William Li. 2005. Applied Linear Statistical Models. 5º ed. New York: McGraw-Hill.\n\n\nMeerschaert, Mark M. 2013. Mathematical Modeling. 4º ed. Academic Press.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, e G. Geoffrey Vining. 2021. Introduction to Linear Regression Analysis. 6º ed. Hoboken: John Wiley & Sons.",
    "crumbs": [
      "Parte I — Modelagem",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelagem Estatística e Regressão</span>"
    ]
  },
  {
    "objectID": "part1_ex.html",
    "href": "part1_ex.html",
    "title": "3  Exercícios e atividades",
    "section": "",
    "text": "3.1 Exercícios conceituais\n\\[Y = f(X) + \\varepsilon.\\]\nAs respostas devem ser redigidas de forma argumentativa, conectando explicitamente os conceitos apresentados no capítulo.",
    "crumbs": [
      "Parte I — Modelagem",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exercícios e atividades</span>"
    ]
  },
  {
    "objectID": "part1_ex.html#exercícios-conceituais",
    "href": "part1_ex.html#exercícios-conceituais",
    "title": "3  Exercícios e atividades",
    "section": "",
    "text": "Ao transformar um problema do mundo real em um modelo matemático ou estatístico, é necessário definir variáveis e relações formais.\n\nO que significa “traduzir um problema real” em linguagem matemática?\n\nComo a escolha das variáveis influencia o tipo de resposta que o modelo pode oferecer?\n\nDê um exemplo em que a escolha inadequada das variáveis leve a conclusões limitadas ou equivocadas.\n\nConsidere um problema de crescimento populacional ao longo do tempo.\n\nExplique a diferença conceitual entre um modelo estático e um modelo dinâmico nesse contexto.\n\nPor que a inclusão explícita do tempo altera a estrutura matemática do modelo?\n\nEm muitos fenômenos reais, diferentes modelos podem ser propostos para descrever o mesmo problema.\n\nQuais critérios podem ser utilizados para escolher entre dois modelos concorrentes?\n\nExplique como o critério da parcimônia atua nesse processo de escolha.\n\nPor que modelos excessivamente complexos podem ser problemáticos, mesmo quando ajustam melhor os dados?\n\nExplique a afirmação: “um modelo não é a realidade; é uma aproximação útil”.\n\nO que significa equilíbrio entre realismo e simplicidade?\n\nPor que todo modelo envolve escolhas e simplificações?\n\nDê um exemplo em que um modelo necessariamente ignora parte da complexidade do fenômeno.\n\nConsidere as duas estruturas apresentadas no capítulo:\n\\[Y = 2X\\]\ne\n\\[Y = 2X + \\varepsilon.\\]\n\nDiferencie conceitualmente modelo determinístico e modelo estatístico.\n\nExplique o papel de \\(\\varepsilon\\) na segunda equação.\n\nListe três possíveis fontes para \\(\\varepsilon\\).\n\nPor que, no modelo estatístico, buscamos uma tendência média e não uma igualdade exata?\n\nO capítulo afirma que a regressão modela a média condicional:\n\\[E(Y \\mid X).\\]\n\nExplique o que significa média condicional em linguagem intuitiva.\n\nMostre como a ideia de \\(E(Y \\mid X)\\) está relacionada à decomposição\n\n\\[Y = \\text{sinal} + \\text{ruído}.\\]\n\nÉ possível que um valor observado de \\(Y\\) esteja distante de \\(E(Y \\mid X)\\) e, ainda assim, o modelo esteja adequado? Justifique.\n\nO texto apresenta duas formas gerais de modelagem:\n\\[Y = f(X) + \\varepsilon\\]\ne\n\\[Y = f(X)\\cdot \\varepsilon.\\]\n\nDiferencie erro aditivo e erro multiplicativo.\n\nEm que tipo de fenômeno o erro tende a ser proporcional ao nível médio?\n\nSobre o ciclo da modelagem:\n\nExplique por que o processo de modelagem é iterativo.\n\nDiferencie estimação, validação e análise de sensibilidade.\n\nDê um exemplo de situação em que, após ajustar o modelo, seria necessário voltar e modificar hipóteses ou forma funcional.\n\nO capítulo mostra que nem toda relação entre \\(X\\) e \\(Y\\) é linear.\n\nPor que correlação linear elevada não garante especificação correta do modelo?\n\nDê um exemplo conceitual de relação não linear em que o coeficiente de correlação de Pearson possa ser próximo de zero.\n\nO que significa dizer que há “erro sistemático ao longo de \\(X\\)”?\n\nSobre associação e causalidade:\n\n\nExplique a diferença entre associação estatística e causalidade.\n\nListe duas razões pelas quais um coeficiente estimado pode ser estatisticamente significativo e ainda não representar um efeito causal.\n\nQue tipo de informação adicional (teórica ou experimental) seria necessária para sustentar uma interpretação causal?\n\n\nSobre previsão e extrapolação:\n\n\nDiferencie interpolação e extrapolação.\n\nPor que extrapolar pode ser arriscado mesmo quando o ajuste parece bom no intervalo observado?\n\nDê um exemplo aplicado em que extrapolação seria particularmente problemática.\n\n\nEscolha um dos contextos aplicados citados no capítulo.\n\n\nDefina uma variável resposta \\(Y\\) e pelo menos três variáveis explicativas \\(X_1, X_2, X_3\\).\n\nIndique se o modelo conceitual seria mais plausivelmente aditivo ou multiplicativo e justifique.\n\nListe duas suposições que você consideraria críticas para interpretar os resultados.\n\n\nReflita sobre a seguinte estrutura geral:\n\n\n\nO que significa assumir que \\(E(\\varepsilon \\mid X) = 0\\)?\n\nPor que essa hipótese é central para interpretar os coeficientes como efeitos médios condicionais?\n\nO que pode acontecer se essa hipótese não for satisfeita?",
    "crumbs": [
      "Parte I — Modelagem",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exercícios e atividades</span>"
    ]
  },
  {
    "objectID": "part1_ex.html#atividade-de-simulação-e-regressão",
    "href": "part1_ex.html#atividade-de-simulação-e-regressão",
    "title": "3  Exercícios e atividades",
    "section": "3.2 Atividade de Simulação e Regressão",
    "text": "3.2 Atividade de Simulação e Regressão\nEsta atividade tem como objetivo consolidar os conceitos estudados ao longo do capítulo 2 por meio de simulações controladas. A proposta é investigar, de forma sistemática, como diferentes estruturas funcionais (linear, quadrática, exponencial e potência) se comportam sob ruído e como o ajuste por mínimos quadrados responde a essas situações.\nEm todos os exercícios:\n\nGere os dados conforme indicado no código.\nProduza os gráficos solicitados.\nAjuste os modelos especificados.\nResponda às questões de forma argumentativa, conectando os resultados aos conceitos de sinal, ruído, forma funcional e especificação do modelo.\n\nObjetivo: simular dados sob diferentes modelos, visualizar dispersões, calcular correlações e ajustar modelos via MQO (OLS), comparando a curva verdadeira que gerou os dados com o ajuste estimado.\n\nPreparação do Ambiente\n\n\n# Preparação do ambiente (simples e reprodutível)\nset.seed(0)\n\n\nSimular dados lineares simples\n\n\n# 2) Simular dados lineares simples\nn &lt;- 30\nx &lt;- seq(0, 10, length.out = n)\nbeta0 &lt;- 2\nbeta1 &lt;- 3\nsigma &lt;- 5\n\nsinal &lt;- beta0 + beta1*x\ny &lt;- sinal + rnorm(n, mean = 0, sd = sigma)\n\n# Visualização: pontos + reta verdadeira\nplot(x, y, pch = 19, xlab = \"X\", ylab = \"Y\",\n     main = \"Linear: sinal + ruído\")\nlines(x, sinal, col = \"red\", lwd = 2)\nlegend(\"topleft\", legend = c(\"dados\", \"reta verdadeira\"),\n       pch = c(19, NA), lty = c(NA, 1), col = c(\"black\", \"red\"), bty = \"n\")\n\n\n\n\n\n\n\n# Correlação\ncor_xy &lt;- cor(x, y)\ncor_xy\n\n[1] 0.8794497\n\n\n\nAjustar reta por MQO\n\n\n# 3) Ajustar reta por MQO (OLS)\nmod_lin &lt;- lm(y ~ x)\n\n# Resumo do ajuste\nsummary(mod_lin)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.6607 -2.8240 -0.1325  3.0743 11.4207 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.4090     1.6321   2.089   0.0459 *  \nx             2.7402     0.2803   9.777 1.58e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.582 on 28 degrees of freedom\nMultiple R-squared:  0.7734,    Adjusted R-squared:  0.7653 \nF-statistic: 95.58 on 1 and 28 DF,  p-value: 1.581e-10\n\n# Visualização: dados + reta verdadeira + reta ajustada\nplot(x, y, pch = 19, xlab = \"X\", ylab = \"Y\",\n     main = \"Linear: reta verdadeira vs MQO\")\nlines(x, sinal, col = \"red\", lwd = 2)\nlines(x, fitted(mod_lin), col = \"darkgreen\", lwd = 2, lty = 2)\n\nlegend(\"topleft\",\n       legend = c(\"dados\", \"reta verdadeira\", \"ajuste MQO\"),\n       pch = c(19, NA, NA),\n       lty = c(NA, 1, 2),\n       col = c(\"black\", \"red\", \"darkgreen\"),\n       bty = \"n\")\n\n\n\n\n\n\n\n\nPerguntas – linear a) A correlação está próxima de 1? Por quê?\nb) O coeficiente estimado da inclinação \\(\\beta_1\\) ficou próximo do valor verdadeiro?\nc) Experimente:\n- Aumente o ruído para rnorm(30, mean = 0, sd = 10) e veja o que acontece com a correlação e o ajuste.\n- Reduza o ruído para rnorm(30, mean = 0, sd = 2) e observe a diferença.\n\nModelo polinomial quadrático\n\n\n# 4) Modelo polinomial quadrático (sinal não linear)\nn &lt;- 30\nx &lt;- seq(-3, 3, length.out = n)\nsigma &lt;- 2\n\nsinal &lt;- 1 + 2*x - 0.05*x^2\ny &lt;- sinal + rnorm(n, mean = 0, sd = sigma)\n\n# Visualização: pontos + curva verdadeira\nplot(x, y, pch = 19, xlab = \"X\", ylab = \"Y\",\n     main = \"Quadrático: sinal + ruído\")\nlines(x, sinal, col = \"red\", lwd = 2)\nlegend(\"topleft\", legend = c(\"dados\", \"curva verdadeira\"),\n       pch = c(19, NA), lty = c(NA, 1), col = c(\"black\", \"red\"), bty = \"n\")\n\n\n\n\n\n\n\n# Correlação (linear) pode enganar em relação não linear\ncor(x, y)\n\n[1] 0.8480377\n\n\nPerguntas – Quadrático a) A correlação de Pearson reflete bem a relação entre x e y neste caso? é linear?\nb) Aumente o coeficiente do termo quadrático: troque -0.05 por -0.5. E agora?\nc) Altere o sinal do termo quadrático: use +0.5. Como fica a concavidade da curva?\nd) Aumente o ruído (ex.: sigma &lt;- 4). O que acontece com a correlação e a visualização da curva?\n\nModelo Exponencial\n\n\n# 5) Modelo exponencial com ruído multiplicativo\nn &lt;- 30\nx &lt;- seq(0, 3, length.out = n)\n\na &lt;- 2\nb &lt;- 0.2\nsigma_log &lt;- 0.2  # ruído no log (multiplicativo em Y)\n\nsinal &lt;- a * exp(b*x)\ny &lt;- sinal * exp(rnorm(n, mean = 0, sd = sigma_log))\n\n# Visualização\nplot(x, y, pch = 19, xlab = \"X\", ylab = \"Y\",\n     main = \"Exponencial: sinal + ruído multiplicativo\")\nlines(x, sinal, col = \"red\", lwd = 2)\nlegend(\"topleft\", legend = c(\"dados\", \"curva verdadeira\"),\n       pch = c(19, NA), lty = c(NA, 1), col = c(\"black\", \"red\"), bty = \"n\")\n\n\n\n\n\n\n\n# Correlação\ncor(x, y)\n\n[1] 0.8255609\n\n\nPerguntas – Exponencial a) A curva gerada com \\(b = 0.2\\) parece linear? A correlação confirma isso?\nb) Aumente o parâmetro de crescimento para \\(b = 0.8\\). A curva agora se afasta de uma reta?\nc) Aumente ainda mais para \\(b = 1.5\\). Como fica a forma da curva e a correlação de Pearson?\nd) Reduza o ruído (0.2 → 0.05). O que muda na dispersão e na clareza da forma exponencial?\n\nModelo Potência\n\n\n# 6) Modelo potência com ruído multiplicativo\nn &lt;- 30\nx &lt;- seq(0.5, 3, length.out = n)\n\nalpha &lt;- 1.5\nexpoente &lt;- 1.1\nsigma_log &lt;- 0.1\n\nsinal &lt;- alpha * (x^expoente)\ny &lt;- sinal * exp(rnorm(n, mean = 0, sd = sigma_log))\n\n# Visualização\nplot(x, y, pch = 19, xlab = \"X\", ylab = \"Y\",\n     main = \"Potência: sinal + ruído multiplicativo\")\nlines(x, sinal, col = \"red\", lwd = 2)\nlegend(\"topleft\", legend = c(\"dados\", \"curva verdadeira\"),\n       pch = c(19, NA), lty = c(NA, 1), col = c(\"black\", \"red\"), bty = \"n\")\n\n\n\n\n\n\n\n# Correlação\ncor(x, y)\n\n[1] 0.9769569\n\n\nPerguntas – Potência a) Com expoente \\(1.1\\), a curva parece linear? A correlação confirma isso?\nb) Aumente o expoente para \\(2.5\\). A curva agora se distancia de uma reta?\nc) Teste com um expoente ainda maior, por exemplo \\(3.5\\). O que muda na curvatura e na dispersão dos pontos?\nd) Dobre o ruído (0.1 → 0.2). O que acontece com a clareza da curva e com a correlação de Pearson?\n\nExponencial — linear vs. polinomial\n\n\n# 7) Exponencial — comparar ajuste linear vs polinomial (grau 2)\nn &lt;- 30\nx &lt;- seq(0, 3, length.out = n)\n\na &lt;- 2.0\nb &lt;- 0.2        # depois teste 0.8 e 1.5\nsigma_log &lt;- 0.1\n\nsinal &lt;- a * exp(b*x)\ny &lt;- sinal * exp(rnorm(n, mean = 0, sd = sigma_log))\n\n# Ajustes\nmod_lin &lt;- lm(y ~ x)\nmod_poly &lt;- lm(y ~ x + I(x^2))\n\n# Comparação visual\nplot(x, y, pch = 19, xlab = \"X\", ylab = \"Y\",\n     main = \"Exponencial: linear vs polinomial (2)\")\nlines(x, sinal, col = \"red\", lwd = 2)\nlines(x, fitted(mod_lin), lwd = 2, lty = 2)\nlines(x, fitted(mod_poly), lwd = 2, lty = 3)\n\nlegend(\"topleft\",\n       legend = c(\"dados\", \"verdade\", \"linear\", \"polinomial (2)\"),\n       pch = c(19, NA, NA, NA),\n       lty = c(NA, 1, 2, 3),\n       col = c(\"black\", \"red\", \"black\", \"black\"),\n       bty = \"n\")\n\n\n\n\n\n\n\n# R² (comparação rápida)\nc(R2_linear = summary(mod_lin)$r.squared,\n  R2_polinomial2 = summary(mod_poly)$r.squared)\n\n     R2_linear R2_polinomial2 \n     0.7751012      0.7760694 \n\n\nPerguntas — Exponencial (começando quase reta) a) Com b = 0.2, os ajustes linear e polinomial (2) parecem semelhantes? O R² confirma? b) Aumente b para 0.8 e depois 1.5. Como mudam o gráfico e os R²? Qual ajuste passa a representar melhor a curva? c) Reduza o ruído para sigma_log &lt;- 0.05. Fica mais fácil perceber a diferença entre o linear e o polinomial quando b é maior?\n\nPotência — linear vs. polinomial\n\n\n# 8) Potência — comparar ajuste linear vs polinomial (grau 2)\nn &lt;- 30\nx &lt;- seq(0.5, 3, length.out = n)\n\nalpha &lt;- 1.5\nexpoente &lt;- 1.1   # depois teste 2.5 e 3.5\nsigma_log &lt;- 0.1\n\nsinal &lt;- alpha * (x^expoente)\ny &lt;- sinal * exp(rnorm(n, mean = 0, sd = sigma_log))\n\n# Ajustes\nmod_lin &lt;- lm(y ~ x)\nmod_poly &lt;- lm(y ~ x + I(x^2))\n\n# Comparação visual\nplot(x, y, pch = 19, xlab = \"X\", ylab = \"Y\",\n     main = \"Potência: linear vs polinomial (2)\")\nlines(x, sinal, col = \"red\", lwd = 2)\nlines(x, fitted(mod_lin), lwd = 2, lty = 2)\nlines(x, fitted(mod_poly), lwd = 2, lty = 3)\n\nlegend(\"topleft\",\n       legend = c(\"dados\", \"verdade\", \"linear\", \"polinomial (2)\"),\n       pch = c(19, NA, NA, NA),\n       lty = c(NA, 1, 2, 3),\n       col = c(\"black\", \"red\", \"black\", \"black\"),\n       bty = \"n\")\n\n\n\n\n\n\n\n# R² (comparação rápida)\nc(R2_linear = summary(mod_lin)$r.squared,\n  R2_polinomial2 = summary(mod_poly)$r.squared)\n\n     R2_linear R2_polinomial2 \n     0.9422860      0.9487419 \n\n\nPerguntas — Potência (começando quase reta)\n\nCom expoente = 1.1, os ajustes linear e polinomial (2) parecem semelhantes? O R² confirma?\nAumente o expoente para 2.5 e depois 3.5. Como mudam o gráfico e os R²? Qual ajuste passa a representar melhor a curva?\nDobre o ruído para sigma_log &lt;- 0.2. Fica mais difícil perceber a diferença entre o linear e o polinomial quando o expoente é maior?",
    "crumbs": [
      "Parte I — Modelagem",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exercícios e atividades</span>"
    ]
  },
  {
    "objectID": "mrls.html",
    "href": "mrls.html",
    "title": "4  O MRLS como Modelo para a Média Condicional",
    "section": "",
    "text": "4.1 Formulação Matemática do MRLS\nA compreensão do Modelo de Regressão Linear Simples (MRLS) é essencial para o estudo dos modelos de regressão. Sua importância não se limita à simplicidade algébrica, mas repousa no fato de que ele estabelece as bases conceituais para toda a teoria de modelagem estatística. Ao assumir que a variação média de uma variável resposta \\(Y\\) pode ser explicada por uma única variável explicativa \\(X\\), o MRLS introduz a noção central de média condicional, isto é, a ideia de que existe uma estrutura determinística que organiza o comportamento médio dos dados, à qual se sobrepõe uma componente aleatória que representa o ruído inevitável das observações empíricas.\nEssa leitura “pela média condicional” é a forma mais precisa de entender o que a regressão linear simples afirma: para cada valor fixado de \\(X\\), existe uma distribuição de \\(Y\\), e o modelo especifica como a média dessa distribuição varia com \\(X\\). (ver Montgomery, Peck, e Vining (2021); Hoffmann (2016))\nO intercepto e a inclinação da reta de regressão apresentadas anteriormente traduzem a parte sistemática do fenômeno, enquanto o erro agrega fatores não observados, variações aleatórias ou imprecisões de medição. É nessa combinação entre regularidade e aleatoriedade que se encontra a força do modelo: a regressão linear simples oferece uma linguagem matemática capaz de quantificar associações e, ao mesmo tempo, de reconhecer que o mundo real não se comporta de maneira perfeitamente determinística.\nEm particular, o termo “erro” não deve ser lido como “falha”: ele representa a parcela de variabilidade de \\(Y\\) que permanece mesmo quando \\(X\\) é conhecido e o componente médio \\(E(Y\\mid X)\\) foi especificado. (ver Kutner et al. (2005))\nO MRLS pode ser usado em diferentes perspectivas. Em um primeiro plano, ele ajuda a compreender como uma variável se relaciona com outra, permitindo isolar a contribuição média de \\(X\\) sobre \\(Y\\). Em seguida, oferece meios de previsão, já que a reta ajustada pode ser utilizada para estimar valores futuros ou não observados de \\(Y\\). Finalmente, ele fornece um instrumento de controle, pois ao quantificar a variação esperada em \\(Y\\) para uma mudança em \\(X\\), torna-se possível avaliar de forma objetiva a influência de um fator específico mantendo os demais aspectos fixos ou controlados no desenho do estudo. Essa tríade, nomeadamente; explicação, predição e controle, sustenta a relevância prática do modelo e justifica sua centralidade tanto no ensino quanto na aplicação da estatística.\nUm cuidado conceitual importante é distinguir “prever o valor médio” de “prever uma observação individual”: mesmo que a média condicional seja bem descrita, observações individuais ainda variam ao redor dessa média por causa do erro aleatório. (ver Montgomery, Peck, e Vining (2021))\nA intuição do MRLS pode ser visualizada em gráficos de dispersão: os pontos \\((X,Y)\\) representam as observações empíricas e, sobre esse conjunto, a reta de regressão traduz a tendência média. As distâncias verticais entre cada ponto e a reta correspondem aos resíduos, isto é, às variações não capturadas pelo modelo. Adicionalmente, sabendo que a relação entre \\(X\\) e \\(Y\\) pode assumir diferentes intensidades e direções, mesmo dentro de um modelo linear simples, considere os quatro cenários a seguir, todos baseados em uma reta verdadeira perturbada por erros aleatórios:\nEssas quatro situações destacam a essência do MRLS: independentemente da direção ou da força da associação, o modelo parte da ideia de que a média condicional de \\(Y\\) pode ser descrita por uma função linear em \\(X\\), à qual se soma um ruído \\(\\varepsilon_i\\) com \\(E[\\varepsilon_i \\mid X_i]=0\\).\nVale enfatizar por que essa condição é conceitualmente importante: ela expressa que, uma vez fixado \\(X_i\\), o termo de erro não tem tendência sistemática (em média, não empurra \\(Y\\) para cima nem para baixo), de modo que toda a variação média de \\(Y\\) com \\(X\\) fica concentrada no termo \\(E(Y\\mid X)\\). Quando essa condição falha, o que se interpreta como “efeito de \\(X\\)” pode estar contaminado por fatores omitidos que variam com \\(X\\) (situação que, em econometria, está relacionada à ideia de endogeneidade). O gráfico, portanto, antecipa de forma intuitiva a formulação matemática que será detalhada na próxima seção.\nEssa representação gráfica simples, mas poderosa, revela a lógica do modelo: a regressão não busca explicar cada observação em particular, mas descrever o comportamento médio de \\(Y\\) em função de \\(X\\). É nesse sentido que ela constitui uma primeira aproximação, um alicerce sobre o qual se constroem modelos mais complexos. (ver Montgomery, Peck, e Vining (2021))\nA formulação do modelo de regressão linear simples parte da ideia de que cada observação \\(Y_i\\) pode ser decomposta em duas partes: uma componente sistemática, que expressa o valor médio de \\(Y\\) condicionado a um dado valor de \\(X_i\\), e uma componente aleatória, que traduz as variações não explicadas. Em notação formal, escrevemos\n\\[\nY_i = \\mu_i + \\varepsilon_i, \\quad \\text{com} \\quad \\mu_i = E[Y_i \\mid X_i],\n\\] em que \\(\\mu_i\\) é a média condicional de \\(Y\\) dado \\(X_i\\), e \\(\\varepsilon_i\\) é o erro aleatório associado à observação \\(i\\).\nPara que essa decomposição tenha interpretação estatística clara, é conveniente explicitar as suposições (ou hipóteses) que conectam o termo aleatório à componente sistemática. A hipótese central é que o erro, em média, não carrega informação adicional além de \\(X_i\\), de modo que\n\\[\nE[\\varepsilon_i \\mid X_i] = 0.\n\\] Essa condição de exogeneidade fraca garante que a parte sistemática do modelo seja, de fato, uma descrição da média condicional, e não uma mistura entre efeito sistemático e ruído (ver Gujarati (2006)). Um modo equivalente (e útil) de ler essa hipótese é: ao fixar \\(X_i\\), a média de \\(Y_i\\) é exatamente \\(\\mu_i\\), pois\n\\[\nE(Y_i\\mid X_i)=E(\\mu_i+\\varepsilon_i\\mid X_i)=\\mu_i+E(\\varepsilon_i\\mid X_i)=\\mu_i.\n\\] Além da condição de média nula, frequentemente se acrescenta uma hipótese sobre a dispersão do erro, que relaciona o modelo à variância condicional de \\(Y\\):\n\\[\nVar(\\varepsilon_i\\mid X_i)=\\sigma^2.\n\\] Por fim, para que as observações tragam informação “nova” umas em relação às outras e para que os resultados usuais de estimação e inferência sejam válidos, costuma-se assumir ausência de dependência linear entre erros de unidades distintas. Uma forma padrão de expressar isso é impor covariância nula entre erros diferentes:\n\\[\nCov(\\varepsilon_i,\\varepsilon_j\\mid X_i,X_j)=0,\\quad \\forall i\\neq j,\n\\]\nisto é, condicionando ao conjunto de regressores, os termos de erro não apresentam associação linear entre observações distintas. Em muitos textos, essa hipótese aparece na forma mais forte de independência entre os erros; a condição de covariância nula é a expressão mínima necessária para várias propriedades algébricas clássicas do modelo linear. (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021))\nSob essas suposições, podemos derivar propriedades imediatas do modelo:\n\\[\nE(Y_i\\mid X_i)=E(\\beta_0+\\beta_1X_i+\\varepsilon_i \\mid X_i)=\\beta_0+\\beta_1X_i.\n\\]\n\\[\nVar(Y_i\\mid X_i)=E\\left\\{\\left[Y_i-E(Y_i\\mid X_i)\\right]^2\\mid X_i\\right\\}=E(\\varepsilon_i^2\\mid X_i)=Var(\\varepsilon_i\\mid X_i)=\\sigma^2.\n\\]\nPara \\(i\\neq j\\), e condicionando ao conjunto de regressores), temos\n\\[\nCov(Y_i,Y_j\\mid X_i,X_j)=Cov(\\beta_0+\\beta_1X_i+\\varepsilon_i,\\ \\beta_0+\\beta_1X_j+\\varepsilon_j\\mid X_i,X_j)=Cov(\\varepsilon_i,\\varepsilon_j\\mid X_i,X_j).\n\\]\nAssim, sob a hipótese \\(Cov(\\varepsilon_i,\\varepsilon_j\\mid X_i,X_j)=0\\) para \\(i\\neq j\\), segue que\n\\[\nCov(Y_i,Y_j\\mid X_i,X_j)=0,\\quad i\\neq j.\n\\]\nA função de regressão do modelo é, portanto, a própria média condicional (ou média do componente sistemático):\n\\[\n\\mu(X_i;\\beta_0,\\beta_1)=E(Y_i\\mid X_i)=\\beta_0+\\beta_1X_i.\n\\]\nNo caso linear, supõe-se que a média condicional é uma função linear nos parâmetros, de forma que\n\\[\nE[Y_i \\mid X_i] = \\mu(X_i;\\beta_0,\\beta_1) = \\beta_0 + \\beta_1 X_i,\n\\]\no que leva à formulação completa do modelo:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\quad i=1,2,\\dots,n.\n\\] Uma vez especificada a função \\(\\mu(X_i;\\beta_0,\\beta_1)\\) e as hipóteses sobre \\(\\varepsilon_i\\), a tarefa estatística passa a ser estimar \\(E(Y_i\\mid X_i)\\) (isto é, a função de regressão) a partir dos dados, tipicamente via métodos como mínimos quadrados (MMQ) e, quando apropriado, máxima verossimilhança (MMV). (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021))",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>O MRLS como Modelo para a Média Condicional</span>"
    ]
  },
  {
    "objectID": "mrls.html#formulação-matemática-do-mrls",
    "href": "mrls.html#formulação-matemática-do-mrls",
    "title": "4  O MRLS como Modelo para a Média Condicional",
    "section": "",
    "text": "Média condicional\n\n\n\nVariância condicional\n\n\n\nCovariância condicional entre observações distintas",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>O MRLS como Modelo para a Média Condicional</span>"
    ]
  },
  {
    "objectID": "mrls.html#interpretação-dos-parâmetros-e-hipóteses-do-modelo",
    "href": "mrls.html#interpretação-dos-parâmetros-e-hipóteses-do-modelo",
    "title": "4  O MRLS como Modelo para a Média Condicional",
    "section": "4.2 Interpretação dos Parâmetros e Hipóteses do Modelo",
    "text": "4.2 Interpretação dos Parâmetros e Hipóteses do Modelo\nNessa estrutura,\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i,\n\\]\n\\(\\beta_0\\) representa o valor esperado de \\(Y\\) quando \\(X=0\\), isto é,\n\\[\nE(Y\\mid X=0)=\\beta_0,\n\\]\nenquanto \\(\\beta_1\\) indica a taxa média de variação de \\(Y\\) a cada incremento unitário em \\(X\\):\n\\[\nE(Y\\mid X=x+1)-E(Y\\mid X=x)=\\beta_1.\n\\]\nAssim, \\(\\beta_1\\) mede o efeito médio de uma variação unitária em \\(X\\) sobre a média condicional de \\(Y\\). O erro \\(\\varepsilon_i\\) traduz tanto a variabilidade natural dos fenômenos quanto fatores não observados, assumindo sempre que sua esperança condicional a \\(X_i\\) seja nula.\nÉ essencial notar que “\\(X=0\\)” pode não ter significado em alguns contextos; ainda assim, \\(\\beta_0\\) permanece necessário como parâmetro de localização da reta. Quando \\(0\\) não pertence ao intervalo observado de \\(X\\), ou quando não possui interpretação prática, pode-se redefinir a variável explicativa por centralização (por exemplo, \\(X_i^\\ast = X_i - \\bar X\\)), de modo que o novo intercepto represente o valor esperado de \\(Y\\) em um ponto de referência mais informativo. (ver Montgomery, Peck, e Vining (2021))",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>O MRLS como Modelo para a Média Condicional</span>"
    ]
  },
  {
    "objectID": "mrls.html#hipóteses-do-mrls",
    "href": "mrls.html#hipóteses-do-mrls",
    "title": "4  O MRLS como Modelo para a Média Condicional",
    "section": "4.3 Hipóteses do MRLS",
    "text": "4.3 Hipóteses do MRLS\nPara que o modelo tenha propriedades estatísticas bem definidas, é conveniente explicitar as hipóteses clássicas do MRLS, usualmente apresentadas na literatura de modelos lineares (ver Kutner et al. (2005)). Sob a formulação clássica, assumimos:\n\nLinearidade nos parâmetros\n\nA função de regressão é linear nos parâmetros:\n\\[\nE(Y_i\\mid X_i)=\\beta_0+\\beta_1X_i.\n\\]\nObserve que a linearidade refere-se aos parâmetros \\(\\beta_0,\\beta_1\\), e não necessariamente à variável \\(X\\) em si.\n\nValores de \\(X\\) fixos (ou condicionais)\n\nOs valores \\(X_i\\) são considerados fixos pelo planejamento do estudo, ou, alternativamente, a análise é conduzida condicionalmente aos valores observados de \\(X\\). Essa hipótese garante que toda a aleatoriedade do modelo esteja concentrada no termo de erro.\n\nErro com média zero\n\n\\[\nE(\\varepsilon_i\\mid X_i)=0.\n\\]\nEssa condição assegura que o componente sistemático do modelo coincide com a média condicional de \\(Y\\).\n\nHomoscedasticidade\n\nA variância do erro é constante para todos os valores de \\(X\\):\n\\[\nVar(\\varepsilon_i\\mid X_i)=\\sigma^2.\n\\]\nConsequentemente,\n\\[\nVar(Y_i\\mid X_i)=\\sigma^2.\n\\]\n\nAusência de correlação entre erros\n\nPara \\(i\\neq j\\),\n\\[\nCov(\\varepsilon_i,\\varepsilon_j\\mid X)=0.\n\\]\nSob essa hipótese, segue que\n\\[\nCov(Y_i,Y_j\\mid X)=0,\\quad \\forall i\\neq j.\n\\]\nEssas cinco condições compõem o conjunto clássico de hipóteses do modelo linear simples conforme apresentado em textos de modelos lineares aplicados (ver Kutner et al. (2005)).\n\n4.3.1 Hipóteses adicionais para inferência\nAté este ponto, não foi necessário supor nenhuma distribuição específica para os erros. As hipóteses acima são suficientes para garantir propriedades como não-viesamento dos estimadores de mínimos quadrados e expressões fechadas para suas variâncias.\nPara a construção de intervalos de confiança exatos e testes de hipóteses com distribuição conhecida em amostras finitas, acrescenta-se frequentemente a suposição de normalidade dos erros:\n\\[\n\\varepsilon_i \\sim N(0,\\sigma^2).\n\\]\nSob essa condição, o vetor de respostas possui distribuição normal multivariada condicional a \\(X\\), o que permite derivar resultados exatos para estatísticas \\(t\\) e \\(F\\).\nÉ conceitualmente importante distinguir:\n\nHipóteses do modelo básico: linearidade em \\(\\beta_0,\\beta_1\\) e \\(E(\\varepsilon\\mid X)=0\\);\nHipóteses para eficiência e inferência exata: homoscedasticidade, ausência de correlação e normalidade.\n\nEssa distinção é enfatizada na literatura de regressão aplicada, que separa claramente a estrutura do modelo da estrutura probabilística necessária para inferência (ver Weisberg (2005)).\nPor fim, essa formulação pode ser interpretada sob duas perspectivas equivalentes. Na abordagem clássica, \\(X\\) é tratado como fixo. Em contextos amostrais, pode-se admitir \\(X\\) aleatório, desde que se mantenha a condição\n\\[\nE(\\varepsilon_i\\mid X_i)=0,\n\\]\nque garante a validade das propriedades do modelo condicionalmente a \\(X\\). Em ambas as leituras, permanece a essência: a regressão linear simples é um modelo para a média condicional de \\(Y\\) dado \\(X\\), e não para cada observação individual. (ver Gujarati (2006))",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>O MRLS como Modelo para a Média Condicional</span>"
    ]
  },
  {
    "objectID": "mrls.html#representação-gráfica-e-intuição-geométrica",
    "href": "mrls.html#representação-gráfica-e-intuição-geométrica",
    "title": "4  O MRLS como Modelo para a Média Condicional",
    "section": "4.4 Representação Gráfica e Intuição Geométrica",
    "text": "4.4 Representação Gráfica e Intuição Geométrica\nA Figura a seguir ilustra o que significa afirmar que o MRLS é um modelo para a média condicional. Os pontos azuis representam as observações empíricas \\((X_i,Y_i)\\), que se espalham devido ao ruído aleatório \\(\\varepsilon_i\\). A reta vermelha mostra a estrutura determinística \\(\\beta_0 + \\beta_1 X\\), isto é, o valor esperado de \\(Y\\) para cada valor de \\(X\\). Já os círculos pretos conectados indicam médias locais de \\(Y\\) em diferentes intervalos de \\(X\\), funcionando como uma aproximação empírica de \\(E[Y \\mid X]\\). O alinhamento dessas médias com a reta reforça a ideia de que o modelo busca descrever a tendência média e não cada observação individual.\nEste gráfico torna visível a condição fundamental \\(E[\\varepsilon_i \\mid X_i]=0\\). Embora cada ponto esteja sujeito a variações não explicadas, quando olhamos para a média em cada faixa de \\(X\\), os erros se compensam e a estrutura linear emerge. Assim, é possível notar intuitivamente por que se fala em “média condicional” e compreendemos que a regressão não elimina o ruído, mas organiza o comportamento médio das observações em torno de uma reta. (ver Charnet et al. (2008))\nMais adiante, quando abordarmos o método dos mínimos quadrados ordinários, introduziremos outra visualização complementar, na qual os resíduos aparecem como segmentos verticais entre os pontos observados e a reta ajustada. Essas representações reforçam a interpretação fundamental: o MRLS não pretende capturar cada realização individual, mas descrever a tendência média de \\(Y\\) em função de \\(X\\), admitindo explicitamente a presença de ruído.\n\n\n\n\n\nMRLS como média condicional: pontos observados (X,Y), reta verdadeira (média condicional) e médias locais de Y por faixas de X.\n\n\n\n\n\n\n\n\nCharnet, Reinaldo, Carlos Alberto Freire, Eliane M. R. Charnet, e Helio Bonvino. 2008. Análise de Modelos de Regressão Linear com Aplicações. 2º ed. Campinas: EDUNICAMP.\n\n\nGujarati, Damodar N. 2006. Econometria Básica. 4º ed. Rio de Janeiro: Elsevier Campus.\n\n\nHoffmann, Rodolfo. 2016. Análise de Regressão: Uma Introdução à Econometria. 5º ed. Portal de Livros Abertos da USP. https://doi.org/10.11606/9788592105709.\n\n\nKutner, Michael H., Christopher J. Nachtsheim, John Neter, e William Li. 2005. Applied Linear Statistical Models. 5º ed. New York: McGraw-Hill.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, e G. Geoffrey Vining. 2021. Introduction to Linear Regression Analysis. 6º ed. Hoboken: John Wiley & Sons.\n\n\nWeisberg, Sanford. 2005. Applied Linear Regression. New York: Wiley.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>O MRLS como Modelo para a Média Condicional</span>"
    ]
  },
  {
    "objectID": "mrls_emq.html",
    "href": "mrls_emq.html",
    "title": "5  Estimação por Mínimos Quadrados no MRLS",
    "section": "",
    "text": "5.1 Paradigmas de Estimação no MRLS\nA formulação do Modelo de Regressão Linear Simples (MRLS), discutida anteriormente, descreve a estrutura da média condicional de \\(Y\\) em função de \\(X\\), isto é,\n\\[\nE(Y_i \\mid X_i) = \\beta_0 + \\beta_1 X_i.\n\\]\nO desafio agora é estimar os parâmetros desconhecidos \\(\\beta_0\\) e \\(\\beta_1\\) a partir de dados observados \\(\\{(X_i,Y_i)\\}_{i=1}^n\\). Esse processo de estimação pode ser realizado via diferentes métodos, cada um apoiado em princípios e hipóteses próprias.\nA estimação pode ser conduzida sob diferentes paradigmas, isto é, diferentes princípios fundamentais que definem o que significa “estimar bem” um parâmetro. Esses paradigmas não diferem apenas em técnica, mas em filosofia estatística e nas hipóteses assumidas sobre o modelo.\nO Método dos Mínimos Quadrados Ordinários (MQO) é a abordagem clássica no contexto da regressão linear. Seu princípio é puramente geométrico e algébrico: escolher \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) de modo a minimizar a soma dos quadrados dos resíduos,\n\\[\nS(\\beta_0,\\beta_1) = \\sum_{i=1}^n \\left[Y_i - (\\beta_0 + \\beta_1 X_i)\\right]^2.\n\\]\nEsse critério não exige, para a obtenção dos estimadores, a especificação de uma distribuição para os erros. A minimização conduz a um sistema de equações conhecido como equações normais, que caracteriza a solução de mínimos quadrados (ver Draper e Smith (1998); Montgomery, Peck, e Vining (2021)). A ausência de suposição distributiva mostra que o MQO é, antes de tudo, um procedimento de ajuste determinístico baseado na estrutura linear do modelo.\nDo ponto de vista estatístico, sob as hipóteses já apresentadas, a saber, linearidade nos parâmetros, \\(E(\\varepsilon_i\\mid X_i)=0\\), homoscedasticidade e ausência de correlação entre erros os estimadores de MQO possuem propriedades fundamentais como não viés e variâncias com forma explícita. Essas propriedades não dependem da normalidade dos erros; a normalidade é necessária apenas quando se desejam distribuições exatas em amostras finitas para testes e intervalos de confiança (ver Kutner et al. (2005)). Assim, o MQO é um método de estimação que se apoia primariamente na estrutura do modelo médio e nas condições de regularidade, e não em hipóteses distributivas fortes.\nOutro caminho é o Método da Máxima Verossimilhança (MV). Nesse paradigma, parte-se da especificação completa da distribuição condicional de \\(Y_i\\mid X_i\\), frequentemente assumindo\n\\[\n\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\n\\]\no que implica que \\(Y_i\\mid X_i\\) também segue distribuição normal com média \\(\\mu_i=\\beta_0+\\beta_1X_i\\) e variância \\(\\sigma^2\\). Os estimadores são então definidos como aqueles que maximizam a função de verossimilhança, isto é, a probabilidade conjunta dos dados observados vista como função dos parâmetros. Quando o modelo probabilístico está corretamente especificado, a MV produz estimadores consistentes, assintoticamente normais e eficientes sob condições regulares (ver Casella e Berger (2002)).\nNo caso particular do modelo linear com erros normais homoscedásticos e não correlacionados, os estimadores de máxima verossimilhança coincidem com os estimadores de mínimos quadrados. Essa coincidência não é acidental: a minimização da soma de quadrados é equivalente à maximização da verossimilhança normal. Contudo, conceitualmente, os dois métodos partem de princípios distintos, um geométrico/algebraico e outro probabilístico.\nUma terceira alternativa são os métodos bayesianos, nos quais os parâmetros \\(\\beta_0\\) e \\(\\beta_1\\) são tratados como variáveis aleatórias. Nesse caso, especifica-se uma distribuição a priori conjunta para \\(\\beta_0\\) e \\(\\beta_1\\) e combina-se essa informação com a verossimilhança dos dados por meio do Teorema de Bayes, obtendo-se a distribuição a posteriori\n\\[\np(\\beta_1,\\beta_2 \\mid y,X) \\propto p(y,X \\mid \\beta_1,\\beta_2)\\, p(\\beta_1,\\beta_2).\n\\]\nA estimação passa então a ser baseada em características dessa distribuição a posteriori (como média, mediana ou moda). Esse paradigma explicita a incerteza sobre os parâmetros e permite incorporar informação prévia de forma formal (ver Casella e Berger (2002); Gelman et al. (2014)).\nPortanto, a estimação no MRLS pode ser conduzida sob diferentes paradigmas: minimização de resíduos (MQO), maximização da verossimilhança (MV) ou atualização bayesiana de crenças. Cada abordagem parte de fundamentos conceituais distintos, tais quais, geométrico-algébrico, probabilístico ou epistemológico, e conduz a interpretações próprias dos parâmetros e da incerteza associada.\nNeste livro, a estimação por mínimos quadrados ordinários (MQO) receberá tratamento mais detalhado e sistemático. A razão é dupla: em primeiro lugar, o MQO não exige a especificação de uma distribuição para os erros para a obtenção dos estimadores, apoiando-se apenas na estrutura do modelo médio e nas hipóteses clássicas de exogeneidade e regularidade; em segundo lugar, ele constitui a base do Teorema de Gauss–Markov e de grande parte da teoria dos modelos lineares, servindo como alicerce conceitual para extensões posteriores.\nA máxima verossimilhança (MV) também será contemplada, sobretudo quando discutirmos aspectos inferenciais e conexões entre estrutura probabilística e eficiência assintótica. No caso do modelo linear com erros normais, veremos inclusive a coincidência formal entre MQO e MV, o que reforça a unidade conceitual entre os métodos sob hipóteses adicionais.\nPor outro lado, embora o paradigma bayesiano seja conceitualmente relevante e metodologicamente poderoso, sua abordagem completa exigiria o desenvolvimento de ferramentas próprias, como escolha de distribuições a priori, análise da posteriori e métodos computacionais, que extrapolam os objetivos centrais deste texto. Assim, ele será mencionado para fins de contextualização, mas não será desenvolvido formalmente neste livro.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimação por Mínimos Quadrados no MRLS</span>"
    ]
  },
  {
    "objectID": "mrls_emq.html#o-critério-dos-mínimos-quadrados-ordinários",
    "href": "mrls_emq.html#o-critério-dos-mínimos-quadrados-ordinários",
    "title": "5  Estimação por Mínimos Quadrados no MRLS",
    "section": "5.2 O Critério dos Mínimos Quadrados Ordinários",
    "text": "5.2 O Critério dos Mínimos Quadrados Ordinários\nO Método dos Mínimos Quadrados Ordinários (MQO) é a abordagem clássica para a estimação em regressão linear. Seu objetivo é encontrar a reta que melhor descreve a relação média entre a variável resposta \\(Y\\) e a variável explicativa \\(X\\). Essa “melhor” reta é definida como aquela que minimiza a soma dos quadrados dos resíduos, isto é, das diferenças entre os valores observados e os valores ajustados pelo modelo (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).\nSe denotarmos por \\(\\hat{Y}_i\\) o valor ajustado para a observação \\(i\\), temos:\n\\[\n\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i,\n\\]\ne o resíduo correspondente é\n\\[\ne_i = Y_i - \\hat{Y}_i.\n\\] O critério de mínimos quadrados escolhe os parâmetros que minimizam a função de perda quadrática\n\\[\nS(\\beta_0, \\beta_1) = \\sum_{i=1}^n \\left[ Y_i - (\\beta_0 + \\beta_1 X_i) \\right]^2.\n\\]\nDo ponto de vista matemático, trata-se de um problema clássico de otimização: encontrar \\((\\hat\\beta_0,\\hat\\beta_1)\\) que minimizam \\(S(\\beta_0,\\beta_1)\\) sobre \\(\\mathbb{R}^2\\). A condição de primeira ordem leva a um sistema de duas equações lineares nas incógnitas \\(\\beta_0\\) e \\(\\beta_1\\), conhecido como equações normais. Essas equações caracterizam completamente a solução de mínimos quadrados no modelo linear simples (ver Draper e Smith (1998)).\nEsse procedimento garante que, entre todas as retas possíveis, a escolhida é aquela que deixa os resíduos, em conjunto, “o mais curtos possível” no sentido quadrático. A escolha da penalização quadrática não é arbitrária: a função objetivo é uma função polinomial de segundo grau nos parâmetros, contínua e diferenciável, e admite solução única sempre que os valores de \\(X\\) não forem todos iguais, isto é, sempre que houver variabilidade na variável explicativa. Essa condição assegura a existência e a unicidade da reta de mínimos quadrados.\nAlém disso, a penalização pelo quadrado dos desvios atribui maior peso a observações mais afastadas, o que explica tanto a eficiência do método sob hipóteses clássicas quanto sua sensibilidade a valores discrepantes (ver Weisberg (2005)).\n\n\n\n\n\nMQO: reta ajustada e resíduos destacados.\n\n\n\n\nA figura acima ilustra essa lógica. Os pontos azuis representam as observações \\((X_i,Y_i)\\), a reta vermelha mostra a reta ajustada pelo MQO, e as linhas tracejadas cinzas indicam os resíduos associados a cada ponto. Visualmente, o MQO busca a reta que minimiza a soma dos quadrados dessas distâncias verticais. Essa interpretação geométrica ajuda a compreender que a regressão não elimina o erro, mas organiza o ruído de forma a recuperar a estrutura média do fenômeno.\nEssa formulação admite duas interpretações complementares.\n\nGeométrica: o MQO pode ser visto como a projeção ortogonal do vetor de respostas \\(\\mathbf{Y}=(Y_1,\\ldots,Y_n)'\\) no subespaço gerado pelos vetores \\(\\mathbf{1}=(1,\\ldots,1)'\\) e \\(\\mathbf{X}=(X_1,\\ldots,X_n)'\\). A condição de minimização implica que o vetor de resíduos \\(\\hat\\varepsilon = Y-\\hat Y\\) é ortogonal ao espaço gerado pelos regressores, isto é,\n\n\\[\n\\sum_{i=1}^n \\hat\\varepsilon_i = 0\n\\quad \\text{e} \\quad\n\\sum_{i=1}^n X_i \\hat\\varepsilon_i = 0.\n\\]\nEssas duas condições são precisamente as equações normais no caso simples.\n\nEstatística: a ortogonalidade amostral dos resíduos aos regressores é o análogo empírico da hipótese populacional\n\n\\[\nE(\\varepsilon_i \\mid X_i)=0.\n\\] Em outras palavras, após o ajuste, não resta componente linear em \\(X\\) capaz de explicar sistematicamente os resíduos. A condição populacional de exogeneidade é refletida, no nível amostral, pela ortogonalidade dos resíduos estimados (ver Kutner et al. (2005)).\nUm aspecto central é que o MQO não exige, para a obtenção dos estimadores, a especificação de uma distribuição para os erros. Sob as hipóteses de média condicional corretamente especificada, homoscedasticidade e ausência de correlação entre erros, os estimadores resultantes são não viesados e apresentam variâncias com forma explícita, propriedades que independem da normalidade (ver Montgomery, Peck, e Vining (2021)).\nA normalidade é introduzida apenas quando se desejam distribuições exatas em amostras finitas para estatísticas de teste e construção de intervalos de confiança. Em contextos práticos com caudas pesadas ou observações discrepantes, podem ser considerados métodos robustos ou funções de perda alternativas. Essa generalidade explica por que o MQO constitui o ponto de partida natural e o método mais amplamente ensinado e utilizado na análise de regressão linear.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimação por Mínimos Quadrados no MRLS</span>"
    ]
  },
  {
    "objectID": "mrls_emq.html#solução-analítica-a-reta-de-regressão-por-mqo",
    "href": "mrls_emq.html#solução-analítica-a-reta-de-regressão-por-mqo",
    "title": "5  Estimação por Mínimos Quadrados no MRLS",
    "section": "5.3 Solução Analítica: A Reta de Regressão por MQO",
    "text": "5.3 Solução Analítica: A Reta de Regressão por MQO\n\n\n\n\n\n\nImportanteTeorema — Reta de Regressão por MQO\n\n\n\nNo modelo de regressão linear simples\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\quad i = 1,2,\\dots,n,\n\\]\nos estimadores de mínimos quadrados ordinários (MQO) de \\(\\beta_0\\) e \\(\\beta_1\\) são obtidos como aqueles que minimizam a soma dos quadrados dos resíduos. A solução do problema de minimização leva às formas fechadas:\n\\[\n\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}},\n\\qquad\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X},\n\\]\nonde\n\\[\nS_{xx}=\\sum_{i=1}^n (X_i-\\bar X)^2,\n\\qquad\nS_{xy}=\\sum_{i=1}^n (X_i-\\bar X)(Y_i-\\bar Y).\n\\]\n\n\nA demonstração resulta da minimização da função \\[\nS(\\beta_0,\\beta_1)=\\sum_{i=1}^n \\left[Y_i-(\\beta_0+\\beta_1X_i)\\right]^2,\n\\]\npor meio do cálculo das derivadas parciais em relação a \\(\\beta_0\\) e \\(\\beta_1\\) e da resolução do sistema de equações normais correspondente. A dedução algébrica completa pode ser consultada no Apêndice de Demonstrações {#demo}.\nEsse resultado estabelece a reta de regressão por MQO como a linha que, ao mesmo tempo, minimiza a soma dos quadrados dos resíduos e traduz o padrão médio de associação entre \\(X\\) e \\(Y\\). A estrutura explícita das soluções mostra que a existência e a unicidade dependem apenas de \\(S_{xx}&gt;0\\), isto é, da presença de variabilidade em \\(X\\), condição necessária para que a informação sobre a inclinação seja identificável (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).\nDo ponto de vista matemático, a minimização de \\(S(\\beta_0,\\beta_1)\\) conduz a uma função quadrática estritamente convexa nos parâmetros quando \\(S_{xx}&gt;0\\), assegurando que a solução encontrada pelas equações normais seja única. A demonstração detalhada dessa propriedade pode ser consultada no Apêndice de Demonstrações {#demo}.\nNo entanto, conhecer a forma explícita da reta ajustada é apenas o primeiro passo. A expressão fechada dos estimadores revela como eles dependem das quantidades amostrais, mas não informa, por si só, se tais estimadores são centrados nos verdadeiros parâmetros, quão precisos são ou como se comportam sob repetição amostral. Para que possamos confiar nesses estimadores e utilizá-los em inferência estatística, precisamos examinar suas propriedades probabilísticas: não viés, variâncias, covariância entre \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) e qualidade das predições produzidas. É justamente esse o foco da próxima seção (ver Kutner et al. (2005)).\n\n5.3.1 Interpretação dos Estimadores obtidos via MQO\n\nO estimador da inclinação pode ser reescrito como\n\n\\[\n\\hat\\beta_1 = \\frac{\\sum_{i=1}^n (X_i-\\bar X)(Y_i-\\bar Y)}\n{\\sum_{i=1}^n (X_i-\\bar X)^2},\n\\]\no que evidencia que ele corresponde à covariância amostral entre \\(X\\) e \\(Y\\) dividida pela variância amostral de \\(X\\). Essa forma deixa claro que \\(\\hat\\beta_1\\) mede a variação média de \\(Y\\) associada a um aumento unitário em \\(X\\), sendo proporcional ao grau de associação linear entre as duas variáveis (ver Montgomery, Peck, e Vining (2021)).\n\nO estimador do intercepto,\n\n\\[\n\\hat\\beta_0 = \\bar Y - \\hat\\beta_1 \\bar X,\n\\]\nimplica que a reta ajustada satisfaz\n\\[\n\\hat Y(\\bar X)=\\bar Y,\n\\]\nou seja, a reta de regressão passa necessariamente pelo ponto médio amostral \\((\\bar X,\\bar Y)\\). Essa propriedade decorre diretamente das equações normais e da ortogonalidade dos resíduos aos regressores (ver Kutner et al. (2005)).",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimação por Mínimos Quadrados no MRLS</span>"
    ]
  },
  {
    "objectID": "mrls_emq.html#propriedades-probabilísticas-dos-estimadores-de-mqo",
    "href": "mrls_emq.html#propriedades-probabilísticas-dos-estimadores-de-mqo",
    "title": "5  Estimação por Mínimos Quadrados no MRLS",
    "section": "5.4 Propriedades Probabilísticas dos Estimadores de MQO",
    "text": "5.4 Propriedades Probabilísticas dos Estimadores de MQO\nNesta seção reunimos as propriedades essenciais dos estimadores de mínimos quadrados no modelo\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\quad i = 1,2,\\dots,n,\n\\] sob as hipóteses usuais de exogeneidade fraca e regularidade: \\[\nE[\\varepsilon_i \\mid X_i]=0,\n\\qquad\nVar(\\varepsilon_i\\mid X_i)=\\sigma^2,\n\\qquad\nCov(\\varepsilon_i,\\varepsilon_j\\mid X_i,X_j)=0 \\ \\forall(i \\neq j),\n\\]\ncom\n\\[\nS_{xx}=\\sum_{i=1}^n (X_i-\\bar X)^2&gt;0.\n\\]\nEssas condições são suficientes para estabelecer as principais propriedades dos estimadores de MQO, sem necessidade de assumir normalidade dos erros. Trata-se exatamente do conjunto de hipóteses sob o qual se desenvolve a teoria clássica do modelo linear (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).\nOs estimadores de mínimos quadrados ordinários (MQO)\n\\[\n\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}},\n\\qquad\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X},\n\\]\nem que\n\\[\nS_{xy}=\\sum_{i=1}^n (X_i-\\bar X)(Y_i-\\bar Y),\n\\qquad\n\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i,\n\\qquad\n\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i,\n\\] possuem propriedades importantes, que garantem sua validade para inferência estatística.\n\n5.4.1 Não viés\nAmbos os estimadores são não viesados:\n\\[\nE[\\hat{\\beta}_0] = \\beta_0\n\\qquad \\text{e} \\qquad\nE[\\hat{\\beta}_1] = \\beta_1.\n\\] Um estimador é dito não viesado quando sua esperança coincide com o parâmetro verdadeiro. No presente caso, os estimadores \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) são centrados em \\(\\beta_0\\) e \\(\\beta_1\\), respectivamente. Em termos frequentistas, isso significa que, sob repetição hipotética do processo amostral nas mesmas condições, a média das estimativas convergiria para os valores verdadeiros.\nA demonstração formal desse resultado baseia-se na linearidade do operador esperança e na hipótese de exogeneidade fraca \\(E[\\varepsilon_i\\mid X_i]=0\\), e pode ser consultada no Apêndice de Demonstrações {#demo}. Conceitualmente, o ponto central é que, ao condicionar em \\(X\\), o erro não contém componente sistemática capaz de deslocar, em média, os estimadores.\n\n\n5.4.2 Variâncias e covariância dos estimadores\nAs variâncias dos estimadores são dadas por\n\\[\nVar(\\hat{\\beta}_0) =  \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{S_{xx}}\\right)\\sigma^2,\n\\qquad\nVar(\\hat{\\beta}_1) = \\frac{1}{S_{xx}}\\sigma^2,\n\\]\ne a covariância entre eles é\n\\[\nCov(\\hat{\\beta}_0,\\hat{\\beta}_1) = - \\frac{\\bar{X}}{S_{xx}}\\sigma^2.\n\\]\nEssas expressões decorrem diretamente da representação linear dos estimadores em função dos \\(Y_i\\) e das hipóteses sobre a estrutura de variância-covariância dos erros. A demonstração detalhada também pode ser vista no Apêndice de Demonstrações {#demo} (ver Kutner et al. (2005)).\nAlgumas interpretações conceituais importantes emergem dessas fórmulas:\n\nInfluência da dispersão de \\(X\\): quanto maior \\(S_{xx}\\), menor \\(Var(\\hat{\\beta}_1)\\). Portanto, amostras com maior variabilidade em \\(X\\) contêm mais informação sobre a inclinação da reta. Se os valores de \\(X\\) estiverem muito concentrados, a estimativa da inclinação torna-se imprecisa.\nDependência do intercepto em relação à origem: a variância de \\(\\hat{\\beta}_0\\) depende de \\(\\bar X\\). Quanto mais distante a média de \\(X\\) estiver da origem, maior será a variância do intercepto, refletindo o fato de que \\(\\hat\\beta_0\\) é obtido por extrapolação da reta até \\(X=0\\).\nCovariância negativa: quando \\(\\bar X&gt;0\\), a covariância entre \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) é negativa. Isso indica que uma estimativa maior da inclinação tende a ser compensada por uma estimativa menor do intercepto, preservando a propriedade geométrica de que a reta ajustada passa por \\((\\bar X,\\bar Y)\\).\n\nDo ponto de vista geométrico, essas propriedades decorrem da ortogonalidade dos resíduos aos regressores, isto é,\n\\[\n\\sum_{i=1}^n \\hat\\varepsilon_i = 0\n\\qquad \\text{e} \\qquad\n\\sum_{i=1}^n X_i \\hat\\varepsilon_i = 0.\n\\]\nEssas condições são equivalentes às equações normais e garantem que a projeção de \\(Y\\) sobre o subespaço gerado por \\(1\\) e \\(X\\) seja ortogonal ao vetor de resíduos. A conexão entre ortogonalidade e estrutura de variâncias é discutida em textos clássicos de regressão linear (ver Montgomery, Peck, e Vining (2021)).\n\n\n5.4.3 Estimativa de \\(\\sigma^2\\) (graus de liberdade e não viés)\nDefinindo a soma dos quadrados dos resíduos como\n\\[\nSQRes = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2,\n\\]\ntemos que\n\\[\nE[SQRes] = (n-2)\\sigma^2.\n\\] A demonstração desse resultado utiliza a decomposição ortogonal do vetor \\(Y\\) em componente ajustada e componente residual, podendo ser consultada no Apêndice de Demonstrações {#demo}.\nAssim, o estimador\n\\[\ns^2 = \\frac{SQRes}{n-2}\n\\]\né não viesado para a variância dos erros:\n\\[\nE[s^2] = \\sigma^2.\n\\] Os dois graus de liberdade subtraídos refletem a estimação dos dois parâmetros do modelo \\((\\beta_0, \\beta_1)\\). Essa correção garante que a variabilidade residual não seja subestimada pelo fato de termos ajustado uma reta aos dados.\nNa prática, substitui-se \\(\\sigma^2\\) por \\(s^2\\) nas expressões de \\(Var(\\hat\\beta_0)\\) e \\(Var(\\hat\\beta_1)\\), obtendo-se estimativas dos erros-padrão. Observe que até aqui não foi necessária a suposição de normalidade: as propriedades de não viés e as fórmulas de variância decorrem apenas das hipóteses de média zero, homoscedasticidade e ausência de correlação entre erros (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).\nPortanto, os estimadores de MQO no MRLS apresentam um conjunto de propriedades fundamentais: são não viesados, possuem variâncias explicitamente caracterizadas, exibem covariância estrutural negativa entre intercepto e inclinação e permitem a construção de um estimador não viesado de \\(\\sigma^2\\) a partir dos resíduos.\nEssas características asseguram a solidez probabilística do método sob hipóteses relativamente gerais e preparam o terreno para a próxima questão natural: dentro da classe dos estimadores lineares não viesados, seria possível obter variâncias menores? O Teorema de Gauss–Markov responde negativamente a essa pergunta, estabelecendo a eficiência relativa do MQO.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimação por Mínimos Quadrados no MRLS</span>"
    ]
  },
  {
    "objectID": "mrls_emq.html#teorema-de-gaussmarkov",
    "href": "mrls_emq.html#teorema-de-gaussmarkov",
    "title": "5  Estimação por Mínimos Quadrados no MRLS",
    "section": "5.5 Teorema de Gauss–Markov",
    "text": "5.5 Teorema de Gauss–Markov\n\n\n\n\n\n\nImportanteTeorema (Gauss–Markov)\n\n\n\nNo modelo de regressão linear simples\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\quad i=1,2,\\dots,n,\n\\] sob as hipóteses\n\\[\nE[\\varepsilon_i\\mid X_i]=0,\n\\qquad\nVar(\\varepsilon_i\\mid X_i)=\\sigma^2,\n\\qquad\nCov(\\varepsilon_i,\\varepsilon_j\\mid X_i,X_j)=0 \\ (\\forall i\\neq j),\n\\]\nos estimadores de mínimos quadrados ordinários são lineares em \\(Y\\), não viesados e possuem variância mínima dentro da classe de todos os estimadores lineares não viesados dos parâmetros \\(\\beta_0\\) e \\(\\beta_1\\).\n\n\nEm outras palavras, se restringirmos nossa atenção a estimadores que sejam combinações lineares das observações \\(Y_i\\) e que sejam não viesados para os parâmetros verdadeiros, então nenhum outro estimador dessa classe terá variância menor que a dos estimadores de MQO. Essa é a essência do qualificativo best: não significa “melhor entre todos os estimadores possíveis”, mas “melhor dentro da classe dos estimadores lineares não viesados”.\nEste teorema organiza três ideias fundamentais:\n\nLinearidade do estimador: o estimador pode ser escrito como combinação linear das respostas observadas.\n\nNão viés: sua esperança coincide com o parâmetro verdadeiro.\n\nEficiência relativa: entre todos os estimadores que satisfazem (1) e (2), o MQO apresenta a menor variância.\n\nO resultado não depende da normalidade dos erros. Essa é uma distinção crucial: a normalidade é necessária apenas quando se deseja obter distribuições exatas finitas para estatísticas. A propriedade BLUE decorre exclusivamente da estrutura de média e variância do modelo linear clássico (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).\nGeometricamente, o teorema está intimamente ligado à interpretação do MQO como projeção ortogonal do vetor \\(Y\\) no subespaço gerado pelos regressores. A projeção ortogonal é, por construção, o vetor ajustado que minimiza a distância quadrática a \\(Y\\). A minimização da distância quadrática no espaço amostral se traduz, no plano probabilístico, em minimização da variância entre estimadores lineares não viesados. Essa ponte entre geometria e probabilidade é um dos aspectos mais profundos do modelo linear.\nÉ importante enfatizar também o alcance do resultado. O teorema não afirma que o MQO é o estimador de menor variância entre todos os estimadores imagináveis. Métodos não lineares ou estimadores viesados podem, em certos contextos, apresentar menor erro quadrático médio. O que o Teorema de Gauss–Markov garante é a otimalidade dentro da classe linear não viesada, uma classe ampla e natural no contexto da regressão.\nA demonstração formal do teorema, baseada em argumentos de decomposição de variância e ortogonalidade, pode ser consultada no Apêndice de Demonstrações {#demo}.\nEm termos práticos, o teorema fornece a base teórica que sustenta o uso do MQO como método padrão de estimação em regressão linear. Ele mostra que, sob hipóteses relativamente fracas e sem necessidade de normalidade, o procedimento adotado é eficiente dentro de uma classe ampla de estimadores. Essa combinação de simplicidade algébrica, interpretação geométrica clara e fundamentação probabilística sólida explica por que o MQO ocupa posição central na estatística aplicada e na econometria.\n\n\n\n\nCasella, George, e Roger L. Berger. 2002. Statistical Inference. 2º ed. Pacific Grove: Duxbury.\n\n\nDraper, Norman R., e Harry Smith. 1998. Applied Regression Analysis. 3º ed. New York: John Wiley & Sons.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, e Donald B. Rubin. 2014. Bayesian Data Analysis. 3º ed. CRC Press.\n\n\nKutner, Michael H., Christopher J. Nachtsheim, John Neter, e William Li. 2005. Applied Linear Statistical Models. 5º ed. New York: McGraw-Hill.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, e G. Geoffrey Vining. 2021. Introduction to Linear Regression Analysis. 6º ed. Hoboken: John Wiley & Sons.\n\n\nWeisberg, Sanford. 2005. Applied Linear Regression. New York: Wiley.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimação por Mínimos Quadrados no MRLS</span>"
    ]
  },
  {
    "objectID": "mrls_inferencia.html",
    "href": "mrls_inferencia.html",
    "title": "6  Inferência no MRLS com erros normais",
    "section": "",
    "text": "6.1 Por que assumir normalidade?\nAté aqui, estudamos as propriedades dos estimadores de mínimos quadrados ordinários (MQO) no Modelo de Regressão Linear Simples (MRLS). Mostramos que \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) são não viesados, possuem variâncias explícitas e, pelo Teorema de Gauss–Markov, são os melhores estimadores lineares não viesados (BLUE) sob as hipóteses clássicas de exogeneidade, homoscedasticidade e independência dos erros (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).\nNo entanto, até este ponto conhecemos apenas momentos de primeira e segunda ordem das distribuições amostrais dos estimadores, ou seja, suas esperanças e variâncias. Não conhecemos suas distribuições exatas. De inferência, já sabemos que um estimador ser não viesado e eficiente dentro de uma classe não é suficiente para construir intervalos de confiança exatos ou realizar testes de hipóteses com nível de significância controlado em amostras finitas.\nPara superar essa limitação, acrescentamos uma hipótese mais forte e específica: a normalidade dos erros,\n\\[\n\\varepsilon_i \\sim N(0,\\sigma^2),\n\\quad i=1,2,\\dots,n,\n\\quad \\text{independentes}.\n\\]\nOu seja, cada erro segue uma distribuição normal com média zero e variância constante \\(\\sigma^2&gt;0\\), sendo ainda independentes entre si.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferência no MRLS com erros normais</span>"
    ]
  },
  {
    "objectID": "mrls_inferencia.html#por-que-assumir-normalidade",
    "href": "mrls_inferencia.html#por-que-assumir-normalidade",
    "title": "6  Inferência no MRLS com erros normais",
    "section": "",
    "text": "6.1.1 Estrutura probabilística do MRLS com erros normais\nCom essa suposição adicional, a formulação probabilística do modelo passa a ser\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i,\n\\quad \\varepsilon_i \\sim N(0,\\sigma^2).\n\\]\nLogo, condicionalmente a \\(X_i\\), temos\n\\[\nY_i \\mid X_i \\sim N(\\beta_0 + \\beta_1 X_i, \\, \\sigma^2).\n\\]\nIsso significa que o modelo deixa de ser apenas um modelo para a média condicional e passa a especificar completamente a distribuição condicional de \\(Y\\) dado \\(X\\). Em outras palavras, a normalidade fornece não apenas a forma do valor esperado, mas também a forma funcional da incerteza em torno dessa média.\n\n\n6.1.2 Consequências conceituais da normalidade\nA introdução da hipótese de normalidade tem implicações precisas:\n\nO MRLS continua sendo um modelo para a média condicional, mas agora a variabilidade em torno dessa média é descrita por uma estrutura probabilística completamente especificada.\nAs hipóteses de Gauss–Markov já asseguravam que os estimadores de MQO eram BLUE, mas não determinavam suas distribuições exatas. A normalidade preenche exatamente essa lacuna.\nComo combinações lineares de variáveis normais são normais, os estimadores \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\), que são combinações lineares dos \\(Y_i\\), passam a ter distribuições normais exatas em amostras finitas (ver Montgomery, Peck, e Vining (2021)).\nA estatística baseada na soma dos quadrados dos resíduos passa a ter distribuição qui-quadrado, o que permite derivar distribuições \\(t\\) e \\(F\\) de forma exata (ver Kutner et al. (2005)).\n\nDo ponto de vista metodológico, a normalidade não é necessária para a obtenção das propriedades de não viés ou eficiência relativa, mas é uma boa alternativa para a construção de procedimentos inferenciais exatos em amostras finitas. Essa hipótese não altera os estimadores de MQO, mas altera o que podemos afirmar sobre sua variabilidade e sobre a incerteza associada às estimativas.\nPortanto, a introdução da normalidade transforma o MRLS de um modelo com propriedades ótimas em termos de média e variância em um modelo com estrutura probabilística completa, apto a sustentar intervalos de confiança e testes de hipóteses com boas propriedades.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferência no MRLS com erros normais</span>"
    ]
  },
  {
    "objectID": "mrls_inferencia.html#distribuições-amostrais-no-mrls-com-erros-normais",
    "href": "mrls_inferencia.html#distribuições-amostrais-no-mrls-com-erros-normais",
    "title": "6  Inferência no MRLS com erros normais",
    "section": "6.2 Distribuições amostrais no MRLS com erros normais",
    "text": "6.2 Distribuições amostrais no MRLS com erros normais\nSob a hipótese adicional de normalidade dos erros no MRLS, podemos derivar as distribuições amostrais exatas dos principais estimadores do modelo. Esse é o ponto de transição entre propriedades puramente algébricas (não viés, variância mínima dentro de uma classe) e inferência estatística formal.\nRecordemos que, sob normalidade,\n\\[\n\\varepsilon_i \\sim N(0,\\sigma^2), \\quad \\text{independentes}.\n\\]\nComo os estimadores de MQO podem ser escritos como combinações lineares dos \\(Y_i\\), e cada \\(Y_i\\) é normal condicionalmente a \\(X_i\\), segue que \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) são também normalmente distribuídos. Essa conclusão decorre do fato fundamental de que combinações lineares de variáveis normais independentes permanecem normais (ver Montgomery, Peck, e Vining (2021)).\n\n6.2.1 Distribuição de \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\)\nPara a inclinação, obtemos:\n\\[\n\\hat\\beta_1 \\sim N\\!\\left(\\beta_1, \\, \\frac{\\sigma^2}{S_{xx}}\\right),\n\\qquad\nS_{xx} = \\sum_{i=1}^n (X_i - \\bar X)^2.\n\\]\nPara o intercepto:\n\\[\n\\hat\\beta_0 \\sim N\\!\\left(\\beta_0, \\, \\sigma^2\\left(\\frac{1}{n}+\\frac{\\bar X^2}{S_{xx}}\\right)\\right).\n\\]\nA demonstração dessas distribuições pode ser vista no Apêndice de Demonstrações {#demo}, onde se explora explicitamente a representação linear dos estimadores em função dos \\(Y_i\\) e a estrutura de variância-covariância do vetor de respostas.\nAlém disso, para o estimador da variância residual,\n\\[\ns^2 = \\frac{SQRes}{n-2},\n\\qquad\nSQRes = \\sum_{i=1}^n (Y_i - \\hat Y_i)^2,\n\\]\nvale o resultado fundamental:\n\\[\n\\frac{(n-2)s^2}{\\sigma^2} \\sim \\chi^2_{n-2}.\n\\]\nEsse resultado decorre da decomposição ortogonal do vetor \\(Y\\) em componente ajustada e componente residual, cuja demonstração também pode ser consultada no Apêndice {#demo} (ver Kutner et al. (2005)). A perda de dois graus de liberdade reflete a estimação dos dois parâmetros \\(\\beta_0\\) e \\(\\beta_1\\).\nÉ importante destacar que a normalidade não altera os estimadores de MQO: as expressões de \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\) e \\(s^2\\) permanecem as mesmas. O ganho está em outro ponto: ela fornece uma descrição probabilística completa da variabilidade desses estimadores, algo que as hipóteses de Gauss–Markov não entregam por si só.\nEm particular, o resultado\n\\[\n\\frac{(n-2)s^2}{\\sigma^2} \\sim \\chi^2_{n-2}\n\\]\né a peça-chave que permite obter, de forma exata, as distribuições \\(t\\) e \\(F\\) usadas em intervalos de confiança e testes de hipóteses. Os \\(n-2\\) graus de liberdade refletem a estimação de \\((\\beta_0,\\beta_1)\\) e garantem que \\(s^2\\) seja não viesado para \\(\\sigma^2\\).",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferência no MRLS com erros normais</span>"
    ]
  },
  {
    "objectID": "mrls_inferencia.html#predição-pontual-da-média-condicional",
    "href": "mrls_inferencia.html#predição-pontual-da-média-condicional",
    "title": "6  Inferência no MRLS com erros normais",
    "section": "6.3 Predição pontual da média condicional",
    "text": "6.3 Predição pontual da média condicional\nA predição pontual é o primeiro passo para inferir sobre a relação média entre \\(Y\\) e \\(X\\) no MRLS. Antes de introduzir intervalos, é útil explicitar que os valores ajustados são quantidades aleatórias (pois dependem da amostra) e, sob normalidade, possuem distribuição conhecida.\n\n6.3.1 Distribuição dos valores ajustados\nPara um valor genérico \\(X_0\\), definimos o valor ajustado (ou média condicional estimada) como\n\\[\n\\hat{\\mu}(X_0) \\;=\\; \\hat\\beta_0 + \\hat\\beta_1 X_0.\n\\]\nSob erros normais, \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) são combinações lineares dos \\(Y_i\\) e, portanto, \\(\\hat{\\mu}(X_0)\\) também é uma combinação linear de variáveis normais. Assim, \\(\\hat{\\mu}(X_0)\\) é normalmente distribuído (ver Montgomery, Peck, e Vining (2021)).\nAlém disso, sua esperança é\n\\[\nE[\\hat{\\mu}(X_0)] = \\beta_0 + \\beta_1 X_0 = \\mu(X_0),\n\\]\nisto é, \\(\\hat{\\mu}(X_0)\\) é não viesado para a média condicional.\nSua variância é\n\\[\nVar(\\hat{\\mu}(X_0))\n=\n\\sigma^2\\left[\n\\frac{1}{n} + \\frac{(X_0-\\bar X)^2}{S_{xx}}\n\\right],\n\\qquad\nS_{xx}=\\sum_{i=1}^n (X_i-\\bar X)^2,\n\\]\nde modo que\n\\[\n\\hat{\\mu}(X_0)\\sim\nN\\!\\left(\\mu(X_0),\\;\n\\sigma^2\\left[\\frac{1}{n} + \\frac{(X_0-\\bar X)^2}{S_{xx}}\\right]\\right).\n\\]\nEssa expressão evidencia um aspecto estrutural da regressão: a incerteza sobre a média ajustada é menor perto de \\(\\bar X\\) e aumenta à medida que \\(X_0\\) se afasta do centro dos dados, refletindo a geometria do ajuste por mínimos quadrados (ver Kutner et al. (2005)).\n\n\n6.3.2 Predição pontual\nA predição pontual da resposta média em \\(X_0\\) é, portanto,\n\\[\n\\hat{Y}_0 = \\hat{\\mu}(X_0)=\\hat\\beta_0+\\hat\\beta_1X_0,\n\\]\nque corresponde à função de regressão estimada avaliada em \\(X_0\\). É importante enfatizar que \\(\\hat{Y}_0\\) se refere à média condicional \\(E[Y\\mid X_0]\\), e não ao valor de uma nova observação individual.\n\n\n6.3.3 Limitação da predição pontual\nEmbora \\(\\hat{Y}_0\\) forneça uma estimativa central, ela não quantifica a incerteza associada ao ajuste. Por isso, em aplicações, a predição pontual deve ser acompanhada de:\n\num intervalo de confiança para \\(\\mu(X_0)\\), quando o interesse é a tendência média; ou\num intervalo de predição, quando o objetivo é prever uma nova observação individual.\n\nEssas duas construções serão desenvolvidas nas próximas subseções (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).\nConsidere o seguinte gráfico, onde o ponto destacado corresponde a \\(\\hat Y_0\\), isto é, à estimativa da média condicional em \\(X_0\\). Observe que o valor pontual não informa, por si só, o grau de incerteza associado à estimativa, questão que será tratada na próxima subseção.\n\n\n\n\n\nPredição pontual: reta ajustada e valor previsto para X0=5 horas.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferência no MRLS com erros normais</span>"
    ]
  },
  {
    "objectID": "mrls_inferencia.html#intervalos-de-confiança-no-mrls",
    "href": "mrls_inferencia.html#intervalos-de-confiança-no-mrls",
    "title": "6  Inferência no MRLS com erros normais",
    "section": "6.4 Intervalos de confiança no MRLS",
    "text": "6.4 Intervalos de confiança no MRLS\nNa seção anterior vimos que, sob a hipótese de erros normais, os estimadores de MQO possuem distribuições normais quando a variância \\(\\sigma^2\\) é conhecida. No entanto, na prática, \\(\\sigma^2\\) é desconhecida e deve ser substituída por seu estimador não viesado\n\\[\ns^2 = \\frac{SQRes}{n-2},\n\\qquad\nSQRes = \\sum_{i=1}^n (Y_i-\\hat Y_i)^2.\n\\]\nEssa substituição tem consequência direta na forma das distribuições amostrais: ao padronizarmos os estimadores utilizando \\(s\\) em vez de \\(\\sigma\\), as estatísticas resultantes deixam de seguir a normal padrão e passam a seguir distribuições \\(t\\) de Student com \\(n-2\\) graus de liberdade (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).\nEssa mudança decorre do fato de que\n\\[\n\\frac{(n-2)s^2}{\\sigma^2} \\sim \\chi^2_{n-2},\n\\]\ne de que \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) são independentes de \\(s^2\\) sob normalidade dos erros, resultado cuja demonstração pode ser consultada no Apêndice de Demonstrações {#demo}.\n\n6.4.1 Intervalos para \\(\\beta_0\\), \\(\\beta_1\\)\nPara a inclinação, a estatística\n\\[\nT_{\\beta_1} =\n\\frac{\\hat\\beta_1 - \\beta_1}\n{s/\\sqrt{S_{xx}}}, \\qquad\nS_{xx}=\\sum_{i=1}^n (X_i-\\bar X)^2.\n\\]\nsegue distribuição t de Student com \\(n-2\\) graus de liberdade\n\\[\nT_{\\beta_1} \\sim t_{n-2}.\n\\]\nAnalogamente, para o intercepto, temos a mesma distribuição t de Student com \\(n-2\\) graus de liberdade\n\\[\nT_{\\beta_0} =\n\\frac{\\hat\\beta_0 - \\beta_0}\n{s\\sqrt{\\tfrac{1}{n}+\\tfrac{\\bar X^2}{S_{xx}}}}\n\\sim t_{n-2}.\n\\]\nA demonstração formal dessas distribuições padronizadas pode ser vista no Apêndice {#demo}, onde se utiliza a independência entre estimadores lineares normais e a soma de quadrados residual.\nCom base nessas estatísticas, os intervalos de confiança de nível \\((1-\\alpha)\\times 100\\%\\) são dados por\n\\[\nIC_{1-\\alpha}(\\beta_0) =\n\\hat\\beta_0\n\\pm\nt_{n-2;1-\\alpha/2}\\;\ns\\sqrt{\\frac{1}{n}+\\frac{\\bar X^2}{S_{xx}}}\n\\]\ne\n\\[\nIC_{1-\\alpha}(\\beta_1) =\n\\hat\\beta_1\n\\pm\nt_{n-2;1-\\alpha/2}\\;\n\\frac{s}{\\sqrt{S_{xx}}}.\n\\]\nAqui, \\(t_{n-2;1-\\alpha/2}\\) denota o quantil superior da distribuição \\(t\\) com \\(n-2\\) graus de liberdade.\n\n\n6.4.2 Intervalo de confiança para a média condicional\nAnteriormente vimos que a predição pontual da média condicional em \\(X_0\\) é\n\\[\n\\hat{\\mu}(X_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_0,\n\\]\nestimativa natural de\n\\[\n\\mu(X_0) = E[Y \\mid X_0].\n\\]\nSob a hipótese de erros normais, a combinação linear \\(\\hat{\\mu}(X_0)\\) possui distribuição normal quando \\(\\sigma^2\\) é conhecido. Como, na prática, \\(\\sigma^2\\) é substituído por seu estimador não viesado \\(s^2 = SQ_{Res}/(n-2)\\), a estatística padronizada\n\\[\nT =\n\\frac{\\hat{\\mu}(X_0) - \\mu(X_0)}\n{s \\sqrt{\\frac{1}{n} + \\frac{(X_0-\\bar X)^2}{S_{xx}}}}\n\\]\nsegue distribuição \\(t\\) de Student com \\(n-2\\) graus de liberdade (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021)). A demonstração formal pode ser consultada no Apêndice de Demonstrações {#demo}.\nConsequentemente, um intervalo de confiança de nível \\((1-\\alpha)\\times 100\\%\\) para a média condicional é\n\\[\nIC_{1-\\alpha}\\left[\\mu(X_0)\\right]\n=\n\\hat{\\mu}(X_0)\n\\pm\nt_{n-2;\\,1-\\alpha/2}\\;\ns \\sqrt{\\frac{1}{n} + \\frac{(X_0-\\bar X)^2}{S_{xx}}},\n\\]\nem que:\n\n\\(s^2 = SQ_{Res}/(n-2)\\) é a variância residual estimada;\n\\(S_{xx} = \\sum_{i=1}^n (X_i-\\bar X)^2\\);\no termo dentro da raiz representa a variabilidade da estimativa da média condicional.\n\nO fator\n\\[\n\\frac{1}{n} + \\frac{(X_0-\\bar X)^2}{S_{xx}}\n\\]\npossui interpretação estrutural clara. Ele combina:\n\n\\(\\frac{1}{n}\\): componente associado à incerteza na estimação do intercepto;\n\\(\\frac{(X_0-\\bar X)^2}{S_{xx}}\\): componente associado à incerteza da inclinação e ao afastamento de \\(X_0\\) em relação ao centro da amostra.\n\nEssa decomposição revela que a precisão da estimativa depende da posição de \\(X_0\\) no domínio observado. O intervalo é:\n\nmais estreito quando \\(X_0=\\bar X\\);\nmais largo à medida que \\(X_0\\) se afasta da média amostral.\n\nEsse comportamento decorre diretamente da geometria da regressão linear e da estrutura de projeção ortogonal subjacente aos mínimos quadrados.\nÉ fundamental enfatizar que esse intervalo refere-se à média condicional\n\\[\n\\mu(X_0) = E[Y \\mid X_0],\n\\]\ne não a uma nova observação individual. Ele quantifica a incerteza sobre a tendência média da resposta para a condição \\(X=X_0\\).\nDicas de uso\n\nUtilize este intervalo quando o objetivo for inferir sobre a tendência média da resposta para um valor específico do regressor.\nNão o confunda com o intervalo de predição para um novo indivíduo, que incorpora variabilidade adicional do erro aleatório.\n\nA figura a seguir ilustra um intervalo de confiança de 95% para a média condicional ao longo do domínio observado.\nObserve qye banda em torno da reta representa a incerteza sobre \\(\\mu(X)\\) ao longo dos valores observados de \\(X\\). Note que ela é mais estreita nas proximidades de \\(\\bar X\\) e se alarga progressivamente nos extremos do domínio amostral, refletindo o aumento da variância de \\(\\hat{\\mu}(X_0)\\).\n\n\n\n\n\nIntervalo de confiança (95%) para a média condicional.\n\n\n\n\n\n\n6.4.3 Interpretação frequentista e significado dos intervalos\nA interpretação frequentista de um intervalo de confiança de nível \\(1-\\alpha\\) é a seguinte: se o experimento fosse repetido um número grande de vezes e sob as mesmas condições, aproximadamente \\((1-\\alpha)\\times 100\\%\\) dos intervalos construídos conteriam o verdadeiro parâmetro. O parâmetro é fixo; o que varia é o intervalo, pois ele depende da amostra observada.\nA introdução da distribuição \\(t\\) de Student desempenha papel essencial nesse contexto. Ao substituir \\(\\sigma\\) por seu estimador \\(s\\), incorporamos a incerteza adicional decorrente da estimação da variância. Essa correção é particularmente relevante em amostras pequenas: quanto menor \\(n\\), mais pesada é a cauda da distribuição \\(t_{n-2}\\) e, consequentemente, mais largos são os intervalos. À medida que \\(n\\) cresce, a distribuição \\(t_{n-2}\\) converge para a normal padrão, e os intervalos passam a se aproximar daqueles que seriam obtidos se \\(\\sigma^2\\) fosse conhecido.\nOs intervalos são centrados em \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) porque esses estimadores são não viesados. Em média, as retas ajustadas coincidem com a reta verdadeira; os intervalos quantificam precisamente a incerteza em torno dessa centralidade.\nÉ fundamental distinguir os diferentes objetos inferenciais:\n\nO intervalo para \\(\\beta_0\\) e \\(\\beta_1\\) refere-se a parâmetros estruturais do modelo.\nO intervalo para \\(\\mu(X_0)=E[Y\\mid X_0]\\) refere-se à média condicional.\nNenhum desses intervalos corresponde à previsão de uma nova observação individual.\n\nA distinção entre inferência sobre a média condicional e previsão individual é conceitualmente importante, pois a segunda incorpora não apenas a incerteza na estimação dos parâmetros, mas também a variabilidade intrínseca do processo aleatório. Essa diferença será aprofundada na subseção seguinte.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferência no MRLS com erros normais</span>"
    ]
  },
  {
    "objectID": "mrls_inferencia.html#intervalo-de-predição-para-nova-observação",
    "href": "mrls_inferencia.html#intervalo-de-predição-para-nova-observação",
    "title": "6  Inferência no MRLS com erros normais",
    "section": "6.5 Intervalo de predição para nova observação",
    "text": "6.5 Intervalo de predição para nova observação\nAté aqui construímos intervalos de confiança para a média condicional \\(\\mu(X_0)=E[Y\\mid X_0]\\). No entanto, muitas aplicações exigem algo diferente: prever o valor de uma nova observação individual associada a \\(X_0\\).\nSe uma nova unidade experimental for observada no mesmo valor \\(X_0\\), seu modelo é\n\\[\nY_{\\text{novo}}(X_0)\n=\n\\beta_0 + \\beta_1 X_0 + \\varepsilon_{\\text{novo}},\n\\]\nem que \\(\\varepsilon_{\\text{novo}} \\sim N(0,\\sigma^2)\\) e é independente dos erros da amostra original.\nA diferença conceitual é que agora não estamos estimando apenas a média condicional, mas prevendo uma realização específica que contém, além da incerteza na estimação dos parâmetros, a variabilidade intrínseca do erro aleatório.\nPortanto, a quantidade relevante é\n\\[\nY_{\\text{novo}}(X_0) - \\hat{\\mu}(X_0),\n\\]\ncuja variância é\n\\[\nVar\\!\\left[Y_{\\text{novo}}(X_0) - \\hat{\\mu}(X_0)\\right]\n=\n\\sigma^2\n\\left[\n1 + \\frac{1}{n} + \\frac{(X_0-\\bar X)^2}{S_{xx}}\n\\right].\n\\]\nO termo adicional “\\(+1\\)” aparece porque a nova observação contém um erro próprio, independente daquele utilizado na estimação dos parâmetros. A demonstração formal dessa variância pode ser consultada no Apêndice de Demonstrações {#demo} (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).\nPadronizando essa quantidade por \\(s\\), obtemos uma estatística que segue distribuição t de Student com \\(n-2\\) graus de liberdade (\\(t_{n-2}\\)) sob normalidade dos erros. Assim, o intervalo de predição de nível \\((1-\\alpha)\\times 100\\%\\) é\n\\[\nIC\\!\\left[Y_{\\text{novo}}(X_0)\\right]\n=\n\\hat{\\mu}(X_0)\n\\pm\nt_{n-2;\\,1-\\alpha/2}\\;\ns\n\\sqrt{\n1 + \\frac{1}{n} + \\frac{(X_0-\\bar X)^2}{S_{xx}}\n}.\n\\]\nComparando com o intervalo para a média condicional, temos\n\\[\n\\hat{\\mu}(X_0)\n\\pm\nt_{n-2;\\,1-\\alpha/2}\\;\ns\n\\sqrt{\n\\frac{1}{n} + \\frac{(X_0-\\bar X)^2}{S_{xx}}\n}.\n\\]\nDaí, vemos claramente a presença do termo adicional \\(1\\) dentro da raiz. Esse termo representa a variabilidade individual irreducível do processo aleatório.\nConsequentemente:\n\nO intervalo de predição é sempre mais largo que o intervalo de confiança da média.\nA diferença entre eles é tanto maior quanto maior for \\(\\sigma^2\\).\nAmbos se alargam quando \\(X_0\\) se afasta de \\(\\bar X\\), refletindo a incerteza adicional associada à extrapolação.\n\nO intervalo de predição fornece um conjunto de valores plausíveis para uma nova observação individual, e não para a média populacional. Em termos frequentistas, se o processo fosse repetido nas mesmas condições, aproximadamente \\((1-\\alpha)\\times 100\\%\\) desses intervalos conteriam a nova observação gerada pelo modelo.\nDica prática\n\nUse intervalo de confiança quando o objetivo for inferir sobre a tendência média.\nUse intervalo de predição quando o interesse for antecipar o valor de um novo indivíduo.\n\nA figura a seguir ilustra simultaneamente o intervalo de confiança para a média e o intervalo de predição para nova observação. Comparando as bandas, perceba que o intervalo de predição é sempre mais largo que o de confiança. Isso ocorre porque ele incorpora a variabilidade individual das novas observações, além da incerteza da média.\n\n\n\n\n\nIntervalo de confiança vs. intervalo de predição (95%).\n\n\n\n\nAdvertência sobre extrapolação\nAs expressões obtidas para o intervalo de predição são válidas para qualquer valor numérico \\(X_0\\). No entanto, é fundamental distinguir entre interpolação (quando \\(X_0\\) pertence ao intervalo observado da amostra) e extrapolação (quando \\(X_0\\) está fora do domínio observado de \\(X\\)).\nSeja o intervalo amostral observado \\[\nX_{(min)} \\leq X_i \\leq X_{(max)}.\n\\]\nQuando \\(X_0 \\in [X_{(min)}, X_{(max)}]\\), o modelo está sendo utilizado em uma região sustentada pelos dados. Nesse caso, embora o intervalo possa se alargar à medida que \\(X_0\\) se afasta de \\(\\bar X\\), a inferência permanece ancorada na informação empírica disponível.\nPor outro lado, quando \\(X_0\\) está fora desse intervalo, ocorre extrapolação. Nessa situação, a validade formal da expressão algébrica do intervalo permanece, mas sua confiabilidade prática pode ser comprometida, pois o modelo passa a depender fortemente da suposição de linearidade fora da região observada. Pequenas violações da forma funcional podem produzir erros substanciais de predição.\nComo destacam Kutner et al. (2005) e Montgomery, Peck, e Vining (2021), a regressão linear deve ser utilizada com cautela fora do domínio amostral, pois o comportamento da relação entre \\(X\\) e \\(Y\\) além dos dados observados não é garantido pelo modelo ajustado. Assim, intervalos de predição em extrapolação tendem a ser não apenas mais largos, mas também potencialmente menos representativos do processo real.\n\n\n\n\nKutner, Michael H., Christopher J. Nachtsheim, John Neter, e William Li. 2005. Applied Linear Statistical Models. 5º ed. New York: McGraw-Hill.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, e G. Geoffrey Vining. 2021. Introduction to Linear Regression Analysis. 6º ed. Hoboken: John Wiley & Sons.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferência no MRLS com erros normais</span>"
    ]
  },
  {
    "objectID": "mrls_testes.html",
    "href": "mrls_testes.html",
    "title": "7  Testes de hipóteses e ANOVA",
    "section": "",
    "text": "7.1 Testes marginais para \\(\\beta_1\\) e \\(\\beta_0\\)\nA construção de intervalos de confiança fornece uma forma de expressar a incerteza nos estimadores. Outra abordagem complementar é a dos testes de hipóteses e análise de variância (ANOVA), que avaliam formalmente se os coeficientes do modelo diferem significativamente de determinados valores, em especial do zero. No Modelo de Regressão Linear Simples (MRLS) com erros normais, os testes se apoiam nas distribuições exatas dos estimadores discutidas anteriormente.\nSob as hipóteses clássicas do MRLS normal: (a) linearidade na forma funcional, (b) independência dos erros, (b) homocedasticidade e (d) normalidade, os estimadores de mínimos quadrados coincidem com os estimadores de máxima verossimilhança, e possuem distribuições amostrais exatas baseadas na distribuição normal e na distribuição \\(t\\) de Student (Hoffmann (2016); Montgomery, Peck, e Vining (2021)). Em particular, condicionalmente aos valores observados de \\(X\\), temos\n\\[\n\\hat\\beta_1 \\sim N\\!\\left(\\beta_1,\\; \\frac{\\sigma^2}{S_{xx}}\\right),\n\\]\ne, como \\(\\sigma^2\\) é desconhecida e estimada por \\(s^2 = SQ_{Res}/(n-2)\\), a padronização conduz à distribuição \\(t_{n-2}\\), resultado central para a inferência clássica em regressão linear simples (Kutner et al. (2005); Casella e Berger (2002)).\nÉ importante enfatizar que os testes de hipóteses são construídos dentro do modelo. A validade exata da distribuição \\(t\\) depende da normalidade dos erros; na ausência dessa hipótese, os resultados passam a ter caráter assintótico. Assim, significância estatística deve sempre ser interpretada à luz das suposições estruturais do modelo.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testes de hipóteses e ANOVA</span>"
    ]
  },
  {
    "objectID": "mrls_testes.html#testes-marginais-para-beta_1-e-beta_0",
    "href": "mrls_testes.html#testes-marginais-para-beta_1-e-beta_0",
    "title": "7  Testes de hipóteses e ANOVA",
    "section": "",
    "text": "7.1.1 Teste para a \\(\\beta_1\\)\nNo caso da inclinação, o teste é:\n\\[\nH_0: \\beta_1 = \\beta_{1,0}\n\\quad \\text{vs} \\quad\nH_1: \\beta_1 \\neq \\beta_{1,0},\n\\]\ncom estatística de teste dada por\n\\[\nT = \\frac{\\hat\\beta_1 - \\beta_{1,0}}{s / \\sqrt{S_{xx}}} \\;\\sim\\; t_{n-2}.\n\\]\nEsse teste é especialmente relevante quando \\(\\beta_{1,0}=0\\), situação em que verificamos se existe associação linear entre \\(X\\) e \\(Y\\). Em termos práticos, ele responde à pergunta: vale a pena incluir \\(X\\) para explicar \\(Y\\)? Se \\(|T|\\) ultrapassa o valor crítico da distribuição \\(t_{n-2}\\), rejeitamos \\(H_0\\) e concluímos que a inclinação é estatisticamente diferente de zero.\nDo ponto de vista conceitual, testar \\(\\beta_1=0\\) equivale a testar se a melhor reta ajustada possui inclinação nula, isto é, se o modelo reduz-se a \\(Y_i=\\beta_0+\\varepsilon_i\\). Portanto, o teste compara dois modelos aninhados: o modelo completo (com inclinação livre) e o modelo restrito (com \\(\\beta_1=0\\)). Essa interpretação como comparação entre modelos é fundamental para compreender a ligação posterior com a estatística \\(F\\) (Draper e Smith (1998); Kutner et al. (2005)).\nAlém disso, há equivalência formal entre o teste \\(t\\) bilateral ao nível \\(\\alpha\\) e o intervalo de confiança \\((1-\\alpha)\\) para \\(\\beta_1\\): rejeitar \\(H_0\\) é equivalente a verificar que \\(\\beta_{1,0}\\) não pertence ao intervalo de confiança correspondente Casella e Berger (2002). O \\(p\\)-valor, por sua vez, é definido como\n\\[\np = P\\left(|T| \\ge |t_{obs}| \\mid H_0 \\right),\n\\]\ne quantifica evidência contra \\(H_0\\) dentro da estrutura probabilística assumida.\n\n\n7.1.2 Teste para a \\(\\beta_0\\)\nDe modo análogo, para o intercepto temos:\n\\[\nH_0: \\beta_0 = \\beta_{0,0}\n\\quad \\text{vs} \\quad\nH_1: \\beta_0 \\neq \\beta_{0,0},\n\\]\ncom estatística de teste\n\\[\nT = \\frac{\\hat\\beta_0 - \\beta_{0,0}}{s \\sqrt{\\tfrac{1}{n}+\\tfrac{\\bar X^2}{S_{xx}}}} \\;\\sim\\; t_{n-2}.\n\\]\nEsse teste é menos central do ponto de vista prático, mas pode ser importante quando se deseja avaliar se o valor médio de \\(Y\\) para \\(X=0\\) coincide com alguma referência teórica ou prática.\nConceitualmente, \\(\\beta_0 = E(Y\\mid X=0)\\) dentro do modelo linear. Assim, sua interpretação depende criticamente de \\(X=0\\) ter significado no fenômeno estudado e estar dentro do intervalo de observação dos dados. Caso contrário, o intercepto pode representar apenas uma extrapolação matemática da reta ajustada, ainda que perfeitamente bem definido do ponto de vista estatístico (Montgomery, Peck, e Vining (2021); Weisberg (2005)).\n\nApêndice de Demonstrações {#demo}: as distribuições exatas das estatísticas \\(T\\) decorrem da normalidade dos erros, da independência entre \\(\\hat\\beta_j\\) e \\(SQ_{Res}\\) e da relação entre variância residual e distribuição qui-quadrado, conforme desenvolvimento clássico da inferência em modelos lineares (Casella e Berger (2002); Kutner et al. (2005)).",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testes de hipóteses e ANOVA</span>"
    ]
  },
  {
    "objectID": "mrls_testes.html#análise-de-variância-no-mrls",
    "href": "mrls_testes.html#análise-de-variância-no-mrls",
    "title": "7  Testes de hipóteses e ANOVA",
    "section": "7.2 Análise de Variância no MRLS",
    "text": "7.2 Análise de Variância no MRLS\n\n7.2.1 Decomposição da soma de quadrados\nO teste \\(F\\) pode ser entendido a partir da decomposição da variabilidade total em \\(Y\\):\n\\[\nSQ_{Total} = SQ_{Reg} + SQ_{Res}.\n\\]\nAqui, \\(SQ_{Total} = \\sum_{i=1}^n (Y_i - \\bar Y)^2\\) mede a variabilidade total das observações em torno da média. Essa variabilidade pode ser separada em duas partes:\n\nvariabilidade explicada pela regressão\n\n\\[\nSQ_{Reg} = \\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2\n\\]\n\nVariabilidade não explicada\n\n\\[\nSQ_{Res} = \\sum_{i=1}^n (Y_i - \\hat Y_i)^2,\n\\]\nassociada aos resíduos. Em termos geométricos, a \\(SQ_{Reg}\\) corresponde à projeção de \\(Y\\) no espaço gerado por \\(X\\), enquanto \\(SQ_{Res}\\) corresponde ao componente ortogonal (erro).\nFormalmente, essa decomposição decorre da ortogonalidade entre resíduos e valores ajustados no método dos mínimos quadrados. No MRLS, tem-se\n\\[\n\\sum_{i=1}^n \\hat\\varepsilon_i (\\hat Y_i - \\bar Y) = 0,\n\\]\no que implica que a variabilidade total pode ser particionada sem termo de cruzamento. Em notação matricial, essa propriedade está associada ao fato de que o vetor de resíduos é ortogonal ao espaço coluna da matriz de projeto \\(\\mathbf{X}\\), isto é, \\(\\mathbf{X}^\\top \\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{0}\\) (Harville (2000); Searle (2016)).\n\nApêndice de Demonstrações {#demo}: a identidade \\(SQ_{Total}=SQ_{Reg}+SQ_{Res}\\) é obtida expandindo \\(\\sum (Y_i-\\bar Y)^2\\) em termos de \\((\\hat Y_i-\\bar Y)\\) e \\((Y_i-\\hat Y_i)\\) e utilizando a ortogonalidade dos resíduos.\n\nEssa decomposição mostra que o ajuste por regressão não apenas fornece estimativas pontuais, mas também permite quantificar de forma clara quanto da variabilidade total de \\(Y\\) é capturada pela relação linear com \\(X\\). Quanto maior \\(SQ_{Reg}\\) em relação a \\(SQ_{Total}\\), maior o poder explicativo do modelo.\nDo ponto de vista probabilístico, sob \\(H_0:\\beta_1=0\\) e normalidade dos erros, as somas de quadrados associadas à regressão e aos resíduos, quando devidamente padronizadas por \\(\\sigma^2\\), seguem distribuições qui-quadrado independentes com 1 e \\(n-2\\) graus de liberdade, respectivamente (Kutner et al. (2005)). Essa independência é a base formal da estatística \\(F\\).\n\n\n7.2.2 Quadrados Médios e Estatística F\nPara formalizar o teste global, cada soma de quadrados é dividida pelos graus de liberdade correspondentes:\n\nQuadrado médio da regressão: \\[\nQM_{Reg} = \\frac{SQ_{Reg}}{1}.\n\\]\nQuadrado médio dos resíduos: \\[\nQM_{Res} = \\frac{SQ_{Res}}{n-2}.\n\\]\n\nA razão entre eles define a estatística \\(F\\):\n\\[\nF = \\frac{QM_{Reg}}{QM_{Res}} \\;\\sim\\; F_{1,n-2} \\quad \\text{sob } H_0: \\beta_1=0.\n\\]\nEsse teste avalia, portanto, se a proporção de variabilidade explicada pela regressão é grande o suficiente em comparação com a variabilidade residual, justificando o uso do modelo.\nInterpretativamente, \\(QM_{Res}\\) é um estimador não viesado de \\(\\sigma^2\\), enquanto \\(QM_{Reg}\\) mede a variação explicada por grau de liberdade associado ao efeito linear de \\(X\\). Assim, o teste \\(F\\) compara um componente sistemático (sinal) com um componente aleatório (ruído).\nValores elevados de \\(F\\) indicam que a redução em \\(SQ_{Res}\\) ao incluir \\(X\\) é grande demais para ser atribuída apenas ao acaso (Montgomery, Peck, e Vining (2021); Weisberg (2005)).\n\n\n7.2.3 Tabela ANOVA do MRLS\n\n\n\n\n\n\n\n\n\n\nFonte de variação\nSQ\nGL\nQM\nEstatística\n\n\n\n\nRegressão\n\\(SQ_{Reg}\\)\n1\n\\(QM_{Reg}\\)\n\\(F=QM_{Reg}/QM_{Res}\\)\n\n\nResíduo\n\\(SQ_{Res}\\)\n\\(n-2\\)\n\\(QM_{Res}\\)\n\n\n\nTotal\n\\(SQ_{Total}\\)\n\\(n-1\\)\n\n\n\n\n\nA tabela resume de maneira padronizada a decomposição da variabilidade e fornece a base para a aplicação do teste \\(F\\). Note que os graus de liberdade totais satisfazem\n\\[\n(n-1) = 1 + (n-2),\n\\]\nrefletindo que dois parâmetros foram estimados no modelo (intercepto e inclinação).\n\n\n7.2.4 Coeficiente de determinação \\(R^2\\)\nUm desdobramento natural dessa análise é o coeficiente de determinação:\n\\[\nR^2 = \\frac{SQ_{Reg}}{SQ_{Total}} = 1 - \\frac{SQ_{Res}}{SQ_{Total}}.\n\\]\nEle mede a proporção da variabilidade total de \\(Y\\) explicada pela regressão e está limitado ao intervalo \\(0 \\leq R^2 \\leq 1\\). Em termos práticos, \\(R^2 \\times 100\\%\\) indica o percentual da variabilidade de \\(Y\\) que é explicado linearmente por \\(X\\). Quanto maior \\(R^2\\), maior o poder explicativo do modelo.\nÉ importante compreender que \\(R^2\\) é uma medida descritiva da qualidade de ajuste dentro da amostra observada. Ele não implica causalidade, nem garante desempenho preditivo. Além disso, em modelos com múltiplos preditores, \\(R^2\\) tende a aumentar com a inclusão de variáveis, mesmo que irrelevantes, motivo pelo qual se introduz posteriormente o \\(R^2\\) ajustado (Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).\nNo caso do MRLS, há uma relação direta com a estatística descritiva da correlação linear:\n\\[\nR^2 = r_{XY}^2,\n\\quad r_{XY} = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}},\n\\]\nem que \\(r_{XY}\\) é o coeficiente de correlação amostral entre \\(X\\) e \\(Y\\). Essa igualdade decorre das expressões algébricas do estimador \\(\\hat\\beta_1\\) e das somas de quadrados no caso univariado (Charnet et al. (2008); Hoffmann (2016)).\nAssim, \\(R^2\\) conecta três dimensões: é a proporção de variabilidade explicada (ANOVA), é equivalente ao quadrado da correlação linear (estatística descritiva) e fundamenta a estatística \\(F\\) da ANOVA por meio da relação\n\\[\nF = \\frac{QM_{Reg}}{QM_{Res}} = \\frac{R^2}{1-R^2}(n-2).\n\\]\n\nApêndice de Demonstrações {#demo}: a relação entre \\(F\\) e \\(R^2\\) é obtida substituindo as definições de \\(SQ_{Reg}\\) e \\(SQ_{Res}\\) na razão \\(QM_{Reg}/QM_{Res}\\) e utilizando a identidade \\(R^2 = SQ_{Reg}/SQ_{Total}\\).\n\nEm resumo, os testes de hipóteses no MRLS permitem verificar tanto a significância individual dos coeficientes quanto o poder explicativo global do modelo. A equivalência entre os testes \\(t\\) e \\(F\\), a decomposição da soma de quadrados e a interpretação do \\(R^2\\) reforçam a visão integrada da regressão como técnica que conecta estimação, inferência e análise da variabilidade em um único arcabouço teórico (Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).\n\n\n7.2.5 Equivalência entre \\(T^2\\) e \\(F\\)\nUm resultado fundamental é que, no MRLS normal, o teste \\(t\\) para a inclinação e o teste \\(F\\) global da regressão são equivalentes. Quando a hipótese nula é \\(\\beta_1=0\\), temos:\n\\[\nF = T^2.\n\\]\nIsso ocorre porque o modelo possui apenas um preditor. Em modelos múltiplos, a situação muda: o teste \\(t\\) continua avaliando parâmetros individuais, enquanto o teste \\(F\\) passa a ter papel central ao considerar hipóteses conjuntas sobre vários coeficientes.\nPara compreender essa equivalência com rigor, observe que, sob \\(H_0:\\beta_1=0\\), a estatística \\(t\\) pode ser escrita como\n\\[\nT = \\frac{\\hat\\beta_1}{s/\\sqrt{S_{xx}}},\n\\]\nde modo que\n\\[\nT^2 = \\frac{\\hat\\beta_1^2 S_{xx}}{s^2}.\n\\]\nPor outro lado, no MRLS, pode-se mostrar que\n\\[\nSQ_{Reg} = \\hat\\beta_1^2 S_{xx},\n\\]\ne que\n\\[\nQM_{Res} = s^2.\n\\]\nLogo,\n\\[\nF = \\frac{QM_{Reg}}{QM_{Res}}\n  = \\frac{SQ_{Reg}/1}{s^2}\n  = \\frac{\\hat\\beta_1^2 S_{xx}}{s^2}\n  = T^2.\n\\]\nPortanto, a estatística \\(F\\) nada mais é do que o quadrado da estatística \\(t\\) quando há apenas um parâmetro de inclinação sendo testado.\nDo ponto de vista distribucional, se\n\\[\nT \\sim t_{n-2},\n\\]\nentão\n\\[\nT^2 \\sim F_{1,n-2},\n\\]\no que decorre da relação geral entre as distribuições \\(t\\) e \\(F\\) (Casella e Berger (2002)). Assim, a equivalência também se verifica ao nível das distribuições.\nEssa identidade tem uma consequência didática importante: no MRLS, o teste global da regressão e o teste individual da inclinação são exatamente o mesmo teste, apenas expressos em escalas diferentes. Em outras palavras, testar a significância global do modelo é o mesmo que testar se a inclinação é nula.\n\nApêndice de Demonstrações {#demo}: a identidade \\(SQ_{Reg}=\\hat\\beta_1^2 S_{xx}\\) e a relação distribucional entre \\(t\\) e \\(F\\) podem ser demonstradas a partir das propriedades do MQO e da definição da distribuição \\(F\\) como razão de qui-quadrados independentes Casella e Berger (2002); Harville (2000).\n\n\n\n\n\nCasella, George, e Roger L. Berger. 2002. Statistical Inference. 2º ed. Pacific Grove: Duxbury.\n\n\nCharnet, Reinaldo, Carlos Alberto Freire, Eliane M. R. Charnet, e Helio Bonvino. 2008. Análise de Modelos de Regressão Linear com Aplicações. 2º ed. Campinas: EDUNICAMP.\n\n\nDraper, Norman R., e Harry Smith. 1998. Applied Regression Analysis. 3º ed. New York: John Wiley & Sons.\n\n\nHarville, David A. 2000. Matrix Algebra from a Statistician’s Perspective. New York: Springer.\n\n\nHoffmann, Rodolfo. 2016. Análise de Regressão: Uma Introdução à Econometria. 5º ed. Portal de Livros Abertos da USP. https://doi.org/10.11606/9788592105709.\n\n\nKutner, Michael H., Christopher J. Nachtsheim, John Neter, e William Li. 2005. Applied Linear Statistical Models. 5º ed. New York: McGraw-Hill.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, e G. Geoffrey Vining. 2021. Introduction to Linear Regression Analysis. 6º ed. Hoboken: John Wiley & Sons.\n\n\nSearle, Shayle R. 2016. Matrix Algebra Useful for Statistics. 2º ed. Hoboken: Wiley.\n\n\nWeisberg, Sanford. 2005. Applied Linear Regression. New York: Wiley.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testes de hipóteses e ANOVA</span>"
    ]
  },
  {
    "objectID": "mrls_diagnostico.html",
    "href": "mrls_diagnostico.html",
    "title": "8  Diagnóstico e Avaliação no MRLS",
    "section": "",
    "text": "8.1 Por que analisar resíduos?\nApós o ajuste de um modelo de regressão, é essencial verificar se as hipóteses do MRLS do modelos para os erros aleatórios foram atendidas. Essa verificação se dá por diversos meios, sendo algumas dela via a análise dos resíduos.\nOs resíduos mais intuitivos são definidos como:\n\\[\ne_i = Y_i - \\hat{Y}_i, \\quad i=1,2,\\dots,n.\n\\]\nEstes resíduos representam a parte de \\(Y\\) que não foi explicada pelo modelo. Enquanto os erros verdadeiros \\(\\varepsilon_i\\) são inobserváveis, os resíduos são acessíveis e servem como suas aproximações.\nUm ponto conceitual importante é distinguir “hipóteses sobre os erros” de “propriedades dos resíduos”. As hipóteses clássicas do MRLS são formuladas para os erros aleatórios \\(\\varepsilon_i\\) (componentes não observáveis do mecanismo gerador de dados). Já os resíduos \\(e_i\\) são funções dos dados e dos estimadores, logo carregam restrições algébricas impostas pelo MQO. Assim, mesmo que o MRLS seja verdadeiro (isto é, as hipóteses sobre \\(\\varepsilon_i\\) sejam satisfeitas), os resíduos não se comportam como uma amostra i.i.d. de uma mesma distribuição; em particular, eles são correlacionados e apresentam variâncias diferentes ao longo de \\(i\\) a dependendo da alavancagem (Searle (2016); Harville (2000)).\nAs principais hipóteses do modelo para os erros (\\(\\varepsilon\\)) do MRLS são:\nPodem ser feitas hipóteses adicionais sobre a forma da distribuição dos erros, como assumir certa assimetria, curtose específica ou até uma distribuição conhecida.\nA suposição (hipótese) distribuição mais considerada para a distribuição dos erros é:\nUm modelo só pode ser considerado adequado se os resíduos se comportarem como erros aleatórios: sem tendência sistemática, com variância aproximadamente constante, não correlacionaos e, em muitos contextos, aproximadamente normais. Em prática aplicada, é útil interpretar isso como: (i) a média condicional foi bem especificada (linearidade na forma funcional), (ii) a variância condicional não muda de forma sistemática (homocedasticidade) e (iii) não há estrutura temporal/espacial remanescente (independência), além de (iv) normalidade como hipótese adicional que viabiliza inferência exata e diagnósticos probabilísticos baseados em caudas (Montgomery, Peck, e Vining (2021); Kutner et al. (2005)).",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diagnóstico e Avaliação no MRLS</span>"
    ]
  },
  {
    "objectID": "mrls_diagnostico.html#por-que-analisar-resíduos",
    "href": "mrls_diagnostico.html#por-que-analisar-resíduos",
    "title": "8  Diagnóstico e Avaliação no MRLS",
    "section": "",
    "text": "Média zero \\((E[\\varepsilon_i] = 0)\\)\nVariância constante \\((Var[\\varepsilon_i] = \\sigma^2)\\)\nNão correlação entre os erros \\((cov[\\varepsilon_i,\\varepsilon_j] = 0, \\forall i \\neq j)\\)\n\n\n\n\nNormalidade \\((\\varepsilon_i \\sim N(0,\\sigma^2))\\).",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diagnóstico e Avaliação no MRLS</span>"
    ]
  },
  {
    "objectID": "mrls_diagnostico.html#tipos-de-resíduos-e-propriedades",
    "href": "mrls_diagnostico.html#tipos-de-resíduos-e-propriedades",
    "title": "8  Diagnóstico e Avaliação no MRLS",
    "section": "8.2 Tipos de resíduos e propriedades",
    "text": "8.2 Tipos de resíduos e propriedades\n\n8.2.1 Resíduos ordinários\nO ponto de partida são os resíduos ordinários:\n\\[\ne_i = Y_i - \\hat{Y}_i.\n\\]\nEles indicam o desvio direto entre a observação e a reta ajustada. Por exemplo, \\(e_i &gt; 0\\) mostra que o modelo subestimou \\(Y_i\\), enquanto \\(e_i &lt; 0\\) mostra que o modelo superestimou.\nDo ponto de vista conceitual, o resíduo é uma estimativa observável do erro aleatório \\(\\varepsilon_i\\). Como \\(\\varepsilon_i\\) não é observável, toda a etapa de diagnóstico repousa sobre a análise do comportamento dos \\(e_i\\). Entretanto, é fundamental compreender que resíduos não são os erros verdadeiros: eles dependem dos parâmetros estimados e, portanto, carregam estrutura imposta pelo método de mínimos quadrados Hoffmann (2016); Montgomery, Peck, e Vining (2021).\n\n8.2.1.1 Propriedades básicas dos resíduos ordinários\nO método dos mínimos quadrados impõe três propriedades estruturais:\n\nSoma nula \\[\n\\sum_{i=1}^n e_i = 0.\n\\] A reta ajustada sempre passa pelo ponto médio amostral \\((\\bar X, \\bar Y)\\).\nOrtogonalidade com o preditor \\[\n\\sum_{i=1}^n e_i X_i = 0.\n\\] Não há associação linear entre os resíduos e a variável explicativa. Caso existisse, o modelo poderia ser melhorado ajustando novamente a inclinação.\nSoma de quadrados dos resíduos \\[\n\\sum_{i=1}^n e_i^2 = SQ_{Res},\n\\] isto é, os resíduos concentram exatamente a variabilidade não explicada pelo modelo.\n\nEssas propriedades decorrem diretamente das equações normais do método dos mínimos quadrados no caso univariado, obtidas pela minimização de \\(\\sum (Y_i - \\beta_0 - \\beta_1 X_i)^2\\) em relação a \\(\\beta_0\\) e \\(\\beta_1\\) (Charnet et al. (2008); Kutner et al. (2005)).\n\nApêndice de Demonstrações {#demo}: as propriedades acima são obtidas substituindo \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) nas expressões dos resíduos e manipulando os somatórios resultantes das equações normais.\n\nEssas três propriedades têm implicações importantes: mesmo que os erros verdadeiros sejam independentes e homocedásticos, os resíduos não são independentes entre si e tampouco possuem variância constante.\n\n\n8.2.1.2 Esperança, variância, covariância e distribuição dos resíduos ordinários\n\nEsperança\n\n\\[\nE[e_i] = 0.\n\\]\nSob as hipóteses do MRLS, cada resíduo tem média zero. Isso significa que, em termos probabilísticos, o modelo não superestima nem subestima sistematicamente a resposta.\n\nVariância\n\n\\[\nVar(e_i) = \\sigma^2 (1 - h_{ii}),\n\\]\nem que\n\\[\nh_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar X)^2}{S_{xx}}.\n\\]\nA quantidade \\(h_{ii}\\) é chamada de alavancagem da observação \\(i\\). Ela mede o quanto o valor de \\(X_i\\) influencia o próprio ajuste \\(\\hat Y_i\\).\nObservações com valores de \\(X_i\\) muito afastados da média \\(\\bar X\\) apresentam maior alavancagem. Como consequência, possuem menor variância residual, pois “ancoram” a reta ajustada com maior intensidade (Belsley, Kuh, e Welsch (1980); Montgomery, Peck, e Vining (2021)).\n\nCovariância\n\n\\[\nCov(e_i, e_j) = -\\sigma^2 h_{ij}, \\quad i \\neq j,\n\\]\nem que\n\\[\nh_{ij} = \\frac{1}{n} + \\frac{(x_i - \\bar X)(x_j - \\bar X)}{S_{xx}}.\n\\]\nPortanto, os resíduos são correlacionados entre si. Isso é consequência direta do fato de que todos os resíduos dependem dos mesmos estimadores \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) e, consequentemente, os resíduos não podem ser tratados como uma nova amostra independente de erros aleatórios (Kutner et al. (2005); Weisberg (2005)).\n\nDistribuição\n\nSe assumimos normalidade para os erros aleatórios,\n\\[\n\\varepsilon_i \\sim N(0,\\sigma^2),\n\\]\nentão os resíduos ordinários também seguem distribuição normal, pois são combinações lineares das variáveis \\(\\varepsilon_i\\):\n\\[\ne_i \\sim N\\!\\big(0,\\sigma^2(1-h_{ii})\\big).\n\\]\nEssa normalidade é exata sob a hipótese de erros normais. Caso a normalidade não seja assumida, a distribuição dos resíduos pode ser aproximada por resultados assintóticos.\n\nApêndice de Demonstrações {#demo}: as expressões de variância e covariância dos resíduos são obtidas substituindo \\(e_i = Y_i - \\hat Y_i\\) e utilizando as propriedades das variâncias de combinações lineares, juntamente com as expressões explícitas de \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) Kutner et al. (2005); Montgomery, Peck, e Vining (2021).\n\n\n\n8.2.1.3 Implicações para diagnóstico\nEsses resultados mostram que, mesmo quando as hipóteses usuais de média zero, variância constante, não correlação e normalidade para os erros aleatórios são satisfeitas, os resíduos ordinários apresentam:\n\nvariância não constante (dependente de \\(h_{ii}\\)),\ncorrelação entre si,\ndependência dos parâmetros estimados.\n\nPortanto, embora úteis para visualização inicial e interpretação direta do ajuste, os resíduos ordinários não são ideais para comparações diretas entre observações com diferentes níveis de alavancagem.\nEssa limitação motiva a construção de resíduos transformados, como os resíduos padronizados e os resíduos estudentizados, que ajustam explicitamente a variabilidade individual e permitem diagnósticos mais adequados de pontos discrepantes e violações das hipóteses do modelo (Belsley, Kuh, e Welsch (1980); Weisberg (2005)).\n\n\n\n8.2.2 Resíduos padronizados\nCom o objetivo de tornar os resíduos comparáveis entre si, ajustando a diferença de variâncias individuais, definem-se os resíduos padronizados como\n\\[\nr_i = \\frac{e_i}{s \\sqrt{1 - h_{ii}}},\n\\quad \\text{com} \\quad\ns^2 = \\frac{SQ_{Res}}{n-2}.\n\\]\nAqui, \\(e_i\\) é o resíduo ordinário, \\(h_{ii}\\) é a alavancagem da observação \\(i\\) e \\(s^2\\) é o estimador não viesado de \\(\\sigma^2\\). A ideia central é simples: como\n\\[\nVar(e_i) = \\sigma^2 (1 - h_{ii}),\n\\]\ndividir \\(e_i\\) por uma estimativa de seu desvio-padrão elimina a heterogeneidade de variâncias e produz uma quantidade adimensional.\nDo ponto de vista conceitual, essa padronização desempenha papel análogo ao de uma estatística \\(z\\): ela mede o “tamanho” do desvio em unidades de desvio-padrão estimado.\n\n8.2.2.1 Propriedades fundamentais\nSob as hipóteses do MRLS:\n\nEsperança aproximada \\[\nE[r_i] \\approx 0.\n\\]\nVariância aproximada \\[\nVar(r_i) \\approx 1.\n\\]\n\nA aproximação decorre do fato de que \\(s^2\\) é uma estimativa de \\(\\sigma^2\\). Se \\(\\sigma^2\\) fosse conhecido, teríamos exatamente\n\\[\n\\frac{e_i}{\\sigma \\sqrt{1-h_{ii}}} \\sim N(0,1),\n\\]\nsob normalidade dos erros.\nEntretanto, como \\(\\sigma^2\\) é substituído por \\(s^2\\), a estatística passa a envolver uma razão entre variáveis aleatórias dependentes.\n\n\n8.2.2.2 Distribuição dos resíduos padronizados\nSe os erros seguem\n\\[\n\\varepsilon_i \\sim N(0,\\sigma^2),\n\\]\nentão, para amostras moderadas ou grandes, vale a aproximação:\n\\[\nr_i \\approx t_{n-2}.\n\\]\nA aproximação não é exata porque \\(e_i\\) e \\(s^2\\) não são independentes: ambos dependem das mesmas observações e dos mesmos estimadores \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) (Weisberg (2005)).\nEm amostras grandes, pela consistência de \\(s^2\\) para \\(\\sigma^2\\), a distribuição de \\(r_i\\) aproxima-se da normal padrão:\n\\[\nr_i \\overset{aprox}{\\sim} N(0,1).\n\\]\n\nApêndice de Demonstrações {#demo}: a aproximação \\(r_i \\approx t_{n-2}\\) decorre da substituição de \\(\\sigma^2\\) por \\(s^2\\) na padronização e do fato de que \\((n-2)s^2/\\sigma^2 \\sim \\chi^2_{n-2}\\) sob normalidade dos erros.\n\n\n\n8.2.2.3 Interpretação prática\nOs resíduos padronizados permitem comparar observações com diferentes alavancagens. Um mesmo valor absoluto de resíduo ordinário pode ser pequeno ou grande dependendo de \\(h_{ii}\\). A padronização corrige esse efeito.\nUma regra prática frequentemente utilizada é:\n\n\\(|r_i| &gt; 2\\) → possível observação discrepante.\n\\(|r_i| &gt; 3\\) → forte indício de discrepância.\n\nEsses limiares baseiam-se na probabilidade de observar valores extremos sob uma distribuição aproximadamente normal ou \\(t\\). Por exemplo, sob normalidade, a probabilidade de \\(|Z|&gt;2\\) é aproximadamente 5%.\nContudo, essa interpretação deve ser feita com cautela:\n\nEm amostras grandes, é esperado que alguns valores ultrapassem 2 apenas por variabilidade natural.\nEm amostras pequenas, a aproximação pode ser imprecisa.\nA presença de múltiplos testes simultâneos pode inflar a taxa de falsos positivos.\n\n\n\n8.2.2.4 Limitações conceituais\nApesar de mais informativos que os resíduos ordinários, os resíduos padronizados ainda apresentam uma limitação importante: o denominador \\(s\\) é calculado utilizando todas as observações, inclusive a própria observação \\(i\\).\nAssim, um ponto extremo pode inflar \\(s\\), reduzindo artificialmente seu próprio resíduo padronizado, fenômeno conhecido como masking (mascaramento) (Belsley, Kuh, e Welsch (1980)).\nEssa limitação motiva a definição dos resíduos estudentizados externos, nos quais a variância é estimada excluindo-se a própria observação sob análise.\nEm síntese:\n\nResíduos ordinários medem o erro bruto.\nResíduos padronizados tornam os erros comparáveis.\nA padronização é essencial para diagnóstico formal de outliers e para construção de gráficos de resíduos mais informativos.\n\nNos próximos tópicos, veremos como a estudentização externa corrige a dependência entre numerador e denominador e fornece uma estatística com distribuição \\(t\\) exata sob as hipóteses do modelo.\n\n\n\n8.2.3 Resíduos estudentizados (externos)\nOs resíduos estudentizados externos (também chamados de externally studentized residuals ou deleted residuals) foram propostos no contexto de diagnóstico de regressão para contornar a dependência entre numerador e denominador presente nos resíduos padronizados (Belsley, Kuh, e Welsch (1980); Weisberg (2005)).\nEles são definidos por\n\\[\nt_i^* = \\frac{e_i}{s_{(i)} \\sqrt{1 - h_{ii}}},\n\\]\nem que:\n\n\\(e_i\\) é o resíduo ordinário da observação \\(i\\);\n\\(h_{ii}\\) é a alavancagem da observação \\(i\\);\n\\(s_{(i)}^2\\) é o estimador da variância do erro calculado excluindo a i-ésima observação.\n\nIsto é, \\(s_{(i)}^2\\) é obtido ajustando o modelo com \\(n-1\\) observações, removendo o ponto \\(i\\). Assim, o denominador não sofre influência direta da própria observação cujo resíduo está sendo avaliado.\n\n8.2.3.1 Motivação conceitual\nNos resíduos padronizados,\n\\[\nr_i = \\frac{e_i}{s \\sqrt{1-h_{ii}}},\n\\]\nComo apresentado anteriormente, o estimador \\(s^2\\) é calculado usando todas as observações. Se a observação \\(i\\) for discrepante, ela pode inflar \\(s^2\\), reduzindo artificialmente \\(|r_i|\\) e dificultando sua própria detecção.\nAo substituir \\(s\\) por \\(s_{(i)}\\), eliminamos essa retroalimentação. O resíduo passa a ser avaliado em relação a um modelo que não foi influenciado por ele mesmo.\n\n\n8.2.3.2 Distribuição exata\nSob as hipóteses do MRLS com erros normais,\n\\[\n\\varepsilon_i \\sim N(0,\\sigma^2),\n\\]\ntemos que\n\\[\nt_i^* \\sim t_{n-3}.\n\\]\nA perda de um grau de liberdade adicional (em comparação com \\(t_{n-2}\\)) decorre do fato de que a variância foi estimada com \\(n-3\\) graus de liberdade no modelo ajustado sem a observação \\(i\\) (Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).\nEssa é uma propriedade importante: diferentemente dos resíduos padronizados, aqui a distribuição \\(t\\) é exata sob normalidade dos erros.\n\nApêndice de Demonstrações {#demo}: a distribuição exata de \\(t_i^*\\) é obtida mostrando que, sob \\(H_0\\), o numerador é normal e independente do estimador \\(s_{(i)}^2\\), o qual é proporcional a uma variável qui-quadrado com \\(n-3\\) graus de liberdade.\n\n\n\n8.2.3.3 Interpretação prática\nComo \\(t_i^*\\) segue exatamente uma distribuição \\(t\\), podemos utilizar pontos críticos formais para avaliar discrepância individual:\n\n\\(|t_i^*| &gt; t_{1-\\alpha/2,\\, n-3}\\) → evidência de que a observação \\(i\\) é discrepante ao nível \\(\\alpha\\).\n\nNa prática:\n\n\\(|t_i^*| &gt; 2\\) sugere possível discrepância;\n\\(|t_i^*| &gt; 3\\) indica forte indício de outlier, especialmente em amostras moderadas.\n\nElevando ao quadrado:\n\\[\nt_i^{*2} \\sim F_{1,n-3},\n\\]\npois o quadrado de uma variável com distribuição \\(t_k\\) segue distribuição \\(F_{1,k}\\) (Casella e Berger (2002)). Essa relação conecta o diagnóstico individual de observações com a lógica dos testes \\(F\\) discutidos anteriormente.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diagnóstico e Avaliação no MRLS</span>"
    ]
  },
  {
    "objectID": "mrls_diagnostico.html#influência-alavancagem-e-leitura-conjunta-dos-resíduos",
    "href": "mrls_diagnostico.html#influência-alavancagem-e-leitura-conjunta-dos-resíduos",
    "title": "8  Diagnóstico e Avaliação no MRLS",
    "section": "8.3 Influência, alavancagem e leitura conjunta dos resíduos",
    "text": "8.3 Influência, alavancagem e leitura conjunta dos resíduos\nA etapa mais importante do diagnóstico no MRLS consiste em integrar três dimensões distintas, mas complementares:\n\ndiscrepância na variável resposta (\\(Y\\));\nposição extrema na variável explicativa (\\(X\\));\nimpacto global sobre os estimadores do modelo.\n\nEssa integração é fundamental para evitar conclusões equivocadas baseadas apenas no tamanho do resíduo.\n\n8.3.1 Relação entre discrepância, alavancagem e influência\nÉ importante distinguir conceitualmente:\n\nPossível outlier em \\(Y\\): grande \\(|t_i^*|\\);\nAlta alavancagem: grande \\(h_{ii}\\);\nObservação influente: combinação de grande \\(|t_i^*|\\) e grande \\(h_{ii}\\).\n\nUm ponto pode apresentar alto resíduo, mas baixa alavancagem, afetando pouco a inclinação da reta. Nesse caso, ele é discrepante na resposta, mas não necessariamente influente.\nPor outro lado, uma observação pode ter pequena discrepância em \\(Y\\), mas alta alavancagem em \\(X\\), alterando significativamente a inclinação estimada \\(\\hat\\beta_1\\). Nesse caso, mesmo com resíduo pequeno, o ponto pode ser estruturalmente influente.\n\n\n8.3.2 Alavancagem no MRLS\nA alavancagem da observação \\(i\\) é dada por\n\\[\nh_{ii} = \\frac{1}{n} + \\frac{(X_i - \\bar X)^2}{S_{xx}},\n\\quad \\text{com} \\quad\nS_{xx} = \\sum_{j=1}^n (X_j - \\bar X)^2.\n\\]\nEla mede o quanto o valor de \\(X_i\\) influencia o próprio ajuste \\(\\hat{Y}_i\\).\nPropriedades importantes no MRLS:\n\n\\(0 &lt; h_{ii} &lt; 1\\);\n\\[\n\\sum_{i=1}^n h_{ii} = 2,\n\\] pois dois parâmetros são estimados (\\(\\beta_0\\) e \\(\\beta_1\\));\na média das alavancagens é \\(2/n\\).\n\nObservações com \\(X_i\\) muito afastado da média \\(\\bar X\\) possuem maior alavancagem e exercem maior influência geométrica sobre a reta ajustada (Montgomery, Peck, e Vining (2021); Kutner et al. (2005)).\nUma regra prática comum é considerar como potencialmente alta alavancagem valores tais que\n\\[\nh_{ii} &gt; \\frac{2p}{n},\n\\]\nem que \\(p\\) é o número de parâmetros do modelo (no MRLS, \\(p=2\\)). Assim, valores acima de \\(4/n\\) merecem atenção especial (Belsley, Kuh, e Welsch (1980)).\n\n\n8.3.3 Conexão entre alavancagem e variância residual\nRecordando que\n\\[\nVar(e_i) = \\sigma^2 (1 - h_{ii}),\n\\]\nvemos que observações com maior alavancagem apresentam menor variância residual. Isso ocorre porque esses pontos “puxam” a reta para mais perto de si.\nPortanto, um ponto com alto \\(h_{ii}\\) pode ter resíduo pequeno não porque esteja bem ajustado, mas porque influenciou fortemente o ajuste.\nEssa distinção é conceitualmente importante:\n\nResíduo mede discrepância vertical.\nAlavancagem mede posição extrema em \\(X\\).\nInfluência mede alteração no modelo quando a observação é removida.\n\n\n\n8.3.4 Síntese diagnóstica\nA leitura conjunta pode ser organizada da seguinte forma:\n\nResíduos grandes + baixa alavancagem\n→ outliers na resposta (\\(Y\\)), com impacto limitado na inclinação.\nResíduos pequenos + alta alavancagem\n→ observações potencialmente influentes, mesmo sem grande discrepância aparente.\nResíduos grandes + alta alavancagem\n→ casos críticos, com forte potencial de distorcer significativamente o ajuste.\n\nMedidas integradas, como a distância de Cook,\n\\[\nD_i = \\frac{t_i^{*2}}{2} \\cdot \\frac{h_{ii}}{1 - h_{ii}},\n\\]\nquantificam diretamente o quanto os estimadores \\((\\hat\\beta_0,\\hat\\beta_1)\\) se alterariam caso a observação \\(i\\) fosse removida (Belsley, Kuh, e Welsch (1980); Weisberg (2005)).\n\n\n8.3.5 Resumo comparativo dos resíduos\n\n\n\n\n\n\n\n\n\n\nTipo de resíduo\nFórmula\n\\(E(.)\\)\n\\(Var(.)\\)\nDistribuição\n\n\n\n\nOrdinário \\(e_i\\)\n\\(Y_i - \\hat Y_i\\)\n\\(0\\)\n\\(\\sigma^2 (1 - h_{ii})\\)\n\\(N(0, \\sigma^2 (1 - h_{ii}))\\)\n\n\nPadronizado \\(r_i\\)\n\\(\\dfrac{e_i}{s \\sqrt{1-h_{ii}}}\\)\n\\(\\approx 0\\)\n\\(\\approx 1\\)\nAprox. \\(t_{n-2}\\)\n\n\nEstudentizado \\(t_i^*\\)\n\\(\\dfrac{e_i}{s_{(i)} \\sqrt{1-h_{ii}}}\\)\n\\(0\\)\n\\(1\\)\n\\(t_{n-3}\\)\n\n\n\nEm síntese:\n\nResíduos ordinários fornecem a discrepância bruta.\nResíduos padronizados tornam as observações comparáveis.\nResíduos estudentizados externos permitem inferência formal com distribuição \\(t\\) exata sob normalidade.\nAlavancagem identifica observações estruturalmente extremas.\nMedidas de influência integram discrepância e posição.\n\nSomente essa leitura integrada permite avaliar adequadamente a robustez do ajuste no MRLS e identificar observações com potencial de comprometer a inferência estatística.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diagnóstico e Avaliação no MRLS</span>"
    ]
  },
  {
    "objectID": "mrls_diagnostico.html#testes-formais-dos-resíduos",
    "href": "mrls_diagnostico.html#testes-formais-dos-resíduos",
    "title": "8  Diagnóstico e Avaliação no MRLS",
    "section": "8.4 Testes formais dos resíduos",
    "text": "8.4 Testes formais dos resíduos\nAntes da inspeção gráfica, é possível realizar testes estatísticos formais aplicados aos resíduos do MRLS. Esses testes não substituem a análise gráfica, mas fornecem evidência quantitativa sobre possíveis violações das hipóteses clássicas, especialmente normalidade e independência dos erros.\nÉ fundamental compreender que tais testes avaliam hipóteses específicas do modelo (por exemplo, normalidade dos erros), e não a “qualidade geral” da regressão. A interpretação correta exige articulação entre teoria, estatística e contexto (Kutner et al. (2005); Montgomery, Peck, e Vining (2021); Weisberg (2005)).\n\n8.4.1 Teste para Assimetria (Skewness)\nA estatística de assimetria é definida por\n\\[\nS = \\frac{\\frac{1}{n}\\sum_{i=1}^n (e_i - \\bar e)^3}\n{\\left(\\frac{1}{n}\\sum_{i=1}^n (e_i - \\bar e)^2\\right)^{3/2}},\n\\]\ncujo valores de referência são:\n\n\\(S=0\\) → simetria;\n\\(S&gt;0\\) → cauda longa à direita;\n\\(S&lt;0\\) → cauda longa à esquerda.\n\nSob a hipótese de normalidade dos resíduos, podemos formular as seguintes hipóteses:\n\\[\nH_0: \\text{Distribuição simétrica (} S = 0\\text{)}\n\\]\n\\[\nH_1: \\text{Distribuição assimétrica (} S \\neq 0\\text{)}\n\\]\nPara amostras grandes, vale a aproximação assintótica:\n\\[\nZ_S = \\sqrt{\\frac{n}{6}}\\, S \\sim N(0,1).\n\\] Esse teste verifica se há evidência estatística de assimetria na distribuição residual. Valores positivos indicam cauda longa à direita; valores negativos indicam cauda longa à esquerda.\nAssimetria residual pode indicar: - variável resposta naturalmente assimétrica (ex.: tempos, rendas); - necessidade de transformação; - presença de outliers em apenas um lado da distribuição.\nA assimetria detectada estatisticamente pode ser irrelevante do ponto de vista prático se o impacto sobre estimativas e previsões for pequeno. Por isso, a análise gráfica (histograma e QQ-plot) é complementar e essencial (Weisberg (2005); Montgomery, Peck, e Vining (2021)).\n\n\n8.4.2 Teste para Curtose (Kurtosis)\nA curtose é definida por\n\\[\nK = \\frac{\\frac{1}{n}\\sum_{i=1}^n (e_i - \\bar e)^4}\n{\\left(\\frac{1}{n}\\sum_{i=1}^n (e_i - \\bar e)^2\\right)^2},\n\\]\ncom os seguinte valores de referência:\n\n\\(K=3\\) → normal (mesocúrtica);\n\\(K&gt;3\\) → caudas pesadas (leptocúrtica);\n\\(K&lt;3\\) → caudas leves (platicúrtica).\n\nSob a hipótese de normalidade dos resíduos, podemos formular as seguintes hipóteses:\n\\[\nH_0: K = 3\n\\]\n\\[\nH_1: K \\neq 3\n\\]\nPara amostras grandes:\n\\[\nZ_K = \\sqrt{\\frac{n}{24}} (K - 3) \\sim N(0,1).\n\\]\nCurtose elevada frequentemente sinaliza presença de outliers ou heterogeneidade de variância. Caudas pesadas significam maior probabilidade de valores extremos, o que pode afetar inferência e previsão.\nAssim como a assimetria, a curtose deve ser interpretada junto com resíduos estudentizados e medidas de influência. Muitas vezes, poucos pontos extremos explicam grande parte da rejeição da normalidade (Belsley, Kuh, e Welsch (1980); Weisberg (2005)).\n\n\n8.4.3 3. Omnibus Test (D’Agostino–Pearson)\nO teste Omnibus combina os dois testes anteriores (assimetria e curtose) em uma única estatística.\nSejam:\n\\[\nZ_1 = Z_S\n\\quad \\text{e} \\quad\nZ_2 = Z_K.\n\\]\nA estatística do teste é:\n\\[\nOM = Z_1^2 + Z_2^2.\n\\]\nOu seja, \\(Z_1\\) é a estatística padronizada da assimetria e \\(Z_2\\) a da curtose. Sob \\(H_0\\) (normalidade), vale assintoticamente:\nPara o teste Omnibus, formulmos as seguintes hipóteses: \\[\nH_0: \\text{Resíduos seguem distribuição normal}\n\\]\n\\[\nH_1: \\text{Resíduos não seguem distribuição normal}\n\\] Sob \\(H_0\\),\n\\[\nOM \\sim \\chi^2_{(2)}.\n\\]\nO Omnibus é um teste conjunto: ele detecta qualquer violação que afete simetria ou curtose. Em vez de avaliar dois testes separados, consolida evidência em uma única estatística. Adicionalmente, como a distribuição é assintótica, sua confiabilidade aumenta com o tamanho amostral. Em amostras pequenas, o teste pode apresentar distorções no nível de significância.\n\n\n8.4.4 4. Jarque–Bera (JB)\nO teste de Jarque–Bera também combina assimetria e curtose, mas diretamente em termos de seus estimadores:\n\\[\nJB = \\frac{n}{6}\\left(S^2 + \\frac{(K-3)^2}{4}\\right).\n\\]\nHipóteses do teste são:\n\\[\nH_0: \\text{Resíduos seguem distribuição normal}\n\\]\n\\[\nH_1: \\text{Resíduos não seguem distribuição normal}\n\\]\nSob \\(H_0\\),\n\\[\nJB \\sim \\chi^2_{(2)}.\n\\]\nObserve que o JB é equivalente, do ponto de vista assintótico, à soma dos quadrados das versões padronizadas de \\(S\\) e \\(K-3\\).\nO JB mede a distância conjunta entre a distribuição empírica dos resíduos e a normal, considerando forma (assimetria) e peso de caudas (curtose).\nNote que rejeitar normalidade não implica que o modelo linear esteja incorreto, isso pode indicar apenas que os erros não são gaussianos. A relevância prática depende do objetivo (estimação, teste, previsão) e do tamanho da amostra (Casella e Berger (2002); Kutner et al. (2005)). ### 5. Durbin–Watson (DW)\nO teste de Durbin–Watson verifica autocorrelação serial:\n\\[\nDW = \\frac{\\sum_{t=2}^n (e_t - e_{t-1})^2}\n{\\sum_{t=1}^n e_t^2}.\n\\]\nAs hipóteses clássicas são:\n\\[\nH_0: \\rho = 0 \\quad (\\text{ausência de autocorrelação})\n\\]\n\\[\nH_1: \\rho \\neq 0 \\quad (\\text{autocorrelação})\n\\]\nA interpretação usual é:\n\n\\(DW \\approx 2\\) → ausência de autocorrelação;\n\\(DW &lt; 2\\) → autocorrelação positiva;\n\\(DW &gt; 2\\) → autocorrelação negativa.\n\nO DW mede o quanto os resíduos consecutivos diferem entre si. Se \\(e_t\\) e \\(e_{t-1}\\) forem semelhantes (dependência positiva), o numerador será pequeno e \\(DW\\) ficará abaixo de 2.\nEste teste é especialmente relevante em dados ordenados temporalmente (econometria, séries temporais). Em dados sem ordem natural, sua aplicação é menos informativa (Gujarati (2006)).\nAutocorrelação residual pode indicar: - tendência não modelada; - variáveis omitidas; - estrutura dinâmica inerente ao fenômeno.\nDetectar autocorrelação é apenas o início do diagnóstico.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diagnóstico e Avaliação no MRLS</span>"
    ]
  },
  {
    "objectID": "mrls_diagnostico.html#diagnóstico-gráfico-do-mrls",
    "href": "mrls_diagnostico.html#diagnóstico-gráfico-do-mrls",
    "title": "8  Diagnóstico e Avaliação no MRLS",
    "section": "8.5 Diagnóstico gráfico do MRLS",
    "text": "8.5 Diagnóstico gráfico do MRLS\nA análise gráfica dos resíduos é uma das etapas mais importantes na verificação das hipóteses do MRLS. Os gráficos funcionam como ferramentas de diagnóstico visual, permitindo identificar padrões que revelem problemas estruturais no modelo Montgomery, Peck, e Vining (2021); Kutner et al. (2005); Weisberg (2005).\nEm um modelo bem especificado, os resíduos devem se comportar como ruído puro: dispersão aleatória em torno de zero, variância aproximadamente constante e sem estrutura aparente. Em termos práticos, isso significa que, condicionado aos valores de \\(X\\), não deve existir informação sistemática remanescente nos resíduos que pudesse ser capturada por uma reespecificação simples do modelo (por exemplo, inclusão de termos não lineares ou transformação da resposta) Montgomery, Peck, e Vining (2021); Weisberg (2005).\nA seguir, são descritos os principais gráficos e o que se esperar de cada um.\n\n8.5.1 Resíduos vs ajustados (linearidade e homoscedasticidade)\nEste é o gráfico diagnóstico mais usado na prática, pois confronta diretamente o “erro” estimado (resíduo) com o nível de resposta previsto pelo modelo.\n\nO que se espera:\n\npontos dispersos aleatoriamente em torno da linha horizontal \\(0\\), sem padrão definido.\n\namplitude (dispersão vertical) aproximadamente constante ao longo de toda a faixa de \\(\\hat Y_i\\).\n\npoucos pontos ultrapassando as faixas de referência usuais (por exemplo, \\(|r_i| \\approx 2\\) ou \\(|t_i^*| \\approx 2\\), dependendo do resíduo adotado).\n\nO que indica problema:\n\npadrão em curva → sugere que a relação média \\(E(Y\\mid X)\\) não está bem representada por uma função linear; pode indicar necessidade de termos como \\(X^2\\) ou outra reespecificação funcional.\n\nforma de funil (variância aumenta ou diminui com \\(\\hat Y_i\\)) → indício de heteroscedasticidade (variância não constante).\n\nconcentração de resíduos positivos (negativos) em certas regiões → modelo subestima (superestima) sistematicamente nessas regiões, sugerindo viés local de especificação.\n\npontos isolados muito afastados do conjunto principal → possível outlier/influência; a confirmação deve ser feita em leitura conjunta com resíduos estudentizados \\(t_i^*\\), alavancagem \\(h_{ii}\\) e medidas de influência como a distância de Cook Belsley, Kuh, e Welsch (1980); Weisberg (2005).\n\n\nPara diagnóstico visual, é recomendável utilizar resíduos que sejam comparáveis entre observações. Assim, em muitos contextos prefere-se plotar resíduos padronizados (\\(r_i\\)) ou estudentizados externos (\\(t_i^*\\)), em vez de resíduos ordinários (\\(e_i\\)), pois estes últimos têm variância dependente da alavancagem \\((1-h_{ii})\\) (Montgomery, Peck, e Vining (2021); Kutner et al. (2005)).\n\n\n\n\n\nResíduos vs Ajustados — Esquerda: Ajuste bom (homocedástico); Direita: Ajuste ruim (funil/heterocedasticidade). Linhas em 0 e ±2.\n\n\n\n\n\n\n8.5.2 Resíduos vs \\(X\\) (forma funcional)\nEste gráfico é conceitualmente muito próximo ao anterior, mas desloca o foco: em vez de relacionar os resíduos com os valores ajustados \\(\\hat Y_i\\), relaciona-os diretamente com a variável explicativa \\(X_i\\).\n\nO que se espera:\n\naleatoriedade semelhante ao gráfico anterior, mas agora em função de \\(X\\).\n\ndispersão aproximadamente constante ao longo de toda a faixa de \\(X\\).\n\nausência de estruturas sistemáticas associadas a regiões específicas de \\(X\\).\n\nO que indica problema:\n\nestruturas em forma de arco ou curva → o efeito de \\(X\\) pode ser não linear; o modelo linear \\(E(Y\\mid X)=\\beta_0+\\beta_1X\\) pode estar omitindo termos relevantes (por exemplo, \\(X^2\\) ou outra transformação).\n\npadrões em “S” ou mudança de inclinação → possível quebra de regime ou efeito estrutural não capturado.\n\nfaixas onde a dispersão muda → variação da variância conforme \\(X\\), sugerindo heteroscedasticidade.\n\nconcentração de pontos extremos em regiões específicas de \\(X\\) → possível influência associada a valores extremos da variável explicativa.\n\n\nEnquanto o gráfico resíduos vs ajustados enfatiza o comportamento do erro em relação à resposta prevista, o gráfico resíduos vs \\(X\\) enfatiza a adequação da forma funcional da regressão. Ele permite avaliar diretamente se a hipótese de linearidade entre \\(X\\) e a média condicional de \\(Y\\) é plausível (Montgomery, Peck, e Vining (2021); Kutner et al. (2005)).\nEste gráfico é especialmente informativo quando \\(X\\) possui interpretação física, econômica ou temporal clara. Nesses casos, padrões sistemáticos ao longo de \\(X\\) podem revelar efeitos omitidos, mudanças estruturais ou fenômenos não lineares que não são imediatamente visíveis no gráfico resíduos vs ajustados.\nAssim como no gráfico anterior, recomenda-se utilizar resíduos padronizados ou estudentizados para tornar a escala comparável entre observações, principalmente quando há variação relevante na alavancagem \\(h_{ii}\\).\n\n\n\n\n\nResíduos vs X — Esquerda: Ajuste bom (linear); Direita: Ajuste ruim (não linearidade em arco). Linhas em 0 e ±2.\n\n\n\n\n\n\n8.5.3 Resíduos estudentizados vs valores ajustados (outliers + estrutura)\nEste gráfico é uma versão refinada do gráfico resíduos vs ajustados, utilizando os resíduos estudentizados externos \\(t_i^*\\). Ele combina duas dimensões do diagnóstico: discrepância individual e possível estrutura sistemática.\n\nPor que usar:\n\ntornam resíduos comparáveis, pois ajustam pela variância individual de cada ponto, incorporando o fator \\((1-h_{ii})\\) associado à alavancagem.\n\nutilizam uma estimativa da variância \\(\\sigma^2\\) calculada sem a observação \\(i\\) (\\(s_{(i)}\\)), reduzindo o efeito de mascaramento que pode ocorrer quando um ponto extremo influencia a própria estimativa de variância.\n\npossuem, sob normalidade dos erros, distribuição exata \\(t_{n-3}\\), permitindo interpretação inferencial mais precisa (Montgomery, Peck, e Vining (2021); Kutner et al. (2005)).\n\nO que se espera:\n\naleatoriedade em torno da linha horizontal \\(0\\).\n\na maioria dos pontos entre \\(-2\\) e \\(+2\\), sendo raros valores com \\(|t_i^*|&gt;3\\) em amostras moderadas.\n\nausência de padrão sistemático ao longo da faixa de valores ajustados.\n\nO que indica problema:\n\npontos fora do intervalo \\([-2,2]\\) → observações potencialmente discrepantes; valores acima de \\(|t_i^*|&gt;3\\) são frequentemente considerados fortemente suspeitos.\n\nestruturas visíveis (curvas, funis) → possíveis violações de linearidade ou homocedasticidade, agora avaliadas com resíduos que já consideram diferenças de variância individual.\n\nconcentração de valores extremos em regiões de alta alavancagem → possível influência desproporcional sobre os estimadores.\n\n\nOs resíduos estudentizados externos medem o quanto cada observação se afasta do modelo ajustado, levando em conta tanto a variabilidade residual quanto sua própria posição geométrica no conjunto de dados. Assim, eles são especialmente adequados para identificar outliers reais, isto é, observações cuja discrepância não pode ser explicada apenas por sua alavancagem.\nUm ponto com resíduo ordinário grande pode deixar de parecer extremo após a estudentização se sua variância condicional for naturalmente maior. Por outro lado, um ponto que permanece extremo mesmo após a correção por \\((1-h_{ii})\\) e por \\(s_{(i)}\\) merece investigação cuidadosa — seja por erro de registro, seja por representar um fenômeno estrutural distinto (Belsley, Kuh, e Welsch (1980); Weisberg (2005)).\n\n\n\n\n\nResíduos estudentizados vs Ajustados — Esquerda: Ajuste bom; Direita: Ajuste ruim (curvatura + funil). Linhas em ±2.\n\n\n\n\n\n\n8.5.4 QQ-plot (normalidade)\nO gráfico QQ-plot (quantile–quantile) compara os quantis empíricos dos resíduos com os quantis teóricos de uma distribuição normal padrão. Ele é uma das ferramentas mais informativas para avaliar a hipótese de normalidade dos erros no MRLS (Montgomery, Peck, e Vining (2021); Kutner et al. (2005); Weisberg (2005)).\n\nO que se espera:\n\npontos aproximadamente alinhados em torno da reta de 45°, indicando que os resíduos seguem aproximadamente uma distribuição normal.\n\npequenas flutuações aleatórias ao redor da reta, especialmente no centro da distribuição.\n\nausência de desvios sistemáticos nas caudas.\n\nO que indica problema:\n\ndesvios sistemáticos nas extremidades → caudas mais pesadas (pontos afastados da reta nas pontas) ou mais leves que a normal.\n\ndesvios em formato de “S” → indício de assimetria dos resíduos.\n\nafastamentos persistentes ao longo de toda a reta → possível inadequação global da suposição de normalidade.\n\npontos isolados muito distantes nas pontas → presença de outliers, que podem ser responsáveis por grande parte da violação observada.\n\n\nO QQ-plot compara toda a forma da distribuição. Se os resíduos forem normais, seus quantis empíricos devem crescer linearmente com os quantis teóricos da normal. Desvios sistemáticos dessa linearidade indicam diferenças estruturais entre as distribuições.\nÉ fundamental interpretar o QQ-plot em conjunto com resíduos estudentizados e medidas de influência. Muitas vezes, poucos pontos extremos explicam a maior parte do desvio observado nas caudas. Além disso, pequenas curvaturas no centro do gráfico, especialmente em amostras grandes, podem não ter relevância prática para a inferência, sobretudo quando o objetivo principal é previsão e não testes exatos em pequenas amostras (Casella e Berger (2002); Kutner et al. (2005)).\nO QQ-plot, portanto, oferece uma visão global da normalidade e complementa tanto os testes formais (como Jarque–Bera) quanto os gráficos de histograma.\n\n\n\n\n\nQQ-plot dos resíduos — Esquerda: Ajuste bom (erros ~ Normal); Direita: Ajuste ruim (erros ~ t com caudas pesadas).\n\n\n\n\n\n\n8.5.5 Histograma (assimetria e caudas)\nO histograma dos resíduos é uma ferramenta complementar ao QQ-plot. Enquanto o QQ-plot enfatiza o alinhamento com a normal teórica por meio de quantis, o histograma permite visualizar diretamente a forma empírica da distribuição residual (Montgomery, Peck, e Vining (2021); Kutner et al. (2005); Weisberg (2005)).\n\nO que se espera:\n\ndistribuição aproximadamente simétrica em torno de zero.\n\nformato aproximadamente em sino (curva unimodal e suave).\n\nmaior concentração de valores próximos de \\(0\\), com frequência decrescente nas extremidades.\n\nO que indica problema:\n\nassimetria → possível necessidade de transformação na resposta (\\(Y\\)), como \\(\\log(Y)\\) ou \\(\\sqrt{Y}\\), especialmente quando a assimetria é estrutural e não causada por poucos pontos extremos.\n\ncaudas longas → presença de outliers ou distribuição com maior probabilidade de valores extremos do que a normal.\n\nbimodalidade ou múltiplos picos → possível mistura de grupos ou estrutura omitida no modelo (por exemplo, variável categórica não incluída).\n\nconcentração excessiva no centro com poucas observações nas extremidades → caudas leves (platicurtose), também incompatíveis com normalidade.\n\n\nO histograma fornece uma visão direta da densidade empírica dos resíduos. Em um modelo com erros normais, espera-se que a forma geral seja compatível com a curva Normal\\((0,\\sigma^2)\\). Desvios sistemáticos dessa forma indicam diferenças estruturais na distribuição do erro.\nAdicionamente, o histograma é sensível à escolha do número de classes (bins). Diferentes escolhas podem alterar a percepção visual da forma. Por isso, recomenda-se utilizá-lo em conjunto com o QQ-plot e com medidas numéricas de assimetria e curtose.\nAlém disso, é importante lembrar que pequenas assimetrias visuais, especialmente em amostras grandes, podem não comprometer de forma relevante a inferência baseada em MQO, cuja robustez assintótica é discutida em (Casella e Berger (2002); Kutner et al. (2005)).\n\n\n\n\n\nHistograma dos resíduos — Esquerda: Ajuste bom (simétrico); Direita: Ajuste ruim (assimetria à direita). Curva Normal(0,1) de referência.\n\n\n\n\n\n\n8.5.6 Resíduos estudentizados vs índice (pontos atípicos)\nEste gráfico apresenta os resíduos estudentizados externos \\(t_i^*\\) em função do índice da observação \\(i\\). Ele é particularmente útil para identificar observações discrepantes individuais, destacando sua posição relativa no conjunto de dados.\n\nPor que usar:\n\nsão melhores na detecção de outliers, pois corrigem a influência da própria observação ao utilizar a estimativa de variância \\(s_{(i)}\\), calculada sem o ponto \\(i\\).\n\npossuem distribuição \\(t_{n-3}\\) sob normalidade dos erros, permitindo interpretação inferencial direta.\n\nfacilitam a visualização de padrões associados à ordem natural dos dados (por exemplo, tempo ou sequência experimental) Montgomery, Peck, e Vining (2021); Kutner et al. (2005).\n\nO que se espera:\n\nquase todos os pontos entre \\(-2\\) e \\(+2\\).\n\nraros pontos ultrapassando \\(|t_i^*|&gt;3\\), especialmente em amostras moderadas.\n\nausência de padrões sistemáticos ao longo do índice.\n\nO que indica problema:\n\nvalores extremos em \\(|t_i^*|\\) → sugerem observações potencialmente discrepantes; quanto maior o valor absoluto, maior a evidência de que o ponto não é compatível com a variabilidade esperada sob o modelo.\n\nagrupamento de valores extremos em determinadas regiões do índice → pode indicar mudança estrutural, dependência temporal ou erro sistemático de medição.\n\npadrões alternados (positivo–negativo–positivo) → possível autocorrelação residual, especialmente quando os dados possuem ordem temporal.\n\n\nEste gráfico não apenas identifica outliers, mas também permite verificar se tais observações estão distribuídas aleatoriamente ao longo do conjunto de dados ou se seguem algum padrão estrutural.\nO significado do eixo “índice” depende do contexto. Se os dados tiverem uma ordem natural (tempo, experimento sequencial, posição espacial), padrões nesse gráfico podem indicar violação da hipótese de independência dos erros. Se não houver ordem natural, o gráfico atua principalmente como ferramenta de localização de observações discrepantes.\nAlém disso, um valor extremo em \\(t_i^*\\) não implica automaticamente exclusão da observação. Deve-se verificar conjuntamente a alavancagem \\(h_{ii}\\) e medidas de influência (como a distância de Cook) antes de qualquer decisão sobre reespecificação ou remoção de dados (Belsley, Kuh, e Welsch (1980); Weisberg (2005)).\n\n\n\n\n\nResíduos estudentizados vs índice — Esquerda: ajuste bom (Normal, sem outliers); Direita: ajuste ruim (caudas pesadas + outliers). Linhas em 0 e ±2; pontos extremos destacados.\n\n\n\n\n\n\n8.5.7 Resíduos estudentizados ao quadrado vs valores ajustados (heteroscedasticidade / influência)\nEste gráfico utiliza \\(t_i^{*2}\\) (resíduos estudentizados externos ao quadrado) no eixo vertical e os valores ajustados \\(\\hat Y_i\\) no eixo horizontal. Ao elevar ao quadrado, eliminamos o sinal e focamos exclusivamente na magnitude da discrepância, o que é particularmente útil para investigar padrões de variância.\n\nO que se espera:\n\ndispersão aproximadamente uniforme ao longo da faixa de \\(\\hat Y_i\\).\n\nausência de tendência sistemática crescente ou decrescente.\n\npontos distribuídos sem estrutura definida ao redor de um nível aproximadamente constante.\n\nO que indica problema:\n\ncrescimento ou redução sistemática de \\(t_i^{*2}\\) conforme \\(\\hat Y_i\\) aumenta → indício de heteroscedasticidade (variância não constante).\n\nestrutura em arco → possível não linearidade na função média.\n\npontos isolados com valores muito elevados de \\(t_i^{*2}\\) → observações potencialmente influentes ou discrepantes.\n\n\nSe o modelo satisfaz a hipótese de homocedasticidade, então \\(Var(e_i)\\) deve ser constante. Como \\(t_i^*\\) já corrige por \\((1-h_{ii})\\) e por \\(s_{(i)}\\), padrões sistemáticos em \\(t_i^{*2}\\) sugerem que a variância condicional de \\(Y\\) depende do nível da resposta, ou seja, \\(Var(Y \\mid X)\\) não é constante.\nAo trabalhar com o quadrado do resíduo, pequenas diferenças tornam-se mais visíveis. Por isso, esse gráfico frequentemente revela tendências de variância que não são tão evidentes no gráfico simples resíduos vs ajustados.\nA inclusão de uma curva suave (por exemplo, LOESS) auxilia na visualização de tendências médias na magnitude dos resíduos. Se essa curva apresentar inclinação clara ou formato sistemático, há evidência visual de heteroscedasticidade (Montgomery, Peck, e Vining (2021); Kutner et al. (2005); Weisberg (2005)).\nEste gráfico, portanto, complementa o diagnóstico tradicional, oferecendo uma perspectiva focada especificamente na estrutura da variância.\n\n\n\n\n\nValores ajustados vs resíduos estudentizados² — Esquerda: ajuste bom (dispersão uniforme); Direita: ajuste ruim (heteroscedasticidade/funil). LOWESS em vermelho.\n\n\n\n\n\n\n8.5.8 Alavancagem vs resíduos estudentizados (influência / Cook)\nEste gráfico combina duas dimensões centrais do diagnóstico no MRLS:\n- alavancagem (\\(h_{ii}\\)), que mede o quão extremo é o valor de \\(X_i\\);\n- resíduo estudentizado externo (\\(t_i^*\\)), que mede a discrepância vertical ajustada pela variância condicional.\nAo analisar ambos simultaneamente, obtemos uma visão direta da influência potencial de cada observação sobre os estimadores do modelo (Belsley, Kuh, e Welsch (1980); Weisberg (2005); Montgomery, Peck, e Vining (2021)).\n\nObjetivo:\n\nidentificar observações influentes, isto é, aquelas que combinam alto resíduo e alta alavancagem.\n\ndistinguir entre pontos apenas discrepantes (grande \\(|t_i^*|\\)) e pontos estruturalmente extremos (grande \\(h_{ii}\\)).\n\n\nO que se espera:\n\nmaioria dos pontos dentro da “nuvem central”, isto é, com valores moderados de \\(h_{ii}\\) e \\(|t_i^*| \\leq 2\\).\n\npoucos pontos próximos do limite usual de alavancagem (por exemplo, \\(h_{ii} &gt; 2p/n\\)).\n\nausência de observações simultaneamente extremas em ambas as direções.\n\nO que indica problema:\n\npontos afastados horizontalmente (alto \\(h_{ii}\\)) → observações com grande potencial de influenciar a inclinação da reta.\n\npontos afastados verticalmente (alto \\(|t_i^*|\\)) → observações discrepantes na resposta.\n\npontos afastados horizontal e verticalmente → fortes candidatos a observações influentes, com impacto potencialmente desproporcional sobre os estimadores.\n\n\n\nInfluência não é sinônimo de discrepância. Um ponto pode ter grande resíduo, mas baixa alavancagem, afetando pouco os coeficientes. Da mesma forma, um ponto pode ter alta alavancagem e resíduo pequeno, mas ainda assim influenciar a inclinação da reta por estar em região extrema de \\(X\\).\nObservações influentes não devem ser automaticamente removidas. Elas podem representar fenômenos legítimos do processo gerador dos dados. O papel do diagnóstico é identificar e compreender tais pontos, não eliminá-los mecanicamente.\n\n\n\n\n\nAlavancagem (h) vs resíduos estudentizados — Esquerda: ajuste bom; Direita: ajuste ruim com ponto de alta alavancagem e discrepância. Área ∝ distância de Cook. Linha vertical: h &gt; 2p/n.\n\n\n\n\nOs gráficos de resíduos são mapas visuais do ajuste. Eles sintetizam, de forma intuitiva, as hipóteses estruturais do MRLS e permitem avaliar se o modelo capturou adequadamente a relação entre \\(X\\) e \\(Y\\) (Montgomery, Peck, e Vining (2021); Kutner et al. (2005); Weisberg (2005)).\nQuando o modelo está bem especificado, espera-se que:\n\nos resíduos flutuem aleatoriamente em torno de zero;\n\na variância seja aproximadamente constante ao longo de toda a faixa de \\(X\\) ou \\(\\hat Y\\);\n\na distribuição seja aproximadamente normal (quando a inferência exata via \\(t\\) e \\(F\\) é relevante);\n\nnão existam observações com influência desproporcional sobre os estimadores.\n\nEssas características indicam que, condicionalmente a \\(X\\), o modelo não deixou estrutura sistemática não explicada.\nQuando há padrões, eles indicam possíveis caminhos de correção:\n\nCurvaturas nos gráficos → possível inadequação da forma funcional; pode ser necessária a inclusão de termos como \\(X^2\\), transformações em \\(X\\) (por exemplo, \\(\\log(X)\\)) ou outra reespecificação da função média.\n\nVariância crescente ou decrescente → indício de heteroscedasticidade; transformações em \\(Y\\) (como \\(\\log(Y)\\) ou \\(\\sqrt{Y}\\)) ou métodos que acomodem variância não constante podem ser considerados.\n\nAssimetria na distribuição dos resíduos → possível necessidade de transformação na resposta ou presença de outliers que devem ser investigados.\n\nObservações influentes → revisão individual do ponto, verificação de erros de registro ou análise substantiva do fenômeno representado.\n\nCada padrão visual corresponde a uma hipótese específica do modelo. Assim, o diagnóstico gráfico não é apenas uma etapa técnica, mas uma verificação das suposições matemáticas que fundamentam a inferência no MRLS.\nTransformações não devem ser aplicadas de forma automática ou mecânica. Elas devem ser justificadas teoricamente, interpretadas no contexto do problema e validadas por novo ciclo de diagnóstico após o reajuste do modelo. O processo é iterativo: ajustar → diagnosticar → reespecificar → diagnosticar novamente.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diagnóstico e Avaliação no MRLS</span>"
    ]
  },
  {
    "objectID": "mrls_diagnostico.html#aspectos-computacionais-para-resíduos-no-r",
    "href": "mrls_diagnostico.html#aspectos-computacionais-para-resíduos-no-r",
    "title": "8  Diagnóstico e Avaliação no MRLS",
    "section": "8.6 Aspectos computacionais para resíduos no R",
    "text": "8.6 Aspectos computacionais para resíduos no R\nPara a análise gráfica de resíduos no MRLS, podemos usar principalmente os pacotes:\n\nstats → para ajustar o modelo com lm() e extrair resíduos e ajustados (residuals(), fitted()).\nggplot2 → para construir gráficos com geom_point(), geom_hline(), geom_vline(), facet_wrap().\nbroom → para organizar saídas do modelo em data frames (augment()), incluindo resíduos e ajustados.\ncar (opcional) → para alguns diagnósticos e gráficos prontos (ex.: residualPlots()).\nggfortify (opcional) → para gráficos diagnósticos automáticos a partir de objetos lm (autoplot()).\nstats + ggplot2 → para QQ-plot (via qqnorm()/qqline() ou construção manual para ggplot2).\nMASS (opcional) → para simulações com distribuições alternativas quando necessário.\nlmtest / sandwich (opcional) → para testes formais (Breusch–Pagan etc.) e erros-padrão robustos.\n\nA seguir, quais funções e pacotes usar em cada gráfico:\n1. Resíduos vs. Valores Ajustados\n\nObjetivo: verificar aleatoriedade e homocedasticidade.\nFunções/objetos:\n\nmodelo &lt;- lm(Y ~ X, data=df)\nfitted &lt;- fitted(modelo)\nresid  &lt;- resid(modelo)\n\nEm ggplot2: geom_point() + geom_hline(yintercept=0, ...).\n\n2. Resíduos vs. Variável Explicativa (\\(X\\))\n\nObjetivo: avaliar linearidade da relação entre \\(Y\\) e \\(X\\).\nFunções/objetos:\n\nresid &lt;- resid(modelo)\n\nEm ggplot2: geom_point() com aes(x=X, y=resid) + geom_hline(yintercept=0, ...).\n\n3. Resíduos Estudentizados vs. Valores Ajustados\n\nObjetivo: detectar outliers e padrões (considerando alavancagem).\nFunções/objetos:\n\nstud &lt;- rstudent(modelo) (resíduos estudentizados externos)\nfitted &lt;- fitted(modelo)\n\nEm ggplot2: geom_point() + geom_hline(yintercept=c(-2,2), ...).\n\n4. QQ-Plot dos Resíduos\n\nObjetivo: verificar normalidade.\nFunções/objetos:\n\nresid &lt;- resid(modelo)\nBase R: qqnorm(resid); qqline(resid)\nPara ggplot2: qq &lt;- qqnorm(resid, plot.it=FALSE) e então geom_point() + geom_abline().\n\n\n5. Histograma dos Resíduos\n\nObjetivo: verificar forma aproximada da distribuição.\nFunções/objetos:\n\nresid_pad &lt;- scale(resid(modelo)) (opcional)\n\nEm ggplot2:\n\ngeom_histogram(aes(y=after_stat(density)))\nCurva Normal de referência: dnorm(x, 0, 1) via geom_line() com uma grade x.\n\n\n6. Resíduos Estudentizados vs. Índice da Observação\n\nObjetivo: identificar observações discrepantes.\nFunções/objetos:\n\nstud &lt;- rstudent(modelo)\nidx &lt;- seq_along(stud)\n\nEm ggplot2: geom_point() + geom_hline(yintercept=c(-2,2), ...).\n\n7. Valores Ajustados vs. Resíduos Estudentizados²\n\nObjetivo: investigar heterocedasticidade.\nFunções/objetos:\n\nfitted &lt;- fitted(modelo)\nt2 &lt;- rstudent(modelo)^2\n\nSuavização:\n\ngeom_smooth(method=\"loess\", se=FALSE, ...) (substitui o LOWESS do Python de forma direta).\n\n\n8. Alavancagem vs. Resíduos Estudentizados (Influence Plot)\n\nObjetivo: detectar observações influentes.\nFunções/objetos:\n\nh &lt;- hatvalues(modelo) (alavancagem)\nstud &lt;- rstudent(modelo) (resíduo estudentizado)\nck &lt;- cooks.distance(modelo) (distância de Cook)\n\nEm ggplot2: geom_point(aes(size=ck)) para tornar a área proporcional a Cook + linhas de referência (ex.: geom_vline() com regra 2(p+1)/n).\n\nResumo didático:\n\nAjuste o modelo com lm():\n\nmodelo &lt;- lm(Y ~ X, data = df)\n\nObtenha resíduos e ajustados:\n\nresid   &lt;- resid(modelo)\nfitted  &lt;- fitted(modelo)\n\nObtenha diagnósticos extras:\n\nstud &lt;- rstudent(modelo)\n\nh    &lt;- hatvalues(modelo)\n\nck   &lt;- cooks.distance(modelo)\n\n(Opcional) Organize tudo em um único data frame (facilita gráficos):\n\nbroom::augment(modelo) (traz .fitted, .resid, .std.resid, .hat, .cooksd)\n\n\nCom esses objetos, é possível montar todos os oito gráficos de resíduos apresentados neste capítulo.\n\n\n\n\nBelsley, David A., Edwin Kuh, e Roy E. Welsch. 1980. Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. New York: John Wiley & Sons.\n\n\nCasella, George, e Roger L. Berger. 2002. Statistical Inference. 2º ed. Pacific Grove: Duxbury.\n\n\nCharnet, Reinaldo, Carlos Alberto Freire, Eliane M. R. Charnet, e Helio Bonvino. 2008. Análise de Modelos de Regressão Linear com Aplicações. 2º ed. Campinas: EDUNICAMP.\n\n\nGujarati, Damodar N. 2006. Econometria Básica. 4º ed. Rio de Janeiro: Elsevier Campus.\n\n\nHarville, David A. 2000. Matrix Algebra from a Statistician’s Perspective. New York: Springer.\n\n\nHoffmann, Rodolfo. 2016. Análise de Regressão: Uma Introdução à Econometria. 5º ed. Portal de Livros Abertos da USP. https://doi.org/10.11606/9788592105709.\n\n\nKutner, Michael H., Christopher J. Nachtsheim, John Neter, e William Li. 2005. Applied Linear Statistical Models. 5º ed. New York: McGraw-Hill.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, e G. Geoffrey Vining. 2021. Introduction to Linear Regression Analysis. 6º ed. Hoboken: John Wiley & Sons.\n\n\nSearle, Shayle R. 2016. Matrix Algebra Useful for Statistics. 2º ed. Hoboken: Wiley.\n\n\nWeisberg, Sanford. 2005. Applied Linear Regression. New York: Wiley.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Diagnóstico e Avaliação no MRLS</span>"
    ]
  },
  {
    "objectID": "mrls_transf.html",
    "href": "mrls_transf.html",
    "title": "9  Transformações nas Variáveis",
    "section": "",
    "text": "9.1 Motivação\nQuando os gráficos de resíduos revelam padrões sistemáticos, heteroscedasticidade ou violações de normalidade, é sinal de que a especificação do modelo pode estar inadequada. Em termos formais, isso significa que ao menos uma das hipóteses centrais do MRLS — linearidade da função média, homoscedasticidade ou normalidade dos erros — pode não estar sendo satisfeita (Montgomery, Peck, e Vining (2021); Kutner et al. (2005)).\nA regressão linear simples assume que\n\\[\nE(Y \\mid X) = \\beta_0 + \\beta_1 X\n\\quad \\text{e} \\quad\nVar(Y \\mid X) = \\sigma^2,\n\\]\nou seja, uma relação linear na média e variância constante condicionalmente a \\(X\\). Quando os resíduos exibem padrões como funil (variância crescente), curvaturas sistemáticas ou forte assimetria, o modelo ajustado não está capturando adequadamente a estrutura do processo gerador dos dados.\nUma das abordagens mais tradicionais para lidar com tais situações é aplicar transformações matemáticas às variáveis, de modo a:\nHá duas formas de realizar tais transformações:\n(a) na variável resposta (\\(Y\\)) e\n(b) na variável explicativa (\\(X\\)).\nNote que, ao aplicarmos uma transformação de variáveis, estamos implicitamente redefinindo o modelo estatístico. Por exemplo, ao transformar \\(Y\\) em \\(\\log(Y)\\), deixamos de modelar diretamente \\(E(Y \\mid X)\\) e passamos a modelar \\(E(\\log Y \\mid X)\\). Isso altera a interpretação dos coeficientes, a distribuição assumida para os erros e, em certos casos, a própria classe de modelos implícita (Weisberg (2005); Kutner et al. (2005)).\nPortanto, transformações devem ser motivadas por evidência empírica (diagnóstico de resíduos) e, sempre que possível, por fundamentação teórica do fenômeno estudado. Aplicá-las de forma automática pode melhorar métricas numéricas de ajuste, mas comprometer a interpretação do modelo.\nAlém disso, a aplicação de uma transformação implica um novo ciclo completo de análise:\nEsse processo é inerentemente iterativo.\nQuando a transformação envolve apenas mudanças algébricas simples (como logaritmo ou raiz quadrada), as justificativas formais podem ser entendidas via propriedades de variância e distribuições conhecidas; demonstrações mais técnicas dessas propriedades podem ser consultadas no Apêndice de Demonstrações {#demo}.\nNas seções seguintes, analisaremos separadamente as transformações aplicadas à variável resposta e à variável explicativa, enfatizando sua motivação estatística e suas implicações interpretativas.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transformações nas Variáveis</span>"
    ]
  },
  {
    "objectID": "mrls_transf.html#motivação",
    "href": "mrls_transf.html#motivação",
    "title": "9  Transformações nas Variáveis",
    "section": "",
    "text": "estabilizar a variância;\naproximar a normalidade dos erros;\nlinearizar a relação entre as variáveis;\ntornar a interpretação estatística mais coerente com a estrutura do fenômeno estudado.\n\n\n\n\n\n\nAjustar o modelo transformado.\n\nReavaliar os resíduos.\n\nVerificar se as hipóteses agora são plausíveis.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transformações nas Variáveis</span>"
    ]
  },
  {
    "objectID": "mrls_transf.html#transformações-na-variável-resposta-y",
    "href": "mrls_transf.html#transformações-na-variável-resposta-y",
    "title": "9  Transformações nas Variáveis",
    "section": "9.2 Transformações na variável resposta (\\(Y\\))",
    "text": "9.2 Transformações na variável resposta (\\(Y\\))\nAs transformações na variável resposta têm como objetivo principal modificar a estrutura probabilística condicional de \\(Y\\) dado \\(X\\). Em muitos contextos aplicados, a violação das hipóteses do MRLS decorre não da forma funcional da média, mas do comportamento da variância ou da distribuição dos erros.\nRecordando que o MRLS assume \\(Var(Y \\mid X) = \\sigma^2\\),qualquer evidência de que\n\\[\nVar(Y \\mid X) \\neq \\text{constante}\n\\]\nou que os erros apresentam forte assimetria ou caudas pesadas pode motivar uma transformação em \\(Y\\) (Montgomery, Peck, e Vining (2021); Kutner et al. (2005); Weisberg (2005)).\nPara exemplificar, suponha que a variância condicional dependa da média:\n\\[\nVar(Y \\mid X) = g\\big(E(Y \\mid X)\\big),\n\\]\npara alguma função \\(g(\\cdot)\\) crescente. Isso ocorre, por exemplo:\n\nquando a variância é proporcional à média (dados de contagem);\nquando a variância cresce aproximadamente com o quadrado da média (dados positivos e assimétricos);\nquando há padrão de funil nos resíduos vs. ajustados.\n\nNesse cenário, podemos buscar uma transformação \\(Y^* = h(Y)\\) tal que\n\\[\nVar(Y^* \\mid X) \\approx \\text{constante}.\n\\]\nEsse princípio é conhecido como estabilização da variância, e sua justificativa formal pode ser obtida por expansão de Taylor de primeira ordem (ver Apêndice de Demonstrações {#demo}).\nÉ importante destacar que transformar \\(Y\\) altera simultaneamente: - a escala da resposta; - a forma da distribuição condicional; - a interpretação dos coeficientes.\nPortanto, não estamos apenas “melhorando resíduos”, mas redefinindo o modelo estatístico.\n\n9.2.1 Transformação e distribuição implícita\nSe ajustamos o modelo\n\\[\nY_i^* = h(Y_i) = \\beta_0^* + \\beta_1^* X_i + \\varepsilon_i,\n\\quad \\varepsilon_i \\sim N(0,\\sigma^2),\n\\]\nentão estamos assumindo normalidade na escala transformada.\nIsso implica que, na escala original, \\(Y\\) possui distribuição derivada da transformação inversa. Por exemplo:\n\nse \\(h(Y)=\\log Y\\), então \\(Y\\) é log-normal condicionalmente a \\(X\\);\nse \\(h(Y)=\\sqrt Y\\), a distribuição resultante não é normal na escala original, mas a variância pode se tornar aproximadamente constante;\nse \\(h(Y)=1/Y\\), a estrutura média passa a ser modelada em termos do inverso da resposta.\n\nEssa mudança tem implicações importantes para:\n\ninterpretação de coeficientes;\nconstrução de intervalos de confiança;\nprevisão na escala original.\n\nAo transformar \\(Y\\), a hipótese de normalidade passa a ser avaliada na nova escala. Isso significa que testes \\(t\\), testes \\(F\\) e intervalos de confiança são válidos sob a suposição de normalidade dos erros na escala transformada, não necessariamente na escala original (Kutner et al. (2005)).\nNas subseções seguintes, discutiremos transformações específicas, a saber, logaritmo, raiz quadrada, inverso e potência.\n\n\n9.2.2 Logaritmo (\\(Y^* = \\log Y\\))\nA transformação logarítmica é uma das mais utilizadas em regressão, especialmente quando \\(Y\\) é positiva e apresenta assimetria à direita ou variância crescente com o nível médio.\nSe ajustamos o modelo\n\\[\nY_i^* = \\log(Y_i) = \\beta_0^* + \\beta_1^* X_i + \\varepsilon_i,\n\\quad \\varepsilon_i \\sim N(0, \\sigma^2),\n\\]\nestamos assumindo normalidade dos erros na escala logarítmica. Isso implica que, condicionalmente a \\(X_i\\), a variável original \\(Y_i\\) segue uma distribuição log-normal (Kutner et al. (2005); Weisberg (2005)).\nMais precisamente, se\n\\[\n\\log(Y_i) \\mid X_i \\sim N(\\mu_i, \\sigma^2),\n\\]\nentão\n\\[\nY_i \\mid X_i \\sim \\text{LogNormal}(\\mu_i, \\sigma^2).\n\\]\nA transformação logarítmica converte relações multiplicativas em aditivas. Se, na escala original,\n\\[\nY = \\alpha \\exp(\\beta X) \\cdot U,\n\\]\nonde \\(U\\) é um termo multiplicativo de erro, então\n\\[\n\\log Y = \\log \\alpha + \\beta X + \\log U,\n\\]\nou seja, o modelo passa a ter estrutura linear com erro aditivo na escala transformada.\nIsso significa que o logaritmo é particularmente adequado quando:\n\na variabilidade é proporcional ao nível médio;\no crescimento é aproximadamente exponencial;\no erro atua de forma multiplicativa na escala original.\n\nNo modelo\n\\[\nE[\\log(Y) \\mid X] = \\beta_0^* + \\beta_1^* X,\n\\]\no coeficiente \\(\\beta_1^*\\) representa a variação aditiva no logaritmo da resposta para cada unidade adicional em \\(X\\).\nNa escala original,\n\\[\nE(Y\\mid X) = \\exp(\\beta_0^*) \\cdot \\exp(\\beta_1^* X).\n\\]\nAssim, um aumento de 1 unidade em \\(X\\) multiplica o valor esperado de \\(Y\\) por \\(\\exp(\\beta_1^*)\\).\nPara valores pequenos de \\(\\beta_1^*\\), podemos usar a aproximação\n\\[\n\\exp(\\beta_1^*) \\approx 1 + \\beta_1^*,\n\\]\no que leva à interpretação aproximada de que \\(\\beta_1^* \\times 100\\%\\) representa uma variação percentual em \\(Y\\) para cada unidade de \\(X\\).\nEssa interpretação percentual é uma aproximação válida apenas quando \\(|\\beta_1^*|\\) é pequeno. O efeito exato é multiplicativo e deve ser interpretado via \\(\\exp(\\beta_1^*)\\).\n\n9.2.2.1 Variância e estabilização\nSe a variância cresce aproximadamente de forma proporcional ao quadrado da média,\n\\[\nVar(Y \\mid X) \\propto [E(Y \\mid X)]^2,\n\\]\nentão a transformação logarítmica tende a produzir variância aproximadamente constante na escala transformada.\nAdvertência sobre retransformação\nAo obter previsões na escala original, não é correto simplesmente calcular\n\\[\n\\widehat{Y} = \\exp(\\widehat{\\log Y}),\n\\]\npois, para variável log-normal,\n\\[\nE(Y \\mid X) = \\exp(\\mu + \\tfrac{1}{2}\\sigma^2),\n\\]\ne não apenas \\(\\exp(\\mu)\\) (Casella e Berger (2002)). Ignorar esse termo pode introduzir viés de retransformação.\nQuando usar\n\nResíduos vs. ajustados exibem padrão de funil.\nHistograma dos resíduos apresenta assimetria à direita.\nQQ-plot mostra caudas pesadas superiores.\nRelação parece exponencial na escala original.\n\nGráficos sugeridos para verificação:\n\nResíduos vs. \\(\\hat Y\\) (o que verificar: formato de funil; resultado esperado após transformação: dispersão homogênea). \nQQ-plot dos resíduos (o que verificar: caudas pesadas; resultado esperado após transformação: alinhamento mais próximo à reta). \nHistograma dos resíduos (o que verificar: assimetria; aresultado esperado após transformação: formato mais próximo da normal).\n\n\n\n\n9.2.3 Raiz quadrada (\\(Y^* = \\sqrt{Y}\\))\nA transformação pela raiz quadrada é frequentemente utilizada quando \\(Y\\) representa contagens ou frequências.\nPor exemplo, se \\(Y_i \\sim \\text{Poisson}(\\mu_i)\\), então\n\\[\nE(Y_i) = \\mu_i,\n\\quad\nVar(Y_i) = \\mu_i,\n\\]\nou seja, a variância é proporcional à média.\nAplicando a transformação\n\\[\nY_i^* = \\sqrt{Y_i},\n\\]\ne utilizando aproximação por expansão de Taylor, obtém-se\n\\[\nVar(Y_i^*) \\approx \\frac{1}{4},\n\\]\nque é aproximadamente constante e independe de \\(\\mu_i\\) (ver Apêndice de Demonstrações {#demo}).\nNote que a raiz quadrada reduz a assimetria típica de distribuições de contagem e estabiliza a variância quando esta é proporcional à média.\nO modelo passa então a explicar a variação na raiz da resposta, não na resposta original. Isso altera a escala e deve ser explicitado ao interpretar coeficientes.\nQuando usar\n\nDados de contagem.\nVariância aproximadamente proporcional à média.\nHistograma com cauda longa à direita.\n\nGráficos sugeridos para verificação:\n\nHistograma dos resíduos (o que verificar: cauda longa; resultado esperado após transformação: simetria). \nResíduos vs. \\(\\hat Y\\) (o que verificar: variância crescente; resultado esperado após transformação: dispersão estável).\n\n\n\n9.2.4 Inverso (\\(Y^* = 1/Y\\))\nA transformação inversa é útil quando a relação entre \\(Y\\) e \\(X\\) apresenta comportamento do tipo decaimento rápido em direção a um limite.\nSe a relação original for não linear, por exemplo,\n\\[\nY = \\frac{1}{\\alpha + \\beta X},\n\\]\nentão\n\\[\n\\frac{1}{Y} = \\alpha + \\beta X,\n\\]\nque é linear.\nEssa transformação é comum em fenômenos físicos e biológicos nos quais a resposta diminui rapidamente e depois se estabiliza.\nO modelo ajustado explica o comportamento do inverso da resposta. A interpretação substantiva deve ser feita com cuidado, pois aumentos em \\(X\\) passam a produzir efeitos lineares sobre \\(1/Y\\), e não diretamente sobre \\(Y\\).\nQuando usar\nModelos onde a resposta diminui de forma não linear (ex.: tempo de reação decrescendo com aumento de dose).\nGráficos sugeridos para verificação:\n\nResíduos vs. \\(X\\) (o que verificar: padrão curvilíneo; resultado esperado após transformação: dispersão aleatória). \nQQ-plot dos resíduos (o que verificar: forte desvio; resultado esperado após transformação: mais alinhado).\n\n\n\n9.2.5 Quadrado (\\(Y^* = Y^2\\))\nA transformação quadrática pode ser útil quando a variabilidade é maior em valores pequenos de \\(Y\\) ou quando a relação é convexa na escala original.\nAo elevar \\(Y\\) ao quadrado, ampliamos diferenças em níveis mais altos da resposta, o que pode reduzir padrões estruturais nos resíduos.\nEssa transformação modifica significativamente a escala e deve ser utilizada apenas quando há justificativa empírica clara nos gráficos de resíduos.\nGráficos sugeridos para verificação:\n\nResíduos vs. \\(\\hat Y\\) (o que verificar: padrão em arco; resultado esperado após transformação: dispersão homogênea). \nHistograma dos resíduos (o que verificar: concentração em torno de 0; resultado esperado após transformação: mais espalhado).",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transformações nas Variáveis</span>"
    ]
  },
  {
    "objectID": "mrls_transf.html#transformações-na-variável-explicativa-x",
    "href": "mrls_transf.html#transformações-na-variável-explicativa-x",
    "title": "9  Transformações nas Variáveis",
    "section": "9.3 Transformações na variável explicativa (\\(X\\))",
    "text": "9.3 Transformações na variável explicativa (\\(X\\))\nAs transformações na variável explicativa têm como objetivo principal linearizar a relação entre \\(X\\) e a média condicional de \\(Y\\), preservando, quando possível, a estrutura de variância constante dos erros.\nRecordando que o MRLS assume \\(E(Y \\mid X) = \\beta_0 + \\beta_1 X\\), portanto, qualquer evidência de que a relação média entre \\(Y\\) e \\(X\\) não é linear, por exemplo, presença de curvatura sistemática no gráfico de resíduos vs. \\(X\\), sugere que a especificação funcional pode estar inadequada (Montgomery, Peck, e Vining (2021); Kutner et al. (2005)).\nNesse caso, buscamos uma transformação \\(X^* = h(X)\\) tal que\n\\[\nE(Y \\mid X^*) = \\beta_0 + \\beta_1 X^*\n\\]\nrepresente melhor a estrutura média do fenômeno.\nTransformar \\(X\\) significa alterar a forma funcional da regressão em relação à variável \\(X\\), mas não a natureza probabilística da variável resposta. Diferentemente das transformações em \\(Y\\), aqui a distribuição condicional de \\(Y\\) não é redefinida; apenas modificamos a forma como a média depende da variável explicativa.\nPortanto:\n\nTransformações em \\(Y\\) → alteram a escala da resposta e a estrutura probabilística.\n\nTransformações em \\(X\\) → alteram a forma funcional da média em relação à variável \\(X\\).\n\n\n9.3.1 Motivação estatística\nSe o gráfico de resíduos vs. \\(X\\) exibe:\n\npadrão em arco (concavidade ou convexidade),\nforma em S,\ntendência sistemática crescente ou decrescente,\n\nentão o modelo linear \\(\\beta_0 + \\beta_1 X\\) não está capturando adequadamente a relação entre as variáveis. A solução é buscar uma transformação \\(h(X)\\) tal que a nova relação seja aproximadamente linear.\nEssa estratégia é coerente com o princípio geral de modelagem estatística: especificar corretamente a função média antes de avaliar a variância dos erros (Weisberg (2005)).\n\n\n9.3.2 Logaritmo (\\(X^* = \\log X\\))\nA transformação logarítmica em \\(X\\) é apropriada quando a relação entre \\(Y\\) e \\(X\\) apresenta comportamento de crescimento proporcional ou lei de potência.\nSuponha que a relação verdadeira seja\n\\[\nY = \\alpha X^\\beta + \\varepsilon.\n\\]\nTomando logaritmo em ambos os lados (desconsiderando momentaneamente o erro),\n\\[\n\\log Y = \\log \\alpha + \\beta \\log X.\n\\]\nSe também transformarmos \\(Y\\), obtemos o chamado modelo log-log, amplamente utilizado em econometria.\nNo entanto, mesmo sem transformar \\(Y\\), pode ser apropriado modelar\n\\[\nY = \\beta_0 + \\beta_1 \\log X + \\varepsilon,\n\\]\nquando o efeito de \\(X\\) diminui à medida que \\(X\\) cresce.\nObserve que no modelo\n\\[\nY = \\beta_0 + \\beta_1 \\log X,\n\\]\no coeficiente \\(\\beta_1\\) representa a variação média em \\(Y\\) associada a uma variação proporcional em \\(X\\). Mais especificamente, um aumento percentual em \\(X\\) está associado a uma variação aproximadamente linear em \\(Y\\).\nPortanto, a transformação logarítmica em \\(X\\) não implica mudança na distribuição de \\(Y\\).\nGráfico sugerido:\n\nResíduos vs. \\(X\\) (o que verificar: curva sistemática; resultado esperado após transformação: dispersão aleatória).\n\n\n\n9.3.3 Inverso (\\(X^* = 1/X\\))\nA transformação inversa é adequada quando o efeito de \\(X\\) é muito forte para valores pequenos e diminui rapidamente conforme \\(X\\) aumenta.\nExemplo típico:\n\nfenômenos de aprendizado;\nprocessos de saturação;\nrespostas que se estabilizam para grandes valores de \\(X\\).\n\nSe a relação verdadeira for aproximadamente\n\\[\nY = \\alpha + \\frac{\\beta}{X},\n\\]\nentão\n\\[\nY = \\alpha + \\beta X^*\n\\]\ncom \\(X^* = 1/X\\).\nA transformação inversa captura comportamentos de retorno decrescente ou aproximação assintótica.\nGráfico sugerido:\n\nResíduos vs. \\(X\\) (o que verificar: curva decrescente; resultado esperado após transformação: sem padrão).\n\n\n\n9.3.4 Potências e termos polinomiais (\\(X^2\\), \\(X^3\\), …)\nQuando a relação apresenta curvatura suave, uma alternativa é expandir o modelo com termos polinomiais:\n\\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\cdots + \\varepsilon.\n\\]\nEsse modelo continua sendo linear nos parâmetros (condição essencial do MRLS), embora não seja linear em \\(X\\).\nTemos então que:\n\n\\(\\beta_2 &gt; 0\\) → concavidade para cima (convexidade).\n\\(\\beta_2 &lt; 0\\) → concavidade para baixo.\n\nApesar de introduzir não linearidade em \\(X\\), o modelo permanece linear em relação aos parâmetros \\(\\beta_j\\). Isso garante que:\n\nos estimadores de MQO continuam válidos;\nas propriedades inferenciais permanecem as mesmas.\n\nEssa distinção entre linearidade nos parâmetros e linearidade na variável explicativa é conceitualmente central em regressão linear.\nGráfico sugerido:\n\nResíduos vs. \\(X\\) (o que verificar: arco ou S-curva; resultado esperado após transformação: resíduos aleatórios).\n\n\n\n9.3.5 Transformar \\(X\\) ou adicionar polinômios?\nHá duas estratégias principais para lidar com curvaturas:\n\nTransformar \\(X\\) (por exemplo, \\(\\log X\\), \\(1/X\\)).\nIncluir termos polinomiais (\\(X^2\\), \\(X^3\\)).\n\nA escolha depende de:\n\ncoerência teórica;\ninterpretação desejada;\nestabilidade numérica do ajuste.\n\nTransformações simples costumam ter interpretação mais direta; polinômios oferecem maior flexibilidade, mas podem introduzir instabilidade em extrapolações.\nLogo, as transformações na variável explicativa:\n\nbuscam linearizar a função média;\nnão alteram a estrutura probabilística da resposta;\npreservam a estrutura básica do MRLS;\nexigem reavaliação completa dos resíduos após o ajuste.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transformações nas Variáveis</span>"
    ]
  },
  {
    "objectID": "mrls_transf.html#guia-prático-escolhendo-a-transformação",
    "href": "mrls_transf.html#guia-prático-escolhendo-a-transformação",
    "title": "9  Transformações nas Variáveis",
    "section": "9.4 Guia prático: escolhendo a transformação",
    "text": "9.4 Guia prático: escolhendo a transformação\nApós compreender as motivações teóricas para transformar \\(Y\\) ou \\(X\\), surge a questão prática: como decidir qual transformação aplicar?\nA escolha não deve ser arbitrária nem baseada apenas em melhora numérica de \\(R^2\\). O critério central é a adequação às hipóteses do modelo e à estrutura substantiva do fenômeno (Montgomery, Peck, e Vining (2021); Kutner et al. (2005)).\nEtapa 1 — Diagnóstico inicial\nAntes de qualquer transformação, ajusta-se o modelo na escala original:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + e_i.\n\\]\nEm seguida, analisam-se os resíduos:\n\nResíduos vs. ajustados → verificar homoscedasticidade.\nResíduos vs. \\(X\\) → verificar linearidade.\nQQ-plot → verificar normalidade.\nHistograma → avaliar assimetria e caudas.\n\nA transformação deve responder a um padrão empírico específico observado nos resíduos. Se não há padrão sistemático, não há justificativa para transformar.\nEtapa 2 — Identificação do padrão\nAlguns padrões típicos e suas possíveis soluções:\n\n\n\n\n\n\n\n\nPadrão observado\nPossível causa\nTransformação sugerida\n\n\n\n\nFunil (variância cresce com média)\n\\(Var(Y \\mid X)\\) proporcional à média ou ao quadrado da média\n\\(\\log(Y)\\) ou \\(\\sqrt{Y}\\)\n\n\nCurvatura em arco\nRelação não linear entre \\(Y\\) e \\(X\\)\n\\(X^2\\) ou \\(\\log(X)\\)\n\n\nAssimetria à direita\nDistribuição assimétrica positiva\n\\(\\log(Y)\\)\n\n\nEfeito forte em valores pequenos de \\(X\\)\nRetorno decrescente\n\\(1/X\\)\n\n\n\nCada transformação responde a um mecanismo estrutural distinto. Não se trata de “corrigir o gráfico”, mas de modelar melhor o processo subjacente.\nEtapa 3 — Ajuste do modelo transformado\nApós escolher a transformação, ajusta-se o novo modelo, por exemplo:\n\\[\n\\log(Y_i) = \\beta_0^* + \\beta_1^* X_i + \\varepsilon_i,\n\\]\nou\n\\[\nY_i = \\beta_0 + \\beta_1 \\log(X_i) + \\varepsilon_i.\n\\]\nNesse momento, todo o ciclo de verificação deve ser repetido:\n\nanálise gráfica dos resíduos;\ntestes formais (quando apropriado);\navaliação da coerência interpretativa.\n\nEtapa 4 — Avaliação comparativa\nA comparação entre modelos deve considerar:\n\nQualidade dos resíduos (homoscedasticidade, normalidade).\nEstabilidade dos coeficientes.\nInterpretação substantiva.\nIntervalos de confiança e precisão inferencial.\n\nUm modelo com menor \\(R^2\\) pode ser preferível se satisfaz melhor as hipóteses do MRLS e apresenta resíduos estruturalmente adequados.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transformações nas Variáveis</span>"
    ]
  },
  {
    "objectID": "mrls_transf.html#exemplo-ilustrativo-antes-e-depois-da-transformação",
    "href": "mrls_transf.html#exemplo-ilustrativo-antes-e-depois-da-transformação",
    "title": "9  Transformações nas Variáveis",
    "section": "9.5 Exemplo ilustrativo: antes e depois da transformação",
    "text": "9.5 Exemplo ilustrativo: antes e depois da transformação\nPara consolidar as ideias discutidas até aqui, apresentamos um exemplo ilustrativo que mostra, de forma integrada, a necessidade estatística e a eficácia inferencial de uma transformação na variável resposta.\n\n9.5.1 Cenário e especificação dos modelos\nSuponha que estejamos interessados em modelar a relação entre horas de prática de estudo (\\(X\\)) e a nota obtida em uma prova (\\(Y\\)). Intuitivamente, espera-se que estudantes que dedicam mais horas de estudo tendam a alcançar notas maiores. Contudo, é plausível que:\n\no ganho médio de desempenho seja aproximadamente exponencial;\na variabilidade das notas aumente à medida que \\(X\\) aumenta.\n\nEssa estrutura implica que a variância condicional pode depender do nível médio da resposta, violando a hipótese de homoscedasticidade do MRLS.\nConsideremos dois modelos concorrentes.\nModelo A (sem transformação):\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + e_i.\n\\]\nEsse modelo assume:\n\\[\nE(Y_i \\mid X_i) = \\beta_0 + \\beta_1 X_i,\n\\qquad\nVar(Y_i \\mid X_i) = \\sigma^2.\n\\]\nModelo B (com transformação logarítmica):\n\\[\nY_i^* = \\log(Y_i) = \\beta_0^* + \\beta_1^* X_i + \\varepsilon_i,\n\\qquad\n\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2).\n\\]\nNesse caso, assumimos normalidade e homoscedasticidade na escala logarítmica. Implicitamente, isso significa que, na escala original,\n\\[\nY_i \\mid X_i \\sim \\text{LogNormal}(\\mu_i, \\sigma^2),\n\\]\no que altera a estrutura probabilística do modelo.\n\n\n9.5.2 Geração do conjunto de dados e análise descritiva\nPara ilustrar essa situação, considere que a relação verdadeira é linear na escala logarítmica:\n\\[\n\\log(Y_i) = 1.0 + 0.20 X_i + u_i,\n\\quad\nu_i \\sim N(0,\\sigma^2).\n\\]\nA partir dessa equação, obtemos\n\\[\nY_i = \\exp(1.0 + 0.20 X_i + u_i),\n\\]\no que gera:\n\ncrescimento exponencial na média;\nvariância crescente com \\(X\\);\nassimetria à direita na escala original.\n\n\nlibrary(dplyr)\nset.seed(2025)\n\nn &lt;- 120\nX &lt;- seq(0, 10, length.out = n)\n\n# Relação linear na escala log(Y)\nsigma &lt;- 0.6\nlogY  &lt;- 1.0 + 0.20*X + rnorm(n, 0, sigma)\nY     &lt;- exp(logY)\n\ndf &lt;- tibble(\n  X = X,\n  Y = Y,\n  logY = log(Y)\n)\n\nEstatísticas descritivas\nDevem ser apresentadas:\n\nprimeiras linhas da base;\nestatísticas resumidas para \\(X\\), \\(Y\\) e \\(\\log(Y)\\).\n\n\nhead(df)\n\n# A tibble: 6 × 3\n       X     Y  logY\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0       3.95 1.37 \n2 0.0840  2.82 1.04 \n3 0.168   4.47 1.50 \n4 0.252   6.13 1.81 \n5 0.336   3.63 1.29 \n6 0.420   2.68 0.986\n\nsummary(df)\n\n       X              Y               logY       \n Min.   : 0.0   Min.   : 1.141   Min.   :0.1318  \n 1st Qu.: 2.5   1st Qu.: 4.384   1st Qu.:1.4779  \n Median : 5.0   Median : 6.833   Median :1.9217  \n Mean   : 5.0   Mean   : 9.724   Mean   :1.9833  \n 3rd Qu.: 7.5   3rd Qu.:12.803   3rd Qu.:2.5497  \n Max.   :10.0   Max.   :40.396   Max.   :3.6987  \n\n\nAjuste dos modelos\nAjustamos ambos os modelos via MQO:\n\nModelo A: \\(Y \\sim X\\)\n\nModelo B: \\(\\log(Y) \\sim X\\)\n\nComparar:\n\nestimativas dos coeficientes;\nerros-padrão;\n\\(R^2\\);\nestatísticas \\(t\\) e \\(F\\).\n\nNote que os \\(R^2\\) não são diretamente comparáveis entre escalas distintas, pois medem proporções de variabilidade em espaços diferentes.\n\nmA &lt;- lm(Y ~ X, data = df)\nmB &lt;- lm(logY ~ X, data = df)\n\ncat(\"=== MODELO A (Y ~ X) ===\\n\")\n\n=== MODELO A (Y ~ X) ===\n\nprint(summary(mA))\n\n\nCall:\nlm(formula = Y ~ X, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8050  -4.1989  -0.8159   1.8826  24.5691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.0801     1.2119   1.716   0.0887 .  \nX             1.5288     0.2095   7.298 3.68e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.68 on 118 degrees of freedom\nMultiple R-squared:  0.311, Adjusted R-squared:  0.3052 \nF-statistic: 53.27 on 1 and 118 DF,  p-value: 3.677e-11\n\ncat(\"\\n=== MODELO B (logY ~ X) ===\\n\")\n\n\n=== MODELO B (logY ~ X) ===\n\nprint(summary(mB))\n\n\nCall:\nlm(formula = logY ~ X, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.43792 -0.41121 -0.01437  0.36590  1.62093 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.15379    0.10872  10.612  &lt; 2e-16 ***\nX            0.16590    0.01879   8.828 1.16e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5992 on 118 degrees of freedom\nMultiple R-squared:  0.3978,    Adjusted R-squared:  0.3927 \nF-statistic: 77.94 on 1 and 118 DF,  p-value: 1.164e-14\n\n\nANOVA\nApresentar as tabelas ANOVA para ambos os modelos.\n\ncat(\"\\n=== ANOVA — MODELO A ===\\n\")\n\n\n=== ANOVA — MODELO A ===\n\nprint(anova(mA))\n\nAnalysis of Variance Table\n\nResponse: Y\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nX           1 2376.6 2376.59  53.266 3.677e-11 ***\nResiduals 118 5264.9   44.62                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncat(\"\\n=== ANOVA — MODELO B ===\\n\")\n\n\n=== ANOVA — MODELO B ===\n\nprint(anova(mB))\n\nAnalysis of Variance Table\n\nResponse: logY\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nX           1 27.984 27.9837  77.938 1.164e-14 ***\nResiduals 118 42.368  0.3591                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n9.5.3 Diagnóstico gráfico: antes e depois\nO contraste entre os modelos deve ser avaliado por meio dos gráficos diagnósticos.\n\n# Padronização visual para TODOS os gráficos do exemplo\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nbase_size &lt;- 12\n\ntheme_diag &lt;- theme_minimal(base_size = base_size) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    strip.text = element_text(face = \"bold\"),\n    strip.background = element_rect(fill = \"grey95\", color = NA),\n    plot.title.position = \"plot\",\n    plot.caption.position = \"plot\"\n  )\n\n\n9.5.3.1 Dispersão da resposta vs. explicativa\n\nModelo A: espera-se tendência crescente com dispersão aumentando (funil).\nModelo B: dispersão aproximadamente constante e relação linear clara na escala transformada.\n\n\ndf_plot &lt;- df %&gt;%\n  select(X, Y, logY) %&gt;%\n  pivot_longer(cols = c(Y, logY), names_to = \"resp\", values_to = \"valor\") %&gt;%\n  mutate(\n    resp = ifelse(resp == \"Y\",\n                  \"Modelo A: escala original (Y)\",\n                  \"Modelo B: escala log (log(Y))\")\n  )\n\nggplot(df_plot, aes(x = X, y = valor)) +\n  geom_point(alpha = 0.8, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.9) +\n  facet_wrap(~resp, ncol = 2, scales = \"free_y\") +\n  labs(\n    x = \"Horas de estudo (X)\",\n    y = NULL\n  ) +\n  theme_diag\n\n\n\n\n\n\n\n\n\n\n9.5.3.2 Resíduos vs. valores ajustados\n\nModelo A: padrão de funil e possível curvatura.\nModelo B: resíduos aleatórios em torno de zero.\n\nA ausência de estrutura no segundo caso indica que\n\\[\nVar(\\varepsilon_i \\mid X_i) \\approx \\sigma^2.\n\\]\n\nresA &lt;- resid(mA); fitA &lt;- fitted(mA)\nresB &lt;- resid(mB); fitB &lt;- fitted(mB)\n\ndados &lt;- bind_rows(\n  tibble(fit = fitA, res = resA, painel = \"Modelo A: escala original (Y)\"),\n  tibble(fit = fitB, res = resB, painel = \"Modelo B: escala log (log(Y))\")\n)\n\nggplot(dados, aes(x = fit, y = res)) +\n  geom_point(alpha = 0.85, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", linewidth = 0.6) +\n  facet_wrap(~painel, ncol = 2, scales = \"free_x\") +\n  labs(\n    x = \"Valores ajustados\",\n    y = \"Resíduo\"\n  ) +\n  theme_diag\n\n\n\n\n\n\n\n\n\n\n9.5.3.3 Resíduos estudentizados vs. ajustados\n\nModelo A: maior número de pontos fora do intervalo \\([-2,2]\\).\nModelo B: poucos pontos extremos, ausência de padrão.\n\n\nstudA &lt;- rstudent(mA)\nstudB &lt;- rstudent(mB)\n\ndados &lt;- bind_rows(\n  tibble(fit = fitA, stud = studA, painel = \"Modelo A: escala original (Y)\"),\n  tibble(fit = fitB, stud = studB, painel = \"Modelo B: escala log (log(Y))\")\n)\n\nggplot(dados, aes(x = fit, y = stud)) +\n  geom_point(alpha = 0.85, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", linewidth = 0.6) +\n  geom_hline(yintercept = c(-2, 2), linetype = \"dotted\", linewidth = 0.7) +\n  facet_wrap(~painel, ncol = 2, scales = \"free_x\") +\n  labs(\n    x = \"Valores ajustados\",\n    y = \"Resíduo estudentizado (t*)\"\n  ) +\n  theme_diag\n\n\n\n\n\n\n\n\n\n\n9.5.3.4 QQ-plot\n\nModelo A: desvios sistemáticos nas caudas.\nModelo B: alinhamento próximo à reta de 45°.\n\nIsso sugere que\n\\[\n\\varepsilon_i \\sim N(0,\\sigma^2)\n\\]\né plausível apenas na escala logarítmica.\n\nqq_df &lt;- function(r, painel){\n  q &lt;- qqnorm(r, plot.it = FALSE)\n  tibble(theo = q$x, samp = q$y, painel = painel)\n}\n\ndados &lt;- bind_rows(\n  qq_df(resA, \"Modelo A: QQ-plot dos resíduos (Y)\"),\n  qq_df(resB, \"Modelo B: QQ-plot dos resíduos (log(Y))\")\n)\n\nggplot(dados, aes(x = theo, y = samp)) +\n  geom_point(alpha = 0.85, size = 2) +\n  geom_abline(intercept = 0, slope = 1, linewidth = 0.7) +\n  facet_wrap(~painel, ncol = 2, scales = \"free\") +\n  labs(\n    x = \"Quantis teóricos (Normal)\",\n    y = \"Quantis amostrais (resíduos)\"\n  ) +\n  theme_diag\n\n\n\n\n\n\n\n\n\n\n9.5.3.5 Histograma\n\nModelo A: assimetria à direita e caudas pesadas.\nModelo B: simetria aproximadamente normal.\n\n\ndados &lt;- bind_rows(\n  tibble(r = resA, painel = \"Modelo A: resíduos (Y)\"),\n  tibble(r = resB, painel = \"Modelo B: resíduos (log(Y))\")\n)\n\nggplot(dados, aes(x = r)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 bins = 14,\n                 alpha = 0.7,\n                 color = \"white\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", linewidth = 0.6) +\n  facet_wrap(~painel, ncol = 2, scales = \"free\") +\n  labs(\n    x = \"Resíduo\",\n    y = \"Densidade\"\n  ) +\n  theme_diag\n\n\n\n\n\n\n\n\n\n\n9.5.3.6 Resíduos vs. \\(X\\)\n\nModelo A: indícios de curvatura e variância crescente.\nModelo B: dispersão uniforme.\n\n\ndados &lt;- bind_rows(\n  tibble(X = df$X, res = resA, painel = \"Modelo A: resíduos vs X (Y)\"),\n  tibble(X = df$X, res = resB, painel = \"Modelo B: resíduos vs X (log(Y))\")\n)\n\nggplot(dados, aes(x = X, y = res)) +\n  geom_point(alpha = 0.85, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", linewidth = 0.6) +\n  facet_wrap(~painel, ncol = 2, scales = \"free_y\") +\n  labs(\n    x = \"Horas de estudo (X)\",\n    y = \"Resíduo\"\n  ) +\n  theme_diag\n\n\n\n\n\n\n\n\n\n\n\n9.5.4 Comparação numérica e conclusão\nModelo A (escala original)\n\n\\(R^2\\) aparentemente satisfatório.\nViolação da homoscedasticidade.\nDesvios de normalidade.\nInferência potencialmente comprometida.\n\nModelo B (escala logarítmica)\n\nMelhor adequação estrutural dos resíduos.\nTestes \\(t\\) e \\(F\\) mais confiáveis.\nIntervalos de confiança coerentes com as hipóteses do modelo.\n\nUm modelo pode apresentar bom \\(R^2\\) e, ainda assim, violar hipóteses fundamentais. A validade da inferência depende da adequação dos resíduos, não apenas do poder explicativo.\n\n9.5.4.1 Interpretação\nNo Modelo B,\n\\[\n\\log(Y) = \\beta_0^* + \\beta_1^* X.\n\\]\nLogo,\n\\[\nY = \\exp(\\beta_0^*) \\exp(\\beta_1^* X).\n\\]\nO efeito de \\(X\\) é multiplicativo:\n\\[\n\\exp(\\beta_1^*)\n\\]\nrepresenta o fator pelo qual \\(Y\\) é multiplicado para cada unidade adicional em \\(X\\).\nEntretanto, para previsões na escala original,\n\\[\nE(Y \\mid X) = \\exp\\left(\\mu + \\tfrac{1}{2}\\sigma^2\\right),\n\\]\ne não simplesmente \\(\\exp(\\mu)\\) (Casella e Berger (2002)). Ignorar esse termo introduz viés de retransformação\nEste exemplo evidencia que:\n\na análise de resíduos é central na validação do modelo;\ntransformações podem corrigir violações estruturais;\na interpretação deve sempre respeitar a escala do modelo ajustado;\na escolha da transformação deve ser guiada por diagnóstico e fundamentação teórica.\n\nEm regressão, a forma funcional adequada é aquela que torna os resíduos compatíveis com as hipóteses do modelo — não necessariamente aquela que produz o maior \\(R^2\\).\n\n\n\n\nCasella, George, e Roger L. Berger. 2002. Statistical Inference. 2º ed. Pacific Grove: Duxbury.\n\n\nKutner, Michael H., Christopher J. Nachtsheim, John Neter, e William Li. 2005. Applied Linear Statistical Models. 5º ed. New York: McGraw-Hill.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, e G. Geoffrey Vining. 2021. Introduction to Linear Regression Analysis. 6º ed. Hoboken: John Wiley & Sons.\n\n\nWeisberg, Sanford. 2005. Applied Linear Regression. New York: Wiley.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Transformações nas Variáveis</span>"
    ]
  },
  {
    "objectID": "mrls_compara.html",
    "href": "mrls_compara.html",
    "title": "10  Comparação de Modelos",
    "section": "",
    "text": "10.1 Motivação e princípios de comparação\nApós discutir a análise de resíduos, surge uma questão natural: como escolher entre diferentes modelos candidatos? Nem sempre um único modelo é suficiente; muitas vezes ajustamos várias alternativas e precisamos compará-las.\nA comparação de modelos, porém, não é um “campeonato de métricas”. Ela deve ser entendida como uma decisão estatística guiada por três ideias centrais: adequação, parcimônia e finalidade (explicar vs prever). Em particular, comparar modelos significa avaliar trocas (trade-offs): um modelo pode ajustar melhor os dados, mas à custa de maior complexidade e menor estabilidade, especialmente em amostras moderadas/pequenas.\nA comparação de modelos envolve duas dimensões principais:\nEm regressão, “modelo melhor” não significa “modelo com o maior ajuste numérico”. O ponto é: um modelo é um compromisso entre (i) representar a estrutura sistemática de \\(Y\\) explicada por \\(X\\) e (ii) não incorporar estrutura espúria (ruído) como se fosse sinal. Critérios como AIC e BIC foram propostos justamente para explicitar essa troca: melhorar o ajuste (via log-verossimilhança) custa complexidade (número de parâmetros), e essa penalização é uma maneira de desencorajar sobreajuste (Akaike (1974); Schwarz (1978); Burnham e Anderson (2002)).\nUm ponto central é que não adianta comparar dois modelos se ambos têm resíduos inadequados. O primeiro filtro deve ser sempre o diagnóstico dos resíduos. Uma vez que os modelos estejam pelo menos aproximadamente bem especificados, podemos partir para critérios comparativos.\nEssa ordem (“diagnóstico \\(\\rightarrow\\) comparação”) é importante porque muitos critérios formais assumem que o modelo está minimamente compatível com as hipóteses estruturais do MRLS (por exemplo: relação média aproximadamente linear na escala adotada, variância aproximadamente constante e ausência de padrões sistemáticos evidentes nos resíduos). Se essas condições falham, é comum observar situações enganosas como:",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparação de Modelos</span>"
    ]
  },
  {
    "objectID": "mrls_compara.html#motivação-e-princípios-de-comparação",
    "href": "mrls_compara.html#motivação-e-princípios-de-comparação",
    "title": "10  Comparação de Modelos",
    "section": "",
    "text": "Qualidade estatística do ajuste – medida por critérios formais (ex.: \\(R^2\\), teste \\(F\\), AIC, BIC).\n\nPertinência substantiva – se o modelo faz sentido em relação ao fenômeno estudado, é parcimonioso e interpretável.\n\n\n\n\n\num modelo com \\(R^2\\) alto, mas com padrão em funil (heteroscedasticidade), o que compromete inferência usual e previsões com incerteza mal calibrada;\num modelo com ligeira melhora em AIC/BIC após adicionar termos, mas com curvatura residual persistente, sinalizando que a forma funcional ainda está incorreta;\num modelo “mais complexo” que parece melhor no ajuste dentro da amostra, mas piora a capacidade de generalização (sobreajuste).\n\n\n10.1.1 Finalidade da comparação: explicar ou prever\nAntes de escolher um critério, é útil explicitar o objetivo:\n\nExplicação/interpretação: prioriza parâmetros estáveis e interpretáveis e tende a valorizar parcimônia (frequentemente BIC é usado como referência em seleção mais “conservadora”).\n\nPredição: prioriza desempenho fora da amostra; critérios baseados em verossimilhança com penalização moderada (como AIC) são frequentemente usados como aproximações de desempenho preditivo, mas idealmente devem ser complementados por validação (ex.: validação cruzada) quando isso fizer parte do desenho analítico (Burnham e Anderson (2002)).\n\nNesta seção, organizamos os principais critérios formais e práticos de comparação, seguidos de exemplos ilustrativos.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparação de Modelos</span>"
    ]
  },
  {
    "objectID": "mrls_compara.html#critérios-clássicos-de-comparação",
    "href": "mrls_compara.html#critérios-clássicos-de-comparação",
    "title": "10  Comparação de Modelos",
    "section": "10.2 Critérios clássicos de comparação",
    "text": "10.2 Critérios clássicos de comparação\n\n10.2.1 Coeficiente de determinação \\(R^2\\)\nO \\(R^2\\) mede a proporção da variabilidade de \\(Y\\) explicada pelo modelo:\n\\[\nR^2 = 1 - \\frac{SQ_{Res}}{SQ_{Tot}},\n\\]\nem que \\(SQ_{Res} = \\sum_{i=1}^n (Y_i-\\hat Y_i)^2\\) é a soma de quadrados dos resíduos e \\(SQ_{Tot} = \\sum_{i=1}^n (Y_i-\\bar Y)^2\\) é a soma de quadrados total.\nO \\(R^2\\) responde à pergunta: “quanto da variabilidade total observada em \\(Y\\) foi capturada pelo componente sistemático do modelo?”. Ele é, portanto, uma medida descritiva de ajuste: compara o erro do modelo com o erro de um “modelo nulo” que sempre prediz \\(\\bar Y\\).\nComo a decomposição \\[\nSQ_{Tot}=SQ_{Reg}+SQ_{Res}\n\\] vale no MRLS com intercepto, também podemos escrever \\[\nR^2=\\frac{SQ_{Reg}}{SQ_{Tot}},\n\\] isto é, a fração da variabilidade total explicada pela regressão.\n\n10.2.1.1 Propriedades e limitações\nÉ tentador ler \\(R^2\\) como “o modelo é bom/ruim”, mas isso é perigoso por três razões:\n\n\\(R^2\\) não diagnostica adequação das hipóteses: um modelo pode ter \\(R^2\\) alto e ainda assim apresentar resíduos com curvatura, heteroscedasticidade ou dependência. Por isso, o diagnóstico de resíduos vem antes.\n\\(R^2\\) não mede capacidade preditiva fora da amostra: ele é calculado na amostra usada no ajuste.\n\\(R^2\\) depende da variabilidade de \\(Y\\): em bases com pouca variação em \\(Y\\), mesmo modelos úteis podem ter \\(R^2\\) modestos; e em bases com grande variação em \\(Y\\), \\(R^2\\) pode ser alto sem que o modelo seja substantivamente satisfatório.\n\nVantagens e limitaçoes \n\nVantagem: fornece uma medida intuitiva de ajuste.\n\nLimitação 1 (monotonicidade): em modelos de MQO com intercepto, \\(R^2\\) nunca diminui quando adicionamos regressores ao conjunto de candidatos. Isso ocorre porque, ao adicionar parâmetros, o MQO minimiza \\(SQ_{Res}\\) em um espaço maior, logo \\(SQ_{Res}\\) só pode diminuir (ou permanecer igual).\n\nLimitação 2 (comparabilidade): \\(R^2\\) não é comparável entre modelos com e sem intercepto, pois a decomposição de somas de quadrados muda. Em particular, quando o intercepto é omitido, o “modelo nulo” implícito deixa de ser \\(Y=\\bar Y\\), e interpretações usuais de \\(R^2\\) podem se tornar enganosas.\nLimitação 3 (escala da resposta): se você transforma \\(Y\\) (por exemplo, \\(\\log(Y)\\)), o \\(R^2\\) passa a descrever ajuste na escala transformada, não na escala original.\n\n\nObservação: no MRLS (com intercepto), vale a relação \\(R^2=r_{XY}^2\\), conectando a medida de ajuste com a intensidade de associação linear entre \\(X\\) e \\(Y\\).\n\n\n\n\n10.2.2 Coeficiente de determinação ajustado \\(R^2_{aj}\\)\nPara penalizar modelos excessivamente complexos, define-se:\n\\[\nR^2_{aj} = 1 - \\frac{SQ_{Res}/(n-p)}{SQ_{Tot}/(n-1)},\n\\]\nem que \\(n\\) é o tamanho da amostra e \\(p\\) é o número de parâmetros (incluindo o intercepto).\nO \\(R^2_{aj}\\) substitui “proporção de variância explicada” por uma comparação entre variâncias estimadas:\n\n\\(SQ_{Res}/(n-p)\\) é o estimador de \\(\\sigma^2\\) (variância do erro) sob o modelo candidato;\n\\(SQ_{Tot}/(n-1)\\) é a variância amostral de \\(Y\\).\n\nAssim, \\(R^2_{aj}\\) pergunta: “o quanto a variância residual estimada caiu em relação à variância total de \\(Y\\), levando em conta quantos parâmetros eu usei para isso?”.\nAo contrário do \\(R^2\\), o \\(R^2_{aj}\\) pode diminuir quando adicionamos regressoras. Isso é desejável: se um novo termo reduz pouco o \\(SQ_{Res}\\), a penalização por perder graus de liberdade pode dominar, sinalizando que o ganho de ajuste não compensa a complexidade.\n\n10.2.2.1 Vantagens e limitações\n\nVantagem: permite comparar modelos com diferentes números de parâmetros, desde que estejam na mesma escala da resposta e com intercepto.\n\nLimitação: em amostras pequenas, ainda pode favorecer modelos com leve sobreajuste, pois sua penalização é relativamente moderada.\n\nUso prático: quando a comparação envolve modelos com diferentes estruturas, \\(R^2_{aj}\\) é preferível ao \\(R^2\\) simples.\n\n\n\n\n10.2.3 Teste \\(F\\) para modelos aninhados\nQuando um modelo é caso particular de outro (modelo restrito vs. modelo completo), podemos usar:\n\\[\nF = \\frac{(SQ_{Res,\\,restrito} - SQ_{Res,\\,completo})/q}{SQ_{Res,\\,completo}/(n-p)},\n\\]\nem que:\n\n\\(q\\) é o número de restrições impostas (equivalentemente, o número de parâmetros “removidos” quando passamos do completo para o restrito);\n\\(p\\) é o número de parâmetros no modelo completo (incluindo intercepto).\n\nO teste \\(F\\) para modelos aninhados avalia se a redução em \\(SQ_{Res}\\) ao passarmos do modelo restrito para o completo é grande o suficiente para justificar os parâmetros adicionais.\n\nNo numerador está o “ganho médio” de ajuste por restrição relaxada: \\((SQ_{Res,restrito}-SQ_{Res,completo})/q\\).\nNo denominador está uma estimativa de \\(\\sigma^2\\) no modelo completo: \\(SQ_{Res,completo}/(n-p)\\).\n\nSe o ganho médio (numerador) é grande comparado ao ruído residual esperado (denominador), a estatística \\(F\\) fica grande e rejeitamos o modelo restrito.\n\n10.2.3.1 Hipóteses e decisão\n\nHipóteses:\n\n\\(H_0\\): as restrições são válidas (o modelo mais simples é suficiente).\n\n\\(H_1\\): pelo menos uma restrição é falsa (o modelo completo melhora o ajuste de forma relevante).\n\nRegra: rejeitar \\(H_0\\) para valores grandes de \\(F\\) (ou valor-p pequeno).\nUso: útil para verificar se a inclusão de um termo (ou intercepto) melhora de forma estatisticamente significativa o ajuste.\n\nLimitação: só se aplica a modelos aninhados (isto é, quando o modelo restrito pode ser obtido impondo restrições lineares nos parâmetros do modelo completo).\n\nAdicionalmente, mesmo quando o teste \\(F\\) rejeita \\(H_0\\), ainda é necessário verificar:\n\nse o termo adicional tem interpretação substantiva coerente;\nse a inclusão introduz instabilidade (por exemplo, poucos dados sustentando um termo);\nse o ganho é relevante na prática (e não apenas detectável por tamanho amostral).",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparação de Modelos</span>"
    ]
  },
  {
    "objectID": "mrls_compara.html#critérios-de-informação",
    "href": "mrls_compara.html#critérios-de-informação",
    "title": "10  Comparação de Modelos",
    "section": "10.3 Critérios de informação",
    "text": "10.3 Critérios de informação\nAlém dos critérios clássicos, que se baseiam em soma de quadrados e variâncias, temos medidas que incorporam explicitamente a ideia de verossimilhança e parcimônia. Esses critérios nascem de uma perspectiva mais geral de modelagem estatística, na qual o modelo é visto como uma aproximação para a distribuição geradora dos dados.\nNo contexto do MRLS com erros normais,\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i,\n\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\n\\]\na estimação por MQO coincide com a estimação por máxima verossimilhança. Assim, podemos usar a log-verossimilhança avaliada nos estimadores para comparar modelos.\n\n10.3.1 Critério de Informação de Akaike (AIC)\n\\[\nAIC = -2\\ell(\\hat\\theta) + 2p,\n\\]\nem que:\n\n\\(\\ell(\\hat\\theta)\\) é a log-verossimilhança no ponto estimado;\n\\(p\\) é o número de parâmetros estimados (incluindo \\(\\sigma^2\\), quando apropriado).\n\nO AIC pode ser interpretado como uma aproximação para a distância de Kullback–Leibler entre o modelo candidato e o verdadeiro mecanismo gerador dos dados (Akaike (1974); Burnham e Anderson (2002)). Ele busca selecionar o modelo que, entre os candidatos, minimiza a perda de informação esperada.\nA estrutura do critério explicita o compromisso:\n\n\\(-2\\ell(\\hat\\theta)\\) recompensa melhor ajuste (maior verossimilhança);\n\\(2p\\) penaliza complexidade.\n\nAssim, modelos com mais parâmetros precisam “ganhar” log-verossimilhança suficiente para compensar a penalização.\nInterpretação prática\n\nRegra: entre modelos comparáveis, preferir o de menor AIC.\nO AIC não tem interpretação absoluta; só faz sentido comparativamente.\nDiferenças pequenas (por exemplo, menores que 2 unidades) geralmente indicam que os modelos têm suporte semelhante nos dados (Burnham e Anderson (2002)).\n\nO AIC não testa hipóteses do tipo \\(H_0\\) vs. \\(H_1\\). Ele não produz valor-p nem decisão “rejeita/não rejeita”. É um critério de seleção por informação, não um teste clássico de significância.\nAlém disso, o AIC tende a favorecer modelos levemente mais complexos, especialmente em amostras pequenas. Em contextos de amostras reduzidas, versões corrigidas (como AICc) podem ser mais adequadas (ver Burnham e Anderson (2002)).\n\n\n10.3.2 Critério Bayesiano de Schwarz (BIC)\n\\[\nBIC = -2\\ell(\\hat\\theta) + p \\log(n).\n\\]\nO BIC possui forma semelhante ao AIC, mas a penalização cresce com \\(\\log(n)\\). Assim:\n\nPara amostras grandes, \\(\\log(n)\\) pode ser bem maior que 2;\nA penalização por parâmetro torna-se mais severa à medida que \\(n\\) aumenta.\n\nO BIC pode ser interpretado como uma aproximação ao log do fator de Bayes sob certas condições assintóticas (Schwarz (1978)). Por isso, ele é frequentemente associado a uma perspectiva de seleção do modelo “verdadeiro” dentro do conjunto candidato.\nInterpretação prática\n\nRegra: entre modelos comparáveis, preferir o de menor BIC.\nO BIC tende a ser mais conservador que o AIC.\nEm amostras grandes, pode penalizar fortemente modelos com muitos parâmetros.\n\n\n\n10.3.3 Boas práticas e comparabilidade entre escalas\nAIC vs. BIC: qual usar?\n\nAIC: mais orientado à predição e desempenho fora da amostra.\n\nBIC: mais orientado à identificação de uma estrutura “mais plausível” dentro do conjunto de candidatos.\nCompare apenas modelos ajustados na mesma escala da variável resposta.\n\nA escolha depende do objetivo da análise. Em contextos aplicados, é comum reportar ambos e discutir a coerência entre eles.\n\nNota importante sobre AIC e BIC:\nEmbora possamos calcular AIC e BIC para qualquer modelo ajustado, não é correto comparar diretamente os valores de um modelo ajustado em \\(Y\\) com outro ajustado em \\(\\log(Y)\\), por exemplo.\nIsso acontece porque a transformação da resposta altera a escala e a própria função de verossimilhança usada no cálculo dos critérios. Assim, os valores de AIC/BIC ficam em “bases diferentes”.\nComo proceder então?\n- Compare AIC/BIC apenas entre modelos ajustados na mesma escala da resposta.\n- Use análise de resíduos e diagnóstico gráfico para avaliar se a transformação foi benéfica.\n- Para fins de predição, utilize métricas de erro calculadas na escala original de \\(Y\\) (ex.: RMSE, MAE) para decidir qual modelo é mais adequado.\nEm síntese: AIC e BIC são ferramentas valiosas, mas devem ser usados com critério. Transformações em \\(Y\\) exigem avaliação adicional baseada em resíduos e desempenho preditivo.\n\n\n\n10.3.4 Estratégia prática de comparação\nAo comparar modelos alternativos:\n\nPrimeiro passo: verifique os resíduos (condição mínima).\n\nSe um modelo viola homocedasticidade ou apresenta estrutura sistemática, mesmo que tenha \\(R^2\\) alto, não deve ser preferido.\n\nSegundo passo: use os critérios formais.\n\nTeste \\(F\\) quando os modelos são aninhados.\n\n\\(R^2_{aj}\\) quando a comparação é dentro da mesma escala.\n\nAIC/BIC para comparações amplas (mesma resposta, diferentes ajustes).\n\nTerceiro passo: considere a interpretação substantiva.\n\nUm modelo matematicamente melhor pode ser substantivamente inadequado.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparação de Modelos</span>"
    ]
  },
  {
    "objectID": "mrls_compara.html#exemplos-ilustrativos-de-comparação-de-modelos",
    "href": "mrls_compara.html#exemplos-ilustrativos-de-comparação-de-modelos",
    "title": "10  Comparação de Modelos",
    "section": "10.4 Exemplos ilustrativos de comparação de modelos",
    "text": "10.4 Exemplos ilustrativos de comparação de modelos\nPara consolidar as ideias, apresentamos dois exemplos didáticos que demonstram como comparar modelos no contexto do MRLS. Cada exemplo explora um tipo de decisão prática enfrentada por analistas de dados.\n\n10.4.1 Exemplo 1 — Intercepto e transformação na resposta\nSimulação do Exemplo 1: gerar df1 com as variáveis X e Y (com \\(Y&gt;0\\), para permitir \\(\\log(Y)\\)).\n\nlibrary(dplyr)\n\nset.seed(2025)\n\n# Simulação: consumo (Y) vs tempo de funcionamento (X)\nn &lt;- 50\nX &lt;- seq(0, 10, length.out = n)\n\n# Consumo com intercepto &gt; 0 (stand-by) e heterocedasticidade moderada\nmu &lt;- 5 + 1.2*X\nsigma &lt;- 0.6 + 0.05*X\nY &lt;- mu + rnorm(n, 0, sigma)\nY &lt;- pmax(Y, 0.1)  # garantir Y&gt;0 (para log)\n\ndf1 &lt;- tibble(X = X, Y = Y)\n\n\nCenário e modelos candidatos: consumo de energia (\\(Y\\)) em função do tempo de funcionamento (\\(X\\)), com \\(n=50\\) observações.\n\nModelos candidatos:\n\nCom intercepto\n\\[\nY = \\beta_0 + \\beta_1 X + \\varepsilon\n\\] Representa consumo mínimo (stand-by). Esperado em contextos onde \\(Y&gt;0\\) mesmo quando \\(X=0\\).\nSem intercepto\n\\[\nY = \\beta_1 X + \\varepsilon\n\\] Força a reta a passar pela origem. Só faz sentido se sabemos que \\(Y=0\\) quando \\(X=0\\).\nTransformado (log da resposta)\n\\[\n\\log Y = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]\n\nNesta configuração com variável transformada, o coeficiente \\(\\beta_1\\) indica a mudança aditiva em \\(\\log(Y)\\) a cada unidade adicional em \\(X\\). Na escala original de \\(Y\\), isso equivale a dizer que um aumento de 1 unidade em \\(X\\) está associado a uma multiplicação esperada de \\(Y\\) por \\(\\exp(\\beta_1)\\).\nAqui estamos comparando três “ideias de modelo” que, apesar de parecidas na forma, respondem a perguntas ligeiramente diferentes:\n\nModelo A (com intercepto) permite que exista um nível médio de consumo quando \\(X=0\\) (consumo residual/stand-by).\n\nModelo A0 (sem intercepto) declara, como hipótese estrutural, que “se \\(X=0\\), então \\(Y=0\\)”. Essa é uma afirmação forte: se ela for falsa, o ajuste pode “compensar” deslocando a inclinação e gerando resíduos estruturados.\n\nModelo B (com \\(\\log Y\\)) muda a escala da resposta e, portanto, muda o tipo de erro “natural” no modelo (efeitos multiplicativos no \\(Y\\) original tendem a virar efeitos aditivos em \\(\\log(Y)\\)).\n\nVisualização da primeiras linhas da Base de dados\n\nhead(df1)\n\n# A tibble: 6 × 2\n      X     Y\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0      5.37\n2 0.204  5.27\n3 0.408  5.97\n4 0.612  6.54\n5 0.816  6.22\n6 1.02   6.12\n\n\n\nEstimação e resumos dos modelos\n\nAjustes e tabela de coeficientes: ajustar mod_A, mod_A0 e mod_log.\n\n# Ajustes\nmod_A   &lt;- lm(Y ~ X, data = df1)       # com intercepto\nmod_A0  &lt;- lm(Y ~ 0 + X, data = df1)   # sem intercepto\nmod_log &lt;- lm(log(Y) ~ X, data = df1)  # resposta em log\n\ncat(\"Modelo com intercepto (A):\\n\")\n\nModelo com intercepto (A):\n\nprint(coef(summary(mod_A)))\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 5.137603 0.24760614 20.74909 1.297563e-25\nX           1.196581 0.04266949 28.04301 2.032694e-31\n\ncat(\"\\nModelo sem intercepto (A0):\\n\")\n\n\nModelo sem intercepto (A0):\n\nprint(coef(summary(mod_A0)))\n\n  Estimate Std. Error t value     Pr(&gt;|t|)\nX 1.959437 0.06767437 28.9539 1.770922e-32\n\ncat(\"\\nModelo com log(Y) (B):\\n\")\n\n\nModelo com log(Y) (B):\n\nprint(coef(summary(mod_log)))\n\n             Estimate  Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 1.7791870 0.025029186 71.08450 2.673492e-50\nX           0.1143677 0.004313232 26.51554 2.548074e-30\n\n\nComentários:\n- O Modelo A (com intercepto) mostra um termo constante significativo, representando consumo em stand-by (\\(Y&gt;0\\) em \\(x \\approx 0\\)).\n- O Modelo A0 (sem intercepto) ignora o consumo em stand-by e força a reta a passar pela origem.\n- O Modelo B (log da resposta) reduz heteroscedasticidade: os coeficientes mantêm significância e a interpretação passa a ser na escala da variável transformada.\nTeste de resíduos (omnibus, Jarque-Bera, skew, kurtosis, Durbin-Watson, etc.)\n\n# Testes/medidas adicionais dos resíduos\n  library(lmtest)   # dwtest\n  library(tseries)  # jarque.bera.test\n  library(moments)  # skewness, kurtosis\n\nres_tests &lt;- function(mod, nome){\n  e &lt;- resid(mod)\n\n  jb &lt;- jarque.bera.test(e)\n  sk &lt;- skewness(e)\n  ku &lt;- kurtosis(e)  # kurtosis \"crua\" (Normal ~ 3)\n\n  # Durbin-Watson (lmtest)\n  dw &lt;- dwtest(mod)\n\n  # Omnibus D'Agostino-Pearson (quando disponível via fBasics::dagoTest)\n  omni_out &lt;- NULL\n  if (requireNamespace(\"fBasics\", quietly = TRUE)) {\n    omni_out &lt;- fBasics::dagoTest(e)\n  }\n\n  cat(\"\\n==============================\\n\")\n  cat(\"Testes dos resíduos —\", nome, \"\\n\")\n  cat(\"==============================\\n\")\n\n  cat(\"Jarque-Bera:\\n\")\n  print(jb)\n\n  cat(\"\\nAssimetria (skewness): \", round(sk, 4), \"\\n\", sep = \"\")\n  cat(\"Curtose (kurtosis):   \", round(ku, 4), \"  (Normal ~ 3)\\n\", sep = \"\")\n\n  cat(\"\\nDurbin-Watson (lmtest::dwtest):\\n\")\n  print(dw)\n\n  if (!is.null(omni_out)) {\n    cat(\"\\nOmnibus (D'Agostino-Pearson) — fBasics::dagoTest:\\n\")\n    print(omni_out)\n  } else {\n    cat(\"\\nOmnibus (D'Agostino-Pearson):\\n\")\n    cat(\"Pacote 'fBasics' não disponível no ambiente. (Opcional: instalar para executar o omnibus.)\\n\")\n  }\n}\n\nres_tests(mod_A,   \"Modelo A (Y ~ X, com intercepto)\")\n\n\n==============================\nTestes dos resíduos — Modelo A (Y ~ X, com intercepto) \n==============================\nJarque-Bera:\n\n    Jarque Bera Test\n\ndata:  e\nX-squared = 0.39891, df = 2, p-value = 0.8192\n\n\nAssimetria (skewness): 0.1998\nCurtose (kurtosis):   2.8216  (Normal ~ 3)\n\nDurbin-Watson (lmtest::dwtest):\n\n    Durbin-Watson test\n\ndata:  mod\nDW = 2.165, p-value = 0.6695\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nOmnibus (D'Agostino-Pearson) — fBasics::dagoTest:\n\nTitle:\n D'Agostino Normality Test\n\nTest Results:\n  STATISTIC:\n    Chi2 | Omnibus: 0.4194\n    Z3  | Skewness: 0.6388\n    Z4  | Kurtosis: 0.1065\n  P VALUE:\n    Omnibus  Test: 0.8108 \n    Skewness Test: 0.523 \n    Kurtosis Test: 0.9152 \n\nres_tests(mod_A0,  \"Modelo A0 (Y ~ X, sem intercepto)\")\n\n\n==============================\nTestes dos resíduos — Modelo A0 (Y ~ X, sem intercepto) \n==============================\nJarque-Bera:\n\n    Jarque Bera Test\n\ndata:  e\nX-squared = 2.6548, df = 2, p-value = 0.2652\n\n\nAssimetria (skewness): -0.0498\nCurtose (kurtosis):   1.8755  (Normal ~ 3)\n\nDurbin-Watson (lmtest::dwtest):\n\n    Durbin-Watson test\n\ndata:  mod\nDW = 0.22013, p-value &lt; 2.2e-16\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nOmnibus (D'Agostino-Pearson) — fBasics::dagoTest:\n\nTitle:\n D'Agostino Normality Test\n\nTest Results:\n  STATISTIC:\n    Chi2 | Omnibus: 8.7587\n    Z3  | Skewness: -0.1602\n    Z4  | Kurtosis: -2.9552\n  P VALUE:\n    Omnibus  Test: 0.01253 \n    Skewness Test: 0.8728 \n    Kurtosis Test: 0.003125 \n\nres_tests(mod_log, \"Modelo B (log(Y) ~ X)\")\n\n\n==============================\nTestes dos resíduos — Modelo B (log(Y) ~ X) \n==============================\nJarque-Bera:\n\n    Jarque Bera Test\n\ndata:  e\nX-squared = 3.1899, df = 2, p-value = 0.2029\n\n\nAssimetria (skewness): 0.5779\nCurtose (kurtosis):   3.4418  (Normal ~ 3)\n\nDurbin-Watson (lmtest::dwtest):\n\n    Durbin-Watson test\n\ndata:  mod\nDW = 1.5831, p-value = 0.04934\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nOmnibus (D'Agostino-Pearson) — fBasics::dagoTest:\n\nTitle:\n D'Agostino Normality Test\n\nTest Results:\n  STATISTIC:\n    Chi2 | Omnibus: 4.236\n    Z3  | Skewness: 1.7691\n    Z4  | Kurtosis: 1.0518\n  P VALUE:\n    Omnibus  Test: 0.1203 \n    Skewness Test: 0.07688 \n    Kurtosis Test: 0.2929 \n\n\nComentários:\n\nModelo A (com intercepto):\n\nO teste Omnibus e o Jarque-Bera não são significativos (valores-p altos), sugerindo que os resíduos não violam fortemente a normalidade.\n\nO Durbin-Watson próximo de 2 indica ausência de autocorrelação serial.\n\nConclusão: resíduos adequados, modelo consistente.\n\nModelo A0 (sem intercepto):\n\nO Durbin-Watson é muito baixo (≈ 0,2), indicando forte autocorrelação positiva dos resíduos.\n\nEmbora Omnibus/Jarque-Bera não rejeitem a normalidade, a dependência serial torna o modelo problemático.\n\nConclusão: estatisticamente inadequado, reforçando que a exclusão do intercepto distorce o ajuste.\n\nModelo B (log da resposta):\n\nTestes de normalidade (Omnibus, JB) continuam não significativos, mas a assimetria (Skew negativo) sugere leve desvio.\n\nO Durbin-Watson ≈ 1,4 aponta alguma autocorrelação positiva.\n\nConclusão: apesar das pequenas imperfeições, o modelo log-transformado melhora a homocedasticidade em relação ao Modelo A.\n\n\nEm dados simulados sem mecanismo temporal explícito, autocorrelação forte geralmente é um sinal de especificação inadequada (por exemplo, restrições erradas como \\(\\beta_0=0\\) podem “organizar” os resíduos e induzir padrões). Em aplicações reais, autocorrelação também pode ocorrer por dependência temporal genuína; nesse caso, o MRLS pode precisar ser estendido (tema para capítulos posteriores).\n\nModelos aninhados: teste F (A0 vs A)\n\n\n# Teste F (modelos aninhados): A0 (restrito) vs A (com intercepto)\ncat(\"ANOVA (Teste F: A0 vs A):\\n\")\n\nANOVA (Teste F: A0 vs A):\n\nprint(anova(mod_A0, mod_A))\n\nAnalysis of Variance Table\n\nModel 1: Y ~ 0 + X\nModel 2: Y ~ X\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     49 377.84                                  \n2     48  37.90  1    339.94 430.52 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nO modelo sem intercepto (A0) é um caso particular do modelo com intercepto (A), obtido ao impor a restrição \\(\\beta_0=0\\). Logo, o teste \\(F\\) avalia se “forçar a origem” piora o ajuste além do esperado por acaso.\nComentários:\n- O teste F compara os modelos A0 (restrito) e A (com intercepto).\n- O resultado altamente significativo (valor-p próximo de zero) indica que o intercepto é necessário.\n- Assim, rejeitamos o modelo sem intercepto e preferimos o modelo com intercepto.\n\nComparação numérica (\\(R^2\\), \\(R_{aj}^2\\), AIC, BIC)\n\n\nlibrary(tidyverse)\n\ncmp1 &lt;- tibble(\n  Modelo = c(\"A: Y~X (c/ intercepto)\", \"A0: Y~X (sem intercepto)\", \n             \"B: log(Y)~X\"),\n  R2     = c(summary(mod_A)$r.squared,     summary(mod_A0)$r.squared,     \n             summary(mod_log)$r.squared),\n  R2_aj  = c(summary(mod_A)$adj.r.squared, summary(mod_A0)$adj.r.squared, \n             summary(mod_log)$adj.r.squared),\n  AIC    = c(AIC(mod_A), AIC(mod_A0), AIC(mod_log)),\n  BIC    = c(BIC(mod_A), BIC(mod_A0), BIC(mod_log))\n)\n\ncmp1\n\n# A tibble: 3 × 5\n  Modelo                      R2 R2_aj   AIC   BIC\n  &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A: Y~X (c/ intercepto)   0.942 0.941 134.  140. \n2 A0: Y~X (sem intercepto) 0.945 0.944 247.  251. \n3 B: log(Y)~X              0.936 0.935 -95.1 -89.4\n\n\nLembre-se que um \\(R^2\\) elevado pode coexistir com resíduos ruins. Isso acontece porque \\(R^2\\) mede “quanto o modelo explica” em termos de variabilidade total, mas não garante que as hipóteses do MRLS estejam razoavelmente satisfeitas. Em particular, um modelo pode ter alto \\(R^2\\) e ainda assim produzir inferências pouco confiáveis (erros-padrão distorcidos).\nComentários:\n- O Modelo A0 apresenta \\(R^2\\) elevado, mas penalizações via AIC/BIC mostram que é muito inferior (valores bem maiores). Logo, acredita-se que os coeficientes ficaram distorcidos, superestimando a inclinação.\n- O Modelo A combina bom ajuste (\\(R^2_{aj}\\) alto) e parcimônia.\n- O Modelo B (log) apresenta os menores valores de AIC/BIC.\n\nDiagnóstico gráfico comparativo\n\nDispersões com retas ajustadas dos dois modelos restantes\n\nsuppressPackageStartupMessages({\n  library(ggplot2)\n  library(dplyr)\n  library(tidyr)\n})\n\ndfA  &lt;- df1 %&gt;% mutate(valor = Y,        modelo = \"A: Y ~ X (c/ intercepto)\")\ndfA0 &lt;- df1 %&gt;% mutate(valor = Y,        modelo = \"A0: Y ~ X (sem intercepto)\")\ndfB  &lt;- df1 %&gt;% mutate(valor = log(Y),   modelo = \"B: log(Y) ~ X\")\n\ndf_plot &lt;- bind_rows(dfA, dfA0, dfB)\n\nggplot(df_plot, aes(x = X, y = valor)) +\n  geom_point(alpha = 0.9, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1) +\n  facet_wrap(~modelo, ncol = 3, scales = \"free_y\") +\n  labs(x = \"X\", y = NULL) +\n  theme_minimal(base_size = 12)\n\n\n\n\nExemplo 1 — Dispersão com reta OLS: Modelo A (Y~X, com intercepto), Modelo A0 (Y~X, sem intercepto) e Modelo B (log(Y)~X).\n\n\n\n\nComentários:\n- Em ambos os Modelos (A e B), não observa-se aumento relevante da variabilidade de \\(Y\\) conforme \\(X\\) cresce.\nResíduos vs. valores ajustados\n\n  library(ggplot2)\n  library(dplyr)\n\ndados &lt;- bind_rows(\n  tibble(fit = fitted(mod_A),   res = resid(mod_A),   modelo = \"A: Y ~ X (c/ intercepto)\"),\n  tibble(fit = fitted(mod_A0),  res = resid(mod_A0),  modelo = \"A0: Y ~ X (sem intercepto)\"),\n  tibble(fit = fitted(mod_log), res = resid(mod_log), modelo = \"B: log(Y) ~ X\")\n)\n\nggplot(dados, aes(x = fit, y = res)) +\n  geom_point(alpha = 0.9, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", linewidth = 0.6) +\n  facet_wrap(~modelo, ncol = 3, scales = \"free_x\") +\n  labs(x = \"Ajustados\", y = \"Resíduo\") +\n  theme_minimal(base_size = 12)\n\n\n\n\nExemplo 1 — Resíduos vs Ajustados: comparação entre A, A0 e B.\n\n\n\n\nComentário:\nNeste gráfico, buscamos uma nuvem aproximadamente aleatória em torno de 0. Três estruturas são especialmente informativas:\n\nFunil (dispersão aumentando/diminuindo com o nível ajustado): sinal de heteroscedasticidade.\n\nCurvatura (padrão em arco): sinal de forma funcional inadequada (não linearidade não capturada).\n\nFaixas (bandas horizontais): podem surgir por discretização/limitação de medida em \\(Y\\).\n\nNa comparação entre modelos, o melhor candidato é o que reduz essas estruturas, sem criar novas.\nResíduos estudentizados vs. valores ajustados\n\n  library(ggplot2)\n  library(dplyr)\n\ndados &lt;- bind_rows(\n  tibble(fit = fitted(mod_A),   stud = rstudent(mod_A),   modelo = \"A: Y ~ X (c/ intercepto)\"),\n  tibble(fit = fitted(mod_A0),  stud = rstudent(mod_A0),  modelo = \"A0: Y ~ X (sem intercepto)\"),\n  tibble(fit = fitted(mod_log), stud = rstudent(mod_log), modelo = \"B: log(Y) ~ X\")\n)\n\nggplot(dados, aes(x = fit, y = stud)) +\n  geom_point(alpha = 0.9, size = 2) +\n  geom_hline(yintercept = 0,  linetype = \"dashed\", linewidth = 0.6) +\n  geom_hline(yintercept = c(-2, 2), linetype = \"dotted\", linewidth = 0.6) +\n  facet_wrap(~modelo, ncol = 3, scales = \"free_x\") +\n  labs(x = \"Ajustados\", y = \"t* (estudentizado)\") +\n  theme_minimal(base_size = 12)\n\n\n\n\nExemplo 1 — Resíduos estudentizados vs Ajustados: comparação entre A, A0 e B.\n\n\n\n\nComentário:\nEstes gráficos apresentam informações similares aos ilustradas nos gráficos anteriores\nQQ-plot dos resíduos\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# dados do QQ-plot + parâmetros da reta ao estilo qqline (base R)\nqq_df_line &lt;- function(mod, modelo){\n  r &lt;- rstandard(mod)\n  q &lt;- qqnorm(r, plot.it = FALSE)\n  df &lt;- tibble(theo = q$x, samp = q$y, modelo = modelo)\n\n  # mesma regra do qqline: reta baseada nos quartis (25% e 75%)\n  yq &lt;- quantile(r, probs = c(0.25, 0.75), na.rm = TRUE)\n  xq &lt;- qnorm(c(0.25, 0.75))\n\n  slope &lt;- (yq[2] - yq[1])/(xq[2] - xq[1])\n  intercept &lt;- yq[1] - slope*xq[1]\n\n  list(df = df, line = tibble(modelo = modelo, intercept = intercept, slope = slope))\n}\n\noutA  &lt;- qq_df_line(mod_A,   \"A: Y ~ X (c/ intercepto)\")\noutA0 &lt;- qq_df_line(mod_A0,  \"A0: Y ~ X (sem intercepto)\")\noutB  &lt;- qq_df_line(mod_log, \"B: log(Y) ~ X\")\n\ndados &lt;- bind_rows(outA$df, outA0$df, outB$df)\nlinhas &lt;- bind_rows(outA$line, outA0$line, outB$line)\n\nggplot(dados, aes(x = theo, y = samp)) +\n  geom_point(alpha = 0.9, size = 2) +\n  geom_abline(data = linhas, aes(intercept = intercept, slope = slope),\n              linewidth = 0.8) +\n  facet_wrap(~modelo, ncol = 3, scales = \"free\") +\n  labs(x = \"Quantis teoricos (Normal)\",\n       y = \"Residuos padronizados\") +\n  theme_minimal(base_size = 12)\n\n\n\n\nExemplo 1 — QQ-plot (rstandard) com reta tipo qqline: comparação entre A, A0 e B.\n\n\n\n\nComentário:\nO QQ-plot avalia normalidade aproximada por meio do alinhamento entre os quantis amostrais dos resíduos e os quantis teóricos da Normal. A interpretação deve ser feita por padrões:\n\nAlinhamento global próximo da reta: evidência visual a favor de resíduos aproximadamente normais (pelo menos no “miolo” da distribuição).\n\nDesvios sistemáticos nas caudas (pontos afastando-se da reta apenas no início e no fim): indicam caudas mais pesadas ou mais leves que a Normal; isso costuma afetar sobretudo inferência em amostras pequenas.\n\nPadrão em “S”: sugere assimetria (skewness diferente de 0).\n\nUm ou poucos pontos muito afastados: podem ser indício de outliers (verificar também resíduos estudentizados e Cook).\n\nAo comparar modelos, prefira o que apresenta menos estrutura sistemática no QQ-plot, especialmente quando isso é coerente com as medidas numéricas de assimetria/curtose e com testes como Jarque–Bera.\nHistograma dos resíduos\n\n  library(ggplot2)\n  library(dplyr)\n\ndados &lt;- bind_rows(\n  tibble(r = resid(mod_A),   modelo = \"A: Y ~ X (c/ intercepto)\"),\n  tibble(r = resid(mod_A0),  modelo = \"A0: Y ~ X (sem intercepto)\"),\n  tibble(r = resid(mod_log), modelo = \"B: log(Y) ~ X\")\n)\n\nggplot(dados, aes(x = r)) +\n  geom_histogram(bins = 12, alpha = 0.7, color = \"white\") +\n  facet_wrap(~modelo, ncol = 3, scales = \"free\") +\n  labs(x = \"Resíduo\", y = \"Frequência\") +\n  theme_minimal(base_size = 12)\n\n\n\n\nExemplo 1 — Histograma dos resíduos: comparação entre A, A0 e B.\n\n\n\n\nComentário:\n\nO histograma é um diagnóstico auxiliar: ele ajuda a visualizar assimetria e caudas. Em geral:\n\nhistograma muito assimétrico sugere assimetria nos resíduos;\n\ncaudas longas ou “ombros” podem sugerir caudas pesadas e/ou outliers.\n\nA interpretação deve ser combinada com QQ-plot e com medidas como assimetria/curtose.\nResíduos vs. X\n\nsuppressPackageStartupMessages({\n  library(ggplot2)\n  library(dplyr)\n})\n\ndados &lt;- bind_rows(\n  tibble(X = df1$X, res = resid(mod_A),   modelo = \"A: Y ~ X (c/ intercepto)\"),\n  tibble(X = df1$X, res = resid(mod_A0),  modelo = \"A0: Y ~ X (sem intercepto)\"),\n  tibble(X = df1$X, res = resid(mod_log), modelo = \"B: log(Y) ~ X\")\n)\n\nggplot(dados, aes(x = X, y = res)) +\n  geom_point(alpha = 0.9, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", linewidth = 0.6) +\n  facet_wrap(~modelo, ncol = 3, scales = \"free_y\") +\n  labs(x = \"X\", y = \"Resíduo\") +\n  theme_minimal(base_size = 12)\n\n\n\n\nExemplo 1 — Resíduos vs X: comparação entre A, A0 e B.\n\n\n\n\nComentário:\n- Ambos os Modelos (A e B) apresentam dispersão uniforme em torno de zero, corroborando a homocedasticidade.\nO diagnóstico gráfico serve como “primeira triagem”: se houver funil, curvatura ou caudas muito pesadas, isso aparece visualmente de modo imediato. Somente depois disso faz sentido dar peso às comparações numéricas.\n\nConclusões do exemplo\n\n\nO modelo sem intercepto tende a se ajustar mal, pois ignora o consumo em stand-by. Este modelo foi descartado no teste com modelo aninhados.\n\nO modelo com intercepto melhora substancialmente o ajuste quando comparado com o modelo sem intercepto.\n\nAssim como o modelo com intercepto (A), o modelo com variável transformada (B) também mostrou um bom ajuste.\n\nOs resíduos dos modelos A e B mostraram que podemos considerar que ambos os modelos foram bem especificados.\n\nO modelo B apresentou AIC/BIC menores. No entanto, como apresentado anteriormente, não é correto comparar diretamente os valores de um modelo ajustado em \\(Y\\) com outro ajustado em \\(\\log(Y)\\), pois a verossimilhança é diferente.\n\nConsiderando que os modelos A e B foram bem especificados e nao teve uma métrica de qualidade de ajuste muito favorável a um deles, é aconselhado o uso do modelo mais simples e na escala original da variável (Modelo A - com intercepto).\n\n\n\n10.4.2 Exemplo 2 — Escolhendo a melhor variável explicativa\n\nCenário e modelos candidatos um pesquisador deseja explicar a produtividade agrícola (\\(Y\\)) a partir de três variáveis candidatas:\n\n\n\\(X_1\\) = quantidade de fertilizante aplicada (kg/ha)\n\n\\(X_2\\) = volume de irrigação (mm)\n\n\\(X_3\\) = horas de sol na safra (h)\n\nO objetivo é descobrir qual dessas variáveis explica melhor \\(Y\\) de forma individual.\nOs três MRLS candidatos são:\n\nModelo A: \\(Y = \\beta_0 + \\beta_1 X_1 + \\varepsilon\\)\n\nModelo B: \\(Y = \\beta_0 + \\beta_2 X_2 + \\varepsilon\\)\n\nModelo C: \\(Y = \\beta_0 + \\beta_3 X_3 + \\varepsilon\\)\n\nPassos da análise comparativa:\n\nAjuste de cada modelo separadamente.\n\nDiagnóstico dos resíduos em cada caso, verificando linearidade, homoscedasticidade e normalidade.\n\nComparação de medidas de ajuste: \\(R^2\\), \\(R^2_{aj}\\), AIC e BIC.\n\nDiscussão substantiva: qual variável faz mais sentido teoricamente como explicativa de \\(Y\\).\n\nCritérios práticos de decisão:\n- Se todos os modelos tiverem resíduos adequados, a comparação pode ser feita pelos critérios formais (AIC/BIC, \\(R^2_{aj}\\)).\n- Se apenas um modelo tiver resíduos consistentes com as hipóteses do MRLS, ele deve ser preferido.\n- Mesmo que dois modelos apresentem ajustes estatisticamente próximos, a escolha deve considerar a interpretação prática.\n\nSimulação dos dados\n\n\nsuppressPackageStartupMessages({\n  library(dplyr)\n})\n\nset.seed(2025)\n\n# Simulação: produtividade (Y) explicada por X1, X2, X3 (candidatas)\nn2 &lt;- 120\nX1 &lt;- runif(n2,   0, 100)   # fertilizante, 0–100 kg/ha\nX2 &lt;- runif(n2,   0,  60)   # irrigação, 0–60 mm\nX3 &lt;- runif(n2, 200, 350)   # horas de sol, 200–350 h\n\n# Verdade de geração\nY2 &lt;- 20 + 0.45*X1 + 0.12*X2 + 0.02*X3 + rnorm(n2, 0, 10)\n\ndf2 &lt;- tibble(Y = Y2, X1 = X1, X2 = X2, X3 = X3)\n\nhead(df2)\n\n# A tibble: 6 × 4\n      Y    X1    X2    X3\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  68.4  73.3  43.4  239.\n2  59.9  47.6  39.9  245.\n3  67.5  51.4  44.3  229.\n4  59.2  49.8  33.2  332.\n5  49.9  78.0  41.1  250.\n6  42.0  50.4  43.0  241.\n\n\nNeste exemplo, as três variáveis candidatas têm escalas diferentes, mas isso não impede o ajuste de três MRLS separados. O que muda é a interpretação dos coeficientes e a magnitude dos erros-padrão. Como o objetivo é “melhor preditor individual”, estamos comparando três modelos alternativos não aninhados.\nEste exemplo buscar mostrar que, mesmo no contexto de MRLS, a comparação entre variáveis explicativas pode guiar a seleção do melhor preditor individual.\n\nRoteiro de tarefas (atividade guiada)\n\n\nAjuste os três modelos candidatos separadamente:\n\nModelo A: $ Y = _0 + _1 X_1 + $\n\nModelo B: $ Y = _0 + _2 X_2 + $\nModelo C: $ Y = _0 + _3 X_3 + $\n\nInspecione os resíduos de cada modelo:\n\nGráficos de resíduos versus ajustados.\n\nQQ-plot para verificar normalidade.\n\nHistograma dos resíduos.\n\nResíduos versus a variável explicativa \\(X\\).\n\nCompare as medidas de ajuste entre os modelos:\n\n\\(R^2_{aj}\\) (ajustado)\n\nAIC\n\nBIC\n\nDiscuta os resultados obtidos:\n\nQual modelo apresentou resíduos mais consistentes com as hipóteses do MRLS?\n\nQual modelo apresentou melhor desempenho segundo \\(R^2_{aj}\\), AIC e BIC?\n\nExiste coerência entre os diagnósticos gráficos e as medidas numéricas?\n\nReflexão substantiva:\n\nDo ponto de vista prático, qual variável é a melhor candidata a explicar a produtividade agrícola (\\(Y\\)) individualmente e por quê?\n\nConsidere plausibilidade causal e relevância no contexto agrícola (fertilizante, irrigação ou horas de sol).\n\nDesafio opcional:\n\nRe-simule os dados com outro seed ou altere o nível de ruído.\n\nObserve se a escolha do melhor modelo permanece a mesma ou se muda.\n\n\n\n# Ajustes dos três MRLS candidatos\nm1 &lt;- lm(Y ~ X1, data = df2)\nm2 &lt;- lm(Y ~ X2, data = df2)\nm3 &lt;- lm(Y ~ X3, data = df2)\n\ncat(\"=== Modelo A: Y ~ X1 ===\\n\"); print(summary(m1))\n\n=== Modelo A: Y ~ X1 ===\n\n\n\nCall:\nlm(formula = Y ~ X1, data = df2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-30.5982  -6.0204   0.6442   6.9029  27.5385 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 31.49842    1.84972   17.03   &lt;2e-16 ***\nX1           0.41940    0.03076   13.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.73 on 118 degrees of freedom\nMultiple R-squared:  0.6117,    Adjusted R-squared:  0.6084 \nF-statistic: 185.9 on 1 and 118 DF,  p-value: &lt; 2.2e-16\n\ncat(\"\\n=== Modelo B: Y ~ X2 ===\\n\"); print(summary(m2))\n\n\n=== Modelo B: Y ~ X2 ===\n\n\n\nCall:\nlm(formula = Y ~ X2, data = df2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-45.332 -10.528   2.142  12.276  35.247 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 47.69985    2.63414  18.108  &lt; 2e-16 ***\nX2           0.19641    0.07436   2.641  0.00937 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.17 on 118 degrees of freedom\nMultiple R-squared:  0.05583,   Adjusted R-squared:  0.04783 \nF-statistic: 6.977 on 1 and 118 DF,  p-value: 0.009374\n\ncat(\"\\n=== Modelo C: Y ~ X3 ===\\n\"); print(summary(m3))\n\n\n=== Modelo C: Y ~ X3 ===\n\n\n\nCall:\nlm(formula = Y ~ X3, data = df2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.032 -12.257   1.166  11.872  31.353 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 58.31156    8.78394   6.638 1.02e-09 ***\nX3          -0.01715    0.03168  -0.541    0.589    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.59 on 118 degrees of freedom\nMultiple R-squared:  0.002478,  Adjusted R-squared:  -0.005975 \nF-statistic: 0.2932 on 1 and 118 DF,  p-value: 0.5892\n\n\n\ncmp2 &lt;- tibble(\n  Modelo = c(\"A: Y~X1\", \"B: Y~X2\", \"C: Y~X3\"),\n  R2     = c(summary(m1)$r.squared,     summary(m2)$r.squared,     summary(m3)$r.squared),\n  R2_aj  = c(summary(m1)$adj.r.squared, summary(m2)$adj.r.squared, summary(m3)$adj.r.squared),\n  AIC    = c(AIC(m1), AIC(m2), AIC(m3)),\n  BIC    = c(BIC(m1), BIC(m2), BIC(m3))\n)\n\ncmp2\n\n# A tibble: 3 × 5\n  Modelo       R2    R2_aj   AIC   BIC\n  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A: Y~X1 0.612    0.608    891.  899.\n2 B: Y~X2 0.0558   0.0478   997. 1006.\n3 C: Y~X3 0.00248 -0.00598 1004. 1012.\n\n\nDiagnóstico gráfico comparativo:\n- dispersão \\(Y\\) vs cada \\(X_j\\) com reta OLS (um painel por modelo);\n- resíduos vs ajustados (um painel por modelo);\n- QQ-plot (um painel por modelo);\n- histograma dos resíduos (um painel por modelo);\n- resíduos vs \\(X_j\\) (um painel por modelo).\n\nsuppressPackageStartupMessages({\n  library(ggplot2)\n  library(dplyr)\n  library(tidyr)\n})\n\n# Organizar dados em formato longo para facilitar facetas\nlong_xy &lt;- df2 %&gt;%\n  pivot_longer(cols = c(X1, X2, X3), names_to = \"Xname\", values_to = \"X\") %&gt;%\n  mutate(modelo = recode(Xname, X1 = \"Modelo A: Y~X1\", X2 = \"Modelo B: Y~X2\", X3 = \"Modelo C: Y~X3\"))\n\n# Função auxiliar para extrair resíduos e ajustados por modelo\naug1 &lt;- tibble(fit = fitted(m1), res = resid(m1), modelo = \"Modelo A: Y~X1\")\naug2 &lt;- tibble(fit = fitted(m2), res = resid(m2), modelo = \"Modelo B: Y~X2\")\naug3 &lt;- tibble(fit = fitted(m3), res = resid(m3), modelo = \"Modelo C: Y~X3\")\naug  &lt;- bind_rows(aug1, aug2, aug3)\n\n# 1) Y vs X com reta OLS\np1 &lt;- ggplot(long_xy, aes(x = X, y = Y)) +\n  geom_point(alpha = 0.85, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1) +\n  facet_wrap(~modelo, ncol = 3, scales = \"free_x\") +\n  labs(x = NULL, y = \"Y\") +\n  theme_minimal(base_size = 12)\n\n# 2) Resíduos vs ajustados\np2 &lt;- ggplot(aug, aes(x = fit, y = res)) +\n  geom_point(alpha = 0.85, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", linewidth = 0.6) +\n  facet_wrap(~modelo, ncol = 3, scales = \"free_x\") +\n  labs(x = \"Ajustados\", y = \"Resíduo\") +\n  theme_minimal(base_size = 12)\n\n# 3) QQ-plot (estilo plot.lm, which = 2)\n\nqq_df_lmstyle &lt;- function(mod, modelo){\n  r &lt;- rstandard(mod)                 # mesmo tipo de resíduo do plot.lm\n  q &lt;- qqnorm(r, plot.it = FALSE)\n\n  pts &lt;- tibble(\n    theo = q$x,\n    samp = q$y,\n    modelo = modelo\n  )\n\n  # reta tipo qqline(): passa pelos quartis 25% e 75%\n  yq &lt;- quantile(r, c(0.25, 0.75))\n  xq &lt;- qnorm(c(0.25, 0.75))\n  slope &lt;- (yq[2] - yq[1])/(xq[2] - xq[1])\n  intercept &lt;- yq[1] - slope*xq[1]\n\n  line &lt;- tibble(\n    modelo = modelo,\n    intercept = as.numeric(intercept),\n    slope = as.numeric(slope)\n  )\n\n  list(pts = pts, line = line)\n}\n\no1 &lt;- qq_df_lmstyle(m1, \"Modelo A: Y~X1\")\no2 &lt;- qq_df_lmstyle(m2, \"Modelo B: Y~X2\")\no3 &lt;- qq_df_lmstyle(m3, \"Modelo C: Y~X3\")\n\nqq_all  &lt;- bind_rows(o1$pts,  o2$pts,  o3$pts)\nqq_line &lt;- bind_rows(o1$line, o2$line, o3$line)\n\np3 &lt;- ggplot(qq_all, aes(x = theo, y = samp)) +\n  geom_point(alpha = 0.85, size = 2) +\n  geom_abline(\n    data = qq_line,\n    aes(intercept = intercept, slope = slope),\n    linewidth = 0.8\n  ) +\n  facet_wrap(~modelo, ncol = 3, scales = \"free\") +\n  labs(x = \"Quantis teóricos (Normal)\", y = \"Resíduos padronizados\") +\n  theme_minimal(base_size = 12)\n\n# 4) Histogramas dos resíduos\nres_all &lt;- bind_rows(\n  tibble(r = resid(m1), modelo = \"Modelo A: Y~X1\"),\n  tibble(r = resid(m2), modelo = \"Modelo B: Y~X2\"),\n  tibble(r = resid(m3), modelo = \"Modelo C: Y~X3\")\n)\n\np4 &lt;- ggplot(res_all, aes(x = r)) +\n  geom_histogram(bins = 12, alpha = 0.7, color = \"white\") +\n  facet_wrap(~modelo, ncol = 3, scales = \"free\") +\n  labs(x = \"Resíduo\", y = \"Frequência\") +\n  theme_minimal(base_size = 12)\n\n# 5) Resíduos vs X (por modelo)\nrx &lt;- bind_rows(\n  df2 %&gt;% transmute(X = X1, res = resid(m1), modelo = \"Modelo A: Y~X1\"),\n  df2 %&gt;% transmute(X = X2, res = resid(m2), modelo = \"Modelo B: Y~X2\"),\n  df2 %&gt;% transmute(X = X3, res = resid(m3), modelo = \"Modelo C: Y~X3\")\n)\n\np5 &lt;- ggplot(rx, aes(x = X, y = res)) +\n  geom_point(alpha = 0.85, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", linewidth = 0.6) +\n  facet_wrap(~modelo, ncol = 3, scales = \"free_x\") +\n  labs(x = NULL, y = \"Resíduo\") +\n  theme_minimal(base_size = 12)\n\n# Mostrar os 5 gráficos (um por vez, na ordem)\nprint(p1)\n\n\n\n\nDiagnóstico gráfico comparativo (Exemplo 2): cada linha corresponde a um modelo (A, B, C).\n\n\n\nprint(p2)\n\n\n\n\nDiagnóstico gráfico comparativo (Exemplo 2): cada linha corresponde a um modelo (A, B, C).\n\n\n\nprint(p3)\n\n\n\n\nDiagnóstico gráfico comparativo (Exemplo 2): cada linha corresponde a um modelo (A, B, C).\n\n\n\nprint(p4)\n\n\n\n\nDiagnóstico gráfico comparativo (Exemplo 2): cada linha corresponde a um modelo (A, B, C).\n\n\n\nprint(p5)\n\n\n\n\nDiagnóstico gráfico comparativo (Exemplo 2): cada linha corresponde a um modelo (A, B, C).\n\n\n\n\n\nSe um modelo “vence” nos critérios numéricos, mas apresenta funil/curvatura/caudas pesadas, ele deve perder força como candidato.\n\nSe dois modelos forem próximos numericamente, a decisão pode depender do contexto (medição, causalidade plausível, custo de obter a variável, etc.).\n\n\nFechamento e síntese\n\nOs dois exemplos mostraram situações complementares na prática do MRLS. O Exemplo 1 destacou como diferentes especificações de um mesmo modelo, com intercepto, sem intercepto e com transformação em (Y), podem levar a conclusões distintas sobre ajuste, resíduos e interpretação. Já o Exemplo 2 explorou a escolha entre diferentes variáveis candidatas, mostrando como a comparação de modelos separados orienta a seleção do preditor mais adequado.\nEm conjunto, os exemplos reforçam que a escolha do modelo não deve se basear apenas em indicadores numéricos, mas sim no equilíbrio entre consistência estatística, parcimônia e coerência substantiva com o fenômeno estudado.\n\n\n\n\nAkaike, Hirotugu. 1974. “A new look at the statistical model identification”. IEEE Transactions on Automatic Control 19 (6): 716–23.\n\n\nBurnham, Kenneth P., e David R. Anderson. 2002. Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach. 2º ed. New York: Springer.\n\n\nSchwarz, Gideon. 1978. “Estimating the Dimension of a Model”. The Annals of Statistics 6 (2): 461–64.",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Comparação de Modelos</span>"
    ]
  },
  {
    "objectID": "part2_ex.html",
    "href": "part2_ex.html",
    "title": "11  Exercícios e atividades",
    "section": "",
    "text": "Em breve!!!",
    "crumbs": [
      "Parte II — Modelo de Regressão Linear Simples (MRLS)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exercícios e atividades</span>"
    ]
  },
  {
    "objectID": "part3_ex.html",
    "href": "part3_ex.html",
    "title": "12  Exercícios e atividades",
    "section": "",
    "text": "Em breve!!!",
    "crumbs": [
      "Parte III — Modelo de Regressão Linear Simples (MRLM)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exercícios e atividades</span>"
    ]
  },
  {
    "objectID": "ap_listas.html",
    "href": "ap_listas.html",
    "title": "13  Lista de Siglas e Símbolos",
    "section": "",
    "text": "13.1 Lista de Siglas",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Lista de Siglas e Símbolos</span>"
    ]
  },
  {
    "objectID": "ap_listas.html#lista-de-siglas",
    "href": "ap_listas.html#lista-de-siglas",
    "title": "13  Lista de Siglas e Símbolos",
    "section": "",
    "text": "Sigla\nSignificado\n\n\n\n\nMRLS\nModelo de Regressão Linear Simples\n\n\nMRLM\nModelo de Regressão Linear Múltipla\n\n\nMQO / OLS\nMínimos Quadrados Ordinários / Ordinary Least Squares\n\n\nMV\nMáxima Verossimilhança\n\n\nWLS\nWeighted Least Squares (Regressão Linear Ponderada)\n\n\nGLS\nGeneralized Least Squares (MQ Generalizados)\n\n\nBLUE\nBest Linear Unbiased Estimator (Melhor Estimador Linear Não-Viesado)\n\n\nGLM\nGeneralized Linear Model (Modelo Linear Generalizado)\n\n\nGAM\nGeneralized Additive Model (Modelo Aditivo Generalizado)\n\n\nGAMLSS\nGeneralized Additive Model for Location, Scale and Shape\n\n\nANOVA\nAnálise de Variância\n\n\nIC\nIntervalo de Confiança\n\n\nEP\nErro-Padrão\n\n\nIC95%\nIntervalo de Confiança a 95%\n\n\nH0, H1\nHipótese nula, Hipótese alternativa\n\n\nPDF, CDF\nFunção Densidade de Probabilidade; Função de Distribuição Acumulada\n\n\nAIC, BIC, AICc\nCritérios de Informação (Akaike, Bayesiano, Akaike corrigido)\n\n\n\\(R^2\\), \\(\\bar{R}^2\\)\nCoeficiente de determinação e ajustado\n\n\nSQTot, SQReg, SQRes\nSomas de Quadrados (Total, Regressão, Resíduos)\n\n\nPRESS\nPredicted Residual Sum of Squares (LOOCV)\n\n\nCV-\\(k\\)\nk-fold Cross-Validation\n\n\nVIF\nVariance Inflation Factor\n\n\ndf\nGraus de liberdade\n\n\nVar, Cov, Corr\nVariância, Covariância, Correlação\n\n\nFDR\nFalse Discovery Rate",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Lista de Siglas e Símbolos</span>"
    ]
  },
  {
    "objectID": "ap_listas.html#lista-de-símbolos",
    "href": "ap_listas.html#lista-de-símbolos",
    "title": "13  Lista de Siglas e Símbolos",
    "section": "13.2 Lista de Símbolos",
    "text": "13.2 Lista de Símbolos\n\n\n\n\n\n\n\nSímbolo\nDescrição\n\n\n\n\n\\(Y,\\ \\mathbf{Y}\\)\nVariável resposta (escalar / vetor de observações)\n\n\n\\(X,\\ \\mathbf{X}\\)\nMatriz de covariáveis (matriz de planejamento)\n\n\n\\(\\beta_0,\\ \\beta_1,\\ \\dots,\\ \\beta_p\\)\nParâmetros do modelo (intercepto e inclinações)\n\n\n\\(\\boldsymbol{\\beta}\\)\nVetor de parâmetros \\((\\beta_0,\\ \\beta_1,\\ \\ldots,\\ \\beta_p)^\\top\\)\n\n\n\\(\\varepsilon_i,\\ \\boldsymbol{\\varepsilon}\\)\nTermo(s) de erro aleatório (escalar / vetor)\n\n\n\\(\\hat{\\beta}_j,\\ \\hat{\\boldsymbol{\\beta}}\\)\nEstimadores (MQO/MV) dos parâmetros\n\n\n\\(\\hat{Y}_i,\\ \\hat{\\mathbf{Y}}\\)\nValores ajustados pelo modelo (escalar / vetor)\n\n\n\\(\\hat{\\varepsilon}_i,\\ \\hat{\\boldsymbol{\\varepsilon}}\\)\nResíduos (observado – ajustado; escalar / vetor)\n\n\n\\(\\hat{\\sigma}^2\\)\nEstimador da variância residual (\\(\\hat{\\sigma}^2=\\mathrm{SQRes}/(n-p-1)\\))\n\n\n\\(\\mathbf{H}\\)\nMatriz “chapéu” (projeção): \\(\\ \\mathbf{H}=\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\)\n\n\n\\(\\mathbf{M}\\)\nMatriz dos resíduos: \\(\\ \\mathbf{M}=\\mathbf{I}_n-\\mathbf{H}\\)\n\n\n\\(h_{ii}\\)\nAlavancagem (diagonal de \\(\\mathbf{H}\\))\n\n\n\\(r_i\\)\nResíduo studentizado: \\(r_i=\\dfrac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\\)\n\n\n\\(D_i\\)\nDistância de Cook\n\n\n\\(\\mathbf{I}_n\\)\nMatriz identidade de dimensão \\(n\\)\n\n\n\\(\\mathrm{col}(\\mathbf{X})\\)\nEspaço coluna de \\(\\mathbf{X}\\)\n\n\n\\(\\operatorname{rank}(\\mathbf{X})\\)\nPosto (rank) de \\(\\mathbf{X}\\)\n\n\n\\(\\mathbf{X}^+\\)\nInversa generalizada de Moore–Penrose\n\n\n\\(\\mathrm{tr}(\\mathbf{A})\\)\nTraço da matriz \\(\\mathbf{A}\\)\n\n\n\\(\\det(\\mathbf{A})\\)\nDeterminante de \\(\\mathbf{A}\\)\n\n\n\\(\\mathbf{A}^{-1}\\), \\(\\mathbf{A}^\\top\\)\nInversa e transposta de \\(\\mathbf{A}\\)\n\n\n\\(\\mathbb{R}^n\\)\nEspaço vetorial real \\(n\\)-dimensional\n\n\n\\(\\mathcal{N}_n(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\)\nDistribuição Normal \\(n\\)-variada\n\n\n\\(\\boldsymbol{\\mu},\\ \\boldsymbol{\\Sigma}\\)\nMédia e matriz de covariância na Normal multivariada\n\n\n\\(n,\\ p\\)\nNº de observações; Nº de variáveis explicativas\n\n\n\\(\\bar{X},\\ \\bar{Y}\\)\nMédias amostrais de \\(X\\) e \\(Y\\)\n\n\n\\(S_{xx},\\ S_{xy}\\)\nSomas de quadrados e produto cruzado (MRLS)\n\n\n\\(\\mathbf{C},\\ d\\)\nMatriz de contrastes e vetor-alvo (testes conjuntos: \\(H_0:\\mathbf{C}\\boldsymbol{\\beta}=d\\))",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Lista de Siglas e Símbolos</span>"
    ]
  },
  {
    "objectID": "ap_matrizes.html",
    "href": "ap_matrizes.html",
    "title": "14  Estrutura Matricial dos Modelos de Regressão Linear",
    "section": "",
    "text": "14.1 Operações Fundamentais com Matrizes e Vetores\nA formulação moderna dos modelos de regressão linear é essencialmente matricial. Essa notação vetorial revela a estrutura geométrica do problema de estimação, explicita as condições necessárias para identificabilidade dos parâmetros e permite analisar propriedades estatísticas dos estimadores de forma sistemática (ver Harville (1997)).\nEste apêndice consolida os principais elementos de Álgebra Linear utilizados ao longo do estudo de regressão, com ênfase nas estruturas que reaparecem na estimação por mínimos quadrados, na inferência e na análise de diagnóstico.\nA linguagem matricial é uma forma compacta de escrever o modelo de regressão que permite enxergar o problema como um problema geométrico em \\(\\mathbb{R}^n\\). Cada vetor corresponde a um ponto ou direção nesse espaço, e cada matriz representa uma transformação linear.\nSejam \\(\\mathbf{A}\\) e \\(\\mathbf{B}\\) matrizes de dimensões compatíveis e \\(\\mathbf{x}, \\mathbf{y}\\) vetores coluna em \\(\\mathbb{R}^n\\).\nPara fixar ideias, considere explicitamente:\n\\[\n\\mathbf{x} =\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix},\n\\qquad\n\\mathbf{y} =\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}.\n\\]",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estrutura Matricial dos Modelos de Regressão Linear</span>"
    ]
  },
  {
    "objectID": "ap_matrizes.html#operações-fundamentais-com-matrizes-e-vetores",
    "href": "ap_matrizes.html#operações-fundamentais-com-matrizes-e-vetores",
    "title": "14  Estrutura Matricial dos Modelos de Regressão Linear",
    "section": "",
    "text": "14.1.1 Soma Matricial\nSe\n\\[\n\\mathbf{A} =\n\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix},\n\\qquad\n\\mathbf{B} =\n\\begin{bmatrix}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22}\n\\end{bmatrix},\n\\]\nentão\n\\[\n\\mathbf{A} + \\mathbf{B}\n=\n\\begin{bmatrix}\na_{11}+b_{11} & a_{12}+b_{12} \\\\\na_{21}+b_{21} & a_{22}+b_{22}\n\\end{bmatrix}.\n\\]\nA soma é definida elemento a elemento e exige dimensões idênticas.\n\n\n14.1.2 Produto Matricial\nSe\n\\[\n\\mathbf{A} =\n\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix},\n\\qquad\n\\mathbf{B} =\n\\begin{bmatrix}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22}\n\\end{bmatrix},\n\\]\nentão\n\\[\n\\mathbf{A}\\mathbf{B}\n=\n\\begin{bmatrix}\na_{11}b_{11}+a_{12}b_{21} & a_{11}b_{12}+a_{12}b_{22} \\\\\na_{21}b_{11}+a_{22}b_{21} & a_{21}b_{12}+a_{22}b_{22}\n\\end{bmatrix}.\n\\]\nO produto matricial corresponde à composição de transformações lineares.\nNo modelo linear\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta},\n\\]\na matriz \\(\\mathbf{X}\\), de dimensão \\(n \\times (p+1)\\) transforma o vetor de parâmetros \\(\\boldsymbol{\\beta}\\), de dimensão \\((p+1) \\times 1\\) em um vetor no espaço das respostas. Assim, \\(\\mathbf{X}\\) pode ser interpretada como uma transformação que leva parâmetros em \\(\\mathbb{R}^{p+1}\\) para vetores ajustados em \\(\\mathbb{R}^n\\).\n\n\n14.1.3 Produto Interno e Norma\nO produto interno entre vetores é dado por\n\\[\n\\mathbf{x}^\\top \\mathbf{y}\n=\n\\sum_{i=1}^n x_i y_i.\n\\]\nQuando \\(\\mathbf{x} = \\mathbf{y}\\), obtemos\n\\[\n\\mathbf{x}^\\top \\mathbf{x}\n=\n\\sum_{i=1}^n x_i^2,\n\\]\nque define o quadrado da norma euclidiana:\n\\[\n\\|\\mathbf{x}\\|^2 = \\mathbf{x}^\\top \\mathbf{x}.\n\\]\nEssa noção de norma é central na regressão, pois a estimação por mínimos quadrados consiste em minimizar\n\\[\n\\|\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2.\n\\]\nPortanto, o problema de estimação é um problema geométrico de minimizar distância no espaço \\(\\mathbb{R}^n\\). A formulação geométrica da regressão em termos de subespaços e projeções é desenvolvida em detalhe em Harville (1997).\n\n\n14.1.4 Forma Quadrática\nUma expressão da forma\n\\[\n\\mathbf{x}^\\top \\mathbf{A}\\mathbf{x}\n\\]\né chamada forma quadrática.\nPara visualizar, considere\n\\[\n\\mathbf{x} =\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix},\n\\qquad\n\\mathbf{A} =\n\\begin{bmatrix}\na & b \\\\\nb & c\n\\end{bmatrix}.\n\\]\nEntão\n\\[\n\\mathbf{x}^\\top \\mathbf{A}\\mathbf{x}\n=\na x_1^2 + 2b x_1 x_2 + c x_2^2.\n\\]\nObserve que surgem termos quadráticos e termos mistos. Em regressão, as somas de quadrados explicada e residual podem ser escritas exatamente como formas quadráticas do vetor \\(\\mathbf{Y}\\) (ver Rencher e Christensen (2012)).\n\n\n14.1.5 Transposição\nA transposta de uma matriz é obtida trocando linhas por colunas:\n\\[\n\\mathbf{A} =\n\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\\quad\n\\Rightarrow\n\\quad\n\\mathbf{A}^\\top =\n\\begin{bmatrix}\na_{11} & a_{21} \\\\\na_{12} & a_{22}\n\\end{bmatrix}.\n\\]\nA transposição é essencial para definir produtos internos e garantir que expressões como \\(\\mathbf{X}^\\top\\mathbf{X}\\) sejam matrizes quadradas.\n\n\n14.1.6 Inversão\nUma matriz quadrada \\(\\mathbf{A}\\) é invertível se existe \\(\\mathbf{A}^{-1}\\) tal que\n\\[\n\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}.\n\\]\nPor exemplo, para uma matriz \\(2 \\times 2\\),\n\\[\n\\mathbf{A} =\n\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix},\n\\]\nse \\(ad - bc \\neq 0\\), então\n\\[\n\\mathbf{A}^{-1}\n=\n\\frac{1}{ad-bc}\n\\begin{bmatrix}\nd & -b \\\\\n-c & a\n\\end{bmatrix}.\n\\]\nNa regressão, a invertibilidade de \\(\\mathbf{X}^\\top \\mathbf{X}\\) é condição necessária para a existência do estimador de mínimos quadrados único.Para o caso de posto deficiente e o uso de decomposição SVD e pseudoinversas, ver Golub e Van Loan (2013).\n\n\n14.1.7 Propriedades Importantes\nDuas identidades frequentemente utilizadas são\n\\[\n(\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top \\mathbf{A}^\\top,\n\\qquad\n(\\mathbf{A}^{-1})^\\top = (\\mathbf{A}^\\top)^{-1}.\n\\]\nEssas propriedades são fundamentais na dedução de resultados como:\n\\[\n\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}})\n=\n\\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1},\n\\]\ne na demonstração das propriedades das matrizes de projeção.\nEssas operações constituem o mecanismo estrutural que permitirá:\n\ninterpretar o estimador como projeção ortogonal;\nescrever somas de quadrados como formas quadráticas;\nanalisar variâncias e covariâncias de estimadores;\ncompreender a geometria do ajuste e do diagnóstico.\n\nNos itens seguintes, essas operações serão organizadas dentro da estrutura específica do modelo linear múltiplo.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estrutura Matricial dos Modelos de Regressão Linear</span>"
    ]
  },
  {
    "objectID": "ap_matrizes.html#estruturas-matriciais-relevantes",
    "href": "ap_matrizes.html#estruturas-matriciais-relevantes",
    "title": "14  Estrutura Matricial dos Modelos de Regressão Linear",
    "section": "14.2 Estruturas Matriciais Relevantes",
    "text": "14.2 Estruturas Matriciais Relevantes\nDeterminados tipos de matrizes surgem de forma recorrente na teoria da regressão linear. Cada uma delas corresponde a uma propriedade geométrica ou estatística que será explorada na estimação, na inferência e na análise de diagnóstico.\nA tabela a seguir resume algumas dessas estruturas fundamentais.\n\n\n\n\n\n\n\n\nTipo\nDefinição\nRelevância\n\n\n\n\nIdentidade \\(\\mathbf{I}_n\\)\nDiagonal principal composta por 1’s\nElemento neutro da multiplicação\n\n\nSimétrica\n\\(\\mathbf{A} = \\mathbf{A}^\\top\\)\nAutovalores reais\n\n\nIdempotente\n\\(\\mathbf{A}^2 = \\mathbf{A}\\)\nProjeções\n\n\nOrtogonal\n\\(\\mathbf{A}^\\top \\mathbf{A} = \\mathbf{I}\\)\nPreserva norma\n\n\nDiagonal\nElementos fora da diagonal iguais a zero\nSimplifica formas quadráticas\n\n\nDefinida positiva\n\\(\\mathbf{x}^\\top \\mathbf{A}\\mathbf{x} &gt; 0\\) para todo \\(\\mathbf{x}\\neq 0\\)\nGarantia de invertibilidade e convexidade\n\n\n\nA seguir, detalham-se as propriedades e implicações dessas estruturas no contexto do modelo linear.\n\n14.2.1 Matriz Identidade\nA matriz identidade de ordem \\(n\\) é dada por\n\\[\n\\mathbf{I}_n =\n\\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}.\n\\]\nEla satisfaz\n\\[\n\\mathbf{I}_n \\mathbf{x} = \\mathbf{x}\n\\quad\n\\text{para todo } \\mathbf{x} \\in \\mathbb{R}^n.\n\\]\nNo modelo linear, a identidade aparece, por exemplo, na matriz de covariância dos erros sob homocedasticidade:\n\\[\n\\mathrm{Cov}(\\boldsymbol{\\varepsilon}) = \\sigma^2 \\mathbf{I}_n.\n\\]\nSob normalidade, essa estrutura implica independência e variância constante dos erros.\n\n\n14.2.2 Matrizes Simétricas\nUma matriz é simétrica se\n\\[\n\\mathbf{A} = \\mathbf{A}^\\top.\n\\]\nPor exemplo,\n\\[\n\\mathbf{A} =\n\\begin{bmatrix}\n2 & -1 \\\\\n-1 & 3\n\\end{bmatrix}\n\\]\né simétrica.\nMatrizes simétricas possuem autovalores reais e podem ser diagonalizadas por matrizes ortogonais. Essa propriedade é crucial para compreender a decomposição espectral de \\(\\mathbf{X}^\\top \\mathbf{X}\\) e analisar problemas como multicolinearidade (ver Harville (1997)).\nNo modelo linear, as matrizes \\(\\mathbf{X}^\\top \\mathbf{X}\\), \\(\\mathbf{H}\\) e \\(\\mathbf{M}\\) são simétricas.\n\n\n14.2.3 Matrizes Idempotentes\nUma matriz é idempotente se\n\\[\n\\mathbf{A}^2 = \\mathbf{A}.\n\\]\nIsso implica que aplicar a transformação duas vezes produz o mesmo resultado que aplicá-la uma vez.\nPor exemplo, a matriz\n\\[\n\\mathbf{P} =\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n\\]\né idempotente.\nNo modelo linear, a matriz de projeção\n\\[\n\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\n\\]\nsatisfaz\n\\[\n\\mathbf{H}^2 = \\mathbf{H}.\n\\]\nIsso significa que \\(\\mathbf{H}\\) projeta vetores no subespaço \\(\\mathrm{col}(\\mathbf{X})\\). Uma vez projetado, aplicar novamente a projeção não altera o vetor.\nEssa propriedade é fundamental para compreender:\n\ndecomposição ortogonal;\nindependência entre componentes projetadas sob normalidade;\ndecomposição da soma de quadrados total.\n\n\n\n14.2.4 Matrizes Ortogonais\nUma matriz \\(\\mathbf{Q}\\) é ortogonal se\n\\[\n\\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{I}.\n\\]\nIsso implica que\n\\[\n\\|\\mathbf{Q}\\mathbf{x}\\| = \\|\\mathbf{x}\\|.\n\\]\nOu seja, matrizes ortogonais preservam comprimentos e ângulos.\nEssa propriedade é central na decomposição espectral de matrizes simétricas:\n\\[\n\\mathbf{A} = \\mathbf{Q}\\boldsymbol{\\Lambda}\\mathbf{Q}^\\top,\n\\]\nem que \\(\\boldsymbol{\\Lambda}\\) é diagonal contendo os autovalores.A análise numérica dessas decomposições é tratada em profundidade em Golub e Van Loan (2013).\nNa regressão, essa decomposição permite analisar:\n\na estrutura de \\(\\mathbf{X}^\\top \\mathbf{X}\\);\na estabilidade numérica da estimação;\no efeito da multicolinearidade.\n\n\n\n14.2.5 Matrizes Diagonais\nUma matriz diagonal possui a forma\n\\[\n\\mathbf{D} =\n\\begin{bmatrix}\nd_1 & 0 & \\cdots & 0 \\\\\n0 & d_2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & d_n\n\\end{bmatrix}.\n\\]\nFormas quadráticas envolvendo matrizes diagonais simplificam-se para\n\\[\n\\mathbf{x}^\\top \\mathbf{D}\\mathbf{x}\n=\n\\sum_{i=1}^n d_i x_i^2.\n\\]\nEssa simplificação é útil na análise de variâncias e na interpretação de decomposições espectrais.\n\n\n14.2.6 Matrizes Definidas Positivas\nUma matriz simétrica \\(\\mathbf{A}\\) é definida positiva se\n\\[\n\\mathbf{x}^\\top \\mathbf{A}\\mathbf{x} &gt; 0\n\\quad\n\\text{para todo } \\mathbf{x} \\neq \\mathbf{0}.\n\\]\nEquivalentemente, todos os seus autovalores são positivos.\nEssa propriedade possui consequências fundamentais:\n\n\\(\\mathbf{A}\\) é invertível;\na função \\(\\mathbf{x}^\\top \\mathbf{A}\\mathbf{x}\\) é estritamente convexa;\nproblemas de minimização associados possuem solução única.\n\nNo modelo de regressão linear, a matriz\n\\[\n\\mathbf{X}^\\top \\mathbf{X}\n\\]\né simétrica e definida positiva se, e somente se, as colunas de \\(\\mathbf{X}\\) forem linearmente independentes. Isso equivale à condição\n\\[\n\\operatorname{rank}(\\mathbf{X}) = p+1.\n\\]\nQuando essa condição é satisfeita, o estimador de mínimos quadrados\n\\[\n\\hat{\\boldsymbol{\\beta}}\n=\n(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}\n\\]\nexiste e é único.\nSe \\(\\mathbf{X}^\\top \\mathbf{X}\\) não for definida positiva, o problema de estimação perde identificabilidade, caracterizando multicolinearidade perfeita.\nA compreensão dessas estruturas matriciais permite interpretar o modelo linear como:\n\num problema geométrico de projeção em subespaços;\num problema analítico de minimização convexa;\num problema espectral envolvendo autovalores e autovetores.\n\nEssas perspectivas convergem na teoria de estimação, inferência e diagnóstico.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estrutura Matricial dos Modelos de Regressão Linear</span>"
    ]
  },
  {
    "objectID": "ap_matrizes.html#estrutura-geométrica-do-modelo-linear",
    "href": "ap_matrizes.html#estrutura-geométrica-do-modelo-linear",
    "title": "14  Estrutura Matricial dos Modelos de Regressão Linear",
    "section": "14.3 Estrutura Geométrica do Modelo Linear",
    "text": "14.3 Estrutura Geométrica do Modelo Linear\nConsidere o modelo linear múltiplo em notação matricial:\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\n\\]\nem que\n\n\\(\\mathbf{Y} \\in \\mathbb{R}^n\\) é o vetor de respostas observadas,\n\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times (p+1)}\\) é a matriz de planejamento,\n\\(\\boldsymbol{\\beta} \\in \\mathbb{R}^{p+1}\\) é o vetor de parâmetros,\n\\(\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n\\) é o vetor de erros.\n\nExplicitamente, pode-se escrever\n\\[\n\\mathbf{X}\n=\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix},\n\\qquad\n\\boldsymbol{\\beta}\n=\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix}.\n\\]\nCada coluna de \\(\\mathbf{X}\\) é um vetor em \\(\\mathbb{R}^n\\). Assim, \\(\\mathbf{X}\\) pode ser vista como um conjunto de \\(p+1\\) vetores que geram um subespaço de \\(\\mathbb{R}^n\\).\n\n14.3.1 Espaço Coluna e Identificabilidade\nO espaço coluna de \\(\\mathbf{X}\\) é definido como\n\\[\n\\mathrm{col}(\\mathbf{X})\n=\n\\left\\{\n\\mathbf{X}\\boldsymbol{\\beta}\n:\n\\boldsymbol{\\beta} \\in \\mathbb{R}^{p+1}\n\\right\\}.\n\\]\nEsse conjunto é um subespaço vetorial de \\(\\mathbb{R}^n\\), cujo posto é\n\\[\n\\operatorname{rank}(\\mathbf{X}) \\leq p+1.\n\\]\nSe as colunas de \\(\\mathbf{X}\\) forem linearmente independentes, então\n\\[\n\\operatorname{rank}(\\mathbf{X}) = p+1,\n\\]\ne o espaço coluna tem dimensão \\(p+1\\).\nEssa condição é equivalente à positividade definida de \\(\\mathbf{X}^\\top \\mathbf{X}\\) e garante a identificabilidade única dos parâmetros.\n\n\n14.3.2 O Problema de Mínimos Quadrados como Problema de Projeção\nO estimador de mínimos quadrados é definido como a solução do problema\n\\[\n\\min_{\\boldsymbol{\\beta}}\n\\|\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2.\n\\]\nGeometricamente, isso significa encontrar o vetor em \\(\\mathrm{col}(\\mathbf{X})\\) que esteja mais próximo de \\(\\mathbf{Y}\\) na métrica euclidiana.\nSeja\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}.\n\\]\nEntão\n\\[\n\\hat{\\mathbf{Y}} \\in \\mathrm{col}(\\mathbf{X})\n\\]\ne\n\\[\n\\mathbf{Y} - \\hat{\\mathbf{Y}}\n\\perp\n\\mathrm{col}(\\mathbf{X}).\n\\]\nOu seja,\n\\[\n\\mathbf{X}^\\top(\\mathbf{Y} - \\hat{\\mathbf{Y}}) = \\mathbf{0}.\n\\]\nEssa condição é exatamente a forma matricial das equações normais.\n\n\n14.3.3 Interpretação Ortogonal e Soma Direta\nSeja \\(V = \\mathbb{R}^n\\) equipado com o produto interno usual \\[\n\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\mathbf{x}^\\top \\mathbf{y}.\n\\]\nSe \\(S\\) é um subespaço de \\(\\mathbb{R}^n\\), define-se seu complemento ortogonal como\n\\[\nS^\\perp\n=\n\\left\\{\n\\mathbf{z} \\in \\mathbb{R}^n :\n\\mathbf{z}^\\top \\mathbf{s} = 0\n\\ \\text{para todo } \\mathbf{s} \\in S\n\\right\\}.\n\\]\nDiz-se que \\(\\mathbb{R}^n\\) é a soma direta ortogonal de dois subespaços \\(S\\) e \\(T\\) se:\n\ntodo vetor de \\(\\mathbb{R}^n\\) pode ser escrito como soma de um vetor em \\(S\\) e um vetor em \\(T\\);\nessa decomposição é única;\n\\(S\\) e \\(T\\) são ortogonais, isto é, \\(\\mathbf{s}^\\top \\mathbf{t} = 0\\) para todo \\(\\mathbf{s}\\in S\\) e \\(\\mathbf{t}\\in T\\).\n\nNessa situação escreve-se\n\\[\n\\mathbb{R}^n = S \\oplus T,\n\\]\nem que o símbolo \\(\\oplus\\) indica soma direta.\nNo contexto do modelo linear, tomando\n\\[\nS = \\mathrm{col}(\\mathbf{X}),\n\\qquad\nT = \\mathrm{col}(\\mathbf{X})^\\perp,\n\\]\nobtém-se\n\\[\n\\mathbb{R}^n\n=\n\\mathrm{col}(\\mathbf{X})\n\\oplus\n\\mathrm{col}(\\mathbf{X})^\\perp.\n\\]\nIsso significa que qualquer vetor \\(\\mathbf{Y} \\in \\mathbb{R}^n\\) pode ser decomposto de maneira única como\n\\[\n\\mathbf{Y}\n=\n\\hat{\\mathbf{Y}}\n+\n\\hat{\\boldsymbol{\\varepsilon}},\n\\]\nonde\n\n\\(\\hat{\\mathbf{Y}} \\in \\mathrm{col}(\\mathbf{X})\\),\n\\(\\hat{\\boldsymbol{\\varepsilon}} \\in \\mathrm{col}(\\mathbf{X})^\\perp\\).\n\nA ortogonalidade implica\n\\[\n\\hat{\\mathbf{Y}}^\\top \\hat{\\boldsymbol{\\varepsilon}} = 0,\n\\]\nou, equivalentemente,\n\\[\n\\mathbf{X}^\\top \\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{0}.\n\\]\nEssa decomposição é puramente geométrica e independe de qualquer hipótese probabilística sobre os erros. Ela constitui o núcleo estrutural do método de mínimos quadrados e fundamenta:\n\na decomposição da soma de quadrados total;\na contagem de graus de liberdade;\na independência entre componentes projetadas sob normalidade.\n\nAssim, o modelo linear pode ser interpretado como a decomposição ortogonal do vetor de respostas em duas componentes pertencentes a subespaços complementares.\n\n\n14.3.4 Relação com Posto e Dimensão\nSe \\(\\operatorname{rank}(\\mathbf{X}) = r\\), então:\n\n\\(\\dim(\\mathrm{col}(\\mathbf{X})) = r\\),\n\\(\\dim(\\mathrm{col}(\\mathbf{X})^\\perp) = n - r\\).\n\nNo modelo linear completo com intercepto e colunas independentes,\n\\[\nr = p+1,\n\\]\ne, portanto, o espaço residual tem dimensão\n\\[\nn - (p+1).\n\\]\nEssa contagem de dimensões será reinterpretada mais adiante como graus de liberdade na decomposição das somas de quadrados.\n\n\n14.3.5 Conexão com Diagnóstico\nA estrutura geométrica permite compreender diversos elementos de diagnóstico:\n\nVetores de alta alavancagem correspondem a observações cuja projeção sobre \\(\\mathrm{col}(\\mathbf{X})\\) é dominante.\nResíduos grandes correspondem a componentes significativas no subespaço ortogonal.\nA decomposição da variabilidade total decorre da ortogonalidade entre componentes projetadas.\n\nO modelo linear é, portato, uma decomposição geométrica do vetor de respostas em dois componentes ortogonais.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estrutura Matricial dos Modelos de Regressão Linear</span>"
    ]
  },
  {
    "objectID": "ap_matrizes.html#matrizes-de-projeção-e-decomposição-ortogonal",
    "href": "ap_matrizes.html#matrizes-de-projeção-e-decomposição-ortogonal",
    "title": "14  Estrutura Matricial dos Modelos de Regressão Linear",
    "section": "14.4 Matrizes de Projeção e Decomposição Ortogonal",
    "text": "14.4 Matrizes de Projeção e Decomposição Ortogonal\nA matriz de projeção associada ao modelo linear múltiplo é definida por\n\\[\n\\mathbf{H}\n=\n\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top.\n\\]\nEssa matriz desempenha papel relevante na teoria da regressão, pois formaliza algebricamente a projeção ortogonal sobre o subespaço \\(\\mathrm{col}(\\mathbf{X})\\).\n\n14.4.0.1 Verificação das Propriedades Estruturais\nSimetria\n\\[\n\\mathbf{H}^\\top\n=\n\\left[\n\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\n\\right]^\\top\n=\n\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\n=\n\\mathbf{H}.\n\\]\nUtilizou-se o fato de que \\(\\mathbf{X}^\\top \\mathbf{X}\\) é simétrica e que a transposta de um produto inverte a ordem dos fatores.\nIdempotência\n\\[\n\\mathbf{H}^2\n=\n\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\n\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top.\n\\]\nComo\n\\[\n\\mathbf{X}^\\top \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\n=\n\\mathbf{I},\n\\]\nsegue que\n\\[\n\\mathbf{H}^2\n=\n\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\n=\n\\mathbf{H}.\n\\]\nA idempotência caracteriza transformações que, uma vez aplicadas, não alteram mais o vetor.\n\n\n14.4.0.2 Interpretação Geométrica\nPara qualquer vetor \\(\\mathbf{y} \\in \\mathbb{R}^n\\),\n\\[\n\\mathbf{H}\\mathbf{y}\n\\in\n\\mathrm{col}(\\mathbf{X}).\n\\]\nAlém disso,\n\\[\n\\mathbf{y} - \\mathbf{H}\\mathbf{y}\n\\in\n\\mathrm{col}(\\mathbf{X})^\\perp.\n\\]\nPortanto,\n\\[\n\\mathbf{y}\n=\n\\mathbf{H}\\mathbf{y}\n+\n(\\mathbf{I}-\\mathbf{H})\\mathbf{y}.\n\\]\nDefinindo\n\\[\n\\mathbf{M} = \\mathbf{I}_n - \\mathbf{H},\n\\]\nobtém-se a projeção complementar sobre o subespaço ortogonal.\n\n\n14.4.0.3 Propriedades da Matriz Residual\nA matriz\n\\[\n\\mathbf{M} = \\mathbf{I}_n - \\mathbf{H}\n\\]\nsatisfaz:\n\n\\(\\mathbf{M}^\\top = \\mathbf{M}\\),\n\\(\\mathbf{M}^2 = \\mathbf{M}\\),\n\\(\\mathbf{H}\\mathbf{M} = \\mathbf{0}\\),\n\\(\\mathbf{M}\\mathbf{H} = \\mathbf{0}\\).\n\nEssas propriedades garantem que os subespaços são ortogonais e complementares.\n\n\n14.4.0.4 Decomposição do Vetor de Respostas\nAplicando as matrizes ao vetor \\(\\mathbf{Y}\\):\n\\[\n\\mathbf{Y}\n=\n\\mathbf{H}\\mathbf{Y}\n+\n\\mathbf{M}\\mathbf{Y}\n=\n\\hat{\\mathbf{Y}}\n+\n\\hat{\\boldsymbol{\\varepsilon}}.\n\\]\nem que\n\n\\(\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\\) pertence ao espaço coluna;\n\\(\\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{M}\\mathbf{Y}\\) pertence ao complemento ortogonal.\n\nA ortogonalidade implica\n\\[\n\\hat{\\mathbf{Y}}^\\top \\hat{\\boldsymbol{\\varepsilon}} = 0.\n\\]\nEssa identidade é a base da decomposição da soma de quadrados total.\n\n\n14.4.0.5 Traço, Posto e Autovalores\nPara matrizes idempotentes e simétricas, os autovalores são apenas 0 ou 1.\nSe\n\\[\n\\operatorname{rank}(\\mathbf{X}) = p+1,\n\\]\nentão:\n\n\\(\\mathbf{H}\\) possui \\(p+1\\) autovalores iguais a 1,\ne \\(n-(p+1)\\) autovalores iguais a 0.\n\nAssim,\n\\[\n\\mathrm{tr}(\\mathbf{H}) = p+1.\n\\]\nDe forma análoga,\n\\[\n\\mathrm{tr}(\\mathbf{M}) = n - p - 1.\n\\]\nComo o traço de uma matriz idempotente simétrica coincide com seu posto, essas quantidades correspondem às dimensões dos subespaços projetados (ver Harville (1997)).\n\n\n14.4.0.6 Conexão com Somas de Quadrados\nA soma de quadrados ajustada pode ser escrita como\n\\[\n\\mathbf{Y}^\\top \\mathbf{H} \\mathbf{Y}.\n\\]\nA soma de quadrados residual é\n\\[\n\\mathbf{Y}^\\top \\mathbf{M} \\mathbf{Y}.\n\\]\nComo \\(\\mathbf{H}\\) e \\(\\mathbf{M}\\) projetam sobre subespaços ortogonais,\n\\[\n\\mathbf{Y}^\\top \\mathbf{Y}\n=\n\\mathbf{Y}^\\top \\mathbf{H} \\mathbf{Y}\n+\n\\mathbf{Y}^\\top \\mathbf{M} \\mathbf{Y}.\n\\]\nEssa identidade é puramente geométrica e antecede qualquer consideração probabilística.\n\n\n14.4.0.7 Relação com Diagnóstico\nA diagonal de \\(\\mathbf{H}\\) contém as alavancagens:\n\\[\nh_{ii} = (\\mathbf{H})_{ii}.\n\\]\nEssas quantidades medem o grau de influência estrutural da \\(i\\)-ésima observação no ajuste, pois determinam o peso da projeção sobre o espaço coluna (ver Weisberg (2005)).\nValores elevados de \\(h_{ii}\\) indicam observações que ocupam posições extremas no espaço das covariáveis.\nA matriz de projeção sintetiza, portanto, três dimensões fundamentais do modelo linear:\n\nEstrutura geométrica (projeção ortogonal);\nEstrutura algébrica (idempotência e posto);\nEstrutura estatística (somas de quadrados e graus de liberdade).",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estrutura Matricial dos Modelos de Regressão Linear</span>"
    ]
  },
  {
    "objectID": "ap_matrizes.html#diferenciação-matricial-e-estimação-por-mínimos-quadrados",
    "href": "ap_matrizes.html#diferenciação-matricial-e-estimação-por-mínimos-quadrados",
    "title": "14  Estrutura Matricial dos Modelos de Regressão Linear",
    "section": "14.5 Diferenciação Matricial e Estimação por Mínimos Quadrados",
    "text": "14.5 Diferenciação Matricial e Estimação por Mínimos Quadrados\nA estimação por mínimos quadrados consiste na minimização da soma de quadrados dos resíduos,\n\\[\nS(\\boldsymbol{\\beta})\n=\n\\|\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2\n=\n(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top\n(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}).\n\\]\nEssa função é uma forma quadrática em \\(\\boldsymbol{\\beta}\\). Para compreender sua estrutura, é útil expandi-la explicitamente:\n\\[\nS(\\boldsymbol{\\beta})\n=\n\\mathbf{Y}^\\top \\mathbf{Y}\n-\n2\\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{Y}\n+\n\\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta}.\n\\]\nObserva-se que:\n\n\\(\\mathbf{Y}^\\top \\mathbf{Y}\\) é constante em relação a \\(\\boldsymbol{\\beta}\\);\n\\(\\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{Y}\\) é termo linear;\n\\(\\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta}\\) é forma quadrática.\n\nA função objetivo é, portanto, um polinômio quadrático convexo sempre que \\(\\mathbf{X}^\\top \\mathbf{X}\\) for definida positiva.\n\n14.5.1 Derivadas Matriciais Fundamentais\nAs identidades de cálculo matricial utilizadas a seguir são sistematizadas em Abadir e Magnus (2005).\n\n\\[\n\\frac{\\partial (\\mathbf{a}^\\top \\mathbf{x})}{\\partial \\mathbf{x}}\n=\n\\mathbf{a}\n\\]\n\\[\n\\frac{\\partial (\\mathbf{x}^\\top \\mathbf{A}\\mathbf{x})}{\\partial \\mathbf{x}}\n=\n(\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}\n\\]\n\nEm particular, se \\(\\mathbf{A}\\) é simétrica,\n\\[\n\\frac{\\partial (\\mathbf{x}^\\top \\mathbf{A}\\mathbf{x})}{\\partial \\mathbf{x}}\n=\n2\\mathbf{A}\\mathbf{x}.\n\\]\n\n\\[\n\\frac{\\partial (\\mathbf{x}^\\top \\mathbf{A}\\mathbf{b})}{\\partial \\mathbf{x}}\n=\n\\mathbf{A}^\\top \\mathbf{b}\n\\]\n\\[\n\\frac{\\partial \\mathrm{tr}(\\mathbf{A}\\mathbf{X})}{\\partial \\mathbf{X}}\n=\n\\mathbf{A}^\\top\n\\]\n\\[\n\\frac{\\partial\n(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top\n(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta})\n}\n{\\partial \\boldsymbol{\\beta}}\n=\n-2\\mathbf{X}^\\top(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}).\n\\]\n\n\n\n14.5.2 Derivação das Equações Normais\nAplicando a derivada à função \\(S(\\boldsymbol{\\beta})\\) (ver Abadir e Magnus (2005)):\n\\[\n\\nabla_{\\boldsymbol{\\beta}} S(\\boldsymbol{\\beta})\n=\n-2\\mathbf{X}^\\top(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}).\n\\]\nIgualando o gradiente a zero:\n\\[\n\\mathbf{X}^\\top(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0.\n\\]\nReorganizando,\n\\[\n\\mathbf{X}^\\top \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n=\n\\mathbf{X}^\\top \\mathbf{Y}.\n\\]\nEssas são as equações normais.\nSe \\(\\mathbf{X}^\\top \\mathbf{X}\\) é invertível, isto é, se as colunas de \\(\\mathbf{X}\\) são linearmente independentes, obtém-se a solução única:\n\\[\n\\hat{\\boldsymbol{\\beta}}\n=\n(\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\mathbf{X}^\\top \\mathbf{Y}.\n\\]\n\n\n14.5.3 Interpretação Algébrica\nA condição\n\\[\n\\mathbf{X}^\\top(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})=0\n\\]\nimplica\n\\[\n\\mathbf{X}^\\top \\hat{\\boldsymbol{\\varepsilon}} = 0,\n\\]\nou seja, o vetor residual é ortogonal a cada coluna de \\(\\mathbf{X}\\).\n\n\n14.5.4 Interpretação Geométrica\nA minimização de \\(S(\\boldsymbol{\\beta})\\) equivale a resolver\n\\[\n\\min_{\\mathbf{v} \\in \\mathrm{col}(\\mathbf{X})}\n\\|\\mathbf{Y} - \\mathbf{v}\\|^2.\n\\]\nPortanto,\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\]\né a projeção ortogonal de \\(\\mathbf{Y}\\) sobre \\(\\mathrm{col}(\\mathbf{X})\\). Essa caracterização independe de qualquer hipótese probabilística.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estrutura Matricial dos Modelos de Regressão Linear</span>"
    ]
  },
  {
    "objectID": "ap_matrizes.html#estrutura-matricial-para-diagnóstico-e-inferência",
    "href": "ap_matrizes.html#estrutura-matricial-para-diagnóstico-e-inferência",
    "title": "14  Estrutura Matricial dos Modelos de Regressão Linear",
    "section": "14.6 Estrutura Matricial para Diagnóstico e Inferência",
    "text": "14.6 Estrutura Matricial para Diagnóstico e Inferência\nA formulação matricial do modelo linear não se limita à obtenção do estimador de mínimos quadrados. Ela estrutura integralmente os procedimentos de diagnóstico e prepara o terreno para a inferência estatística.\nRecordemos a decomposição fundamental:\n\\[\n\\mathbf{Y}\n=\n\\mathbf{H}\\mathbf{Y}\n+\n\\mathbf{M}\\mathbf{Y}\n=\n\\hat{\\mathbf{Y}}\n+\n\\hat{\\boldsymbol{\\varepsilon}}.\n\\]\nEssa identidade organiza o vetor de respostas em duas componentes ortogonais pertencentes a subespaços complementares.\n\n14.6.1 Alavancagem e Estrutura da Matriz \\(\\mathbf{H}\\)\nA diagonal da matriz de projeção\n\\[\nh_{ii} = (\\mathbf{H})_{ii}\n\\]\nmensura o quanto a \\(i\\)-ésima observação contribui estruturalmente para sua própria projeção.\nExplicitamente,\n\\[\nh_{ii}\n=\n\\mathbf{x}_i^\\top\n(\\mathbf{X}^\\top \\mathbf{X})^{-1}\n\\mathbf{x}_i,\n\\]\nem que \\(\\mathbf{x}_i^\\top\\) é a \\(i\\)-ésima linha da matriz \\(\\mathbf{X}\\).\nPropriedades fundamentais:\n\n\\(0 \\le h_{ii} \\le 1\\);\n\\(\\sum_{i=1}^n h_{ii} = p+1\\);\nobservações com valores elevados de \\(h_{ii}\\) ocupam posições extremas no espaço das covariáveis.\n\n\n\n14.6.2 Estrutura dos Resíduos\nOs resíduos podem ser escritos como\n\\[\n\\hat{\\boldsymbol{\\varepsilon}}\n=\n\\mathbf{M}\\mathbf{Y}.\n\\]\nComo \\(\\mathbf{M}\\) é simétrica e idempotente,\n\\[\n\\mathbf{M}^2 = \\mathbf{M},\n\\qquad\n\\mathbf{M}^\\top = \\mathbf{M}.\n\\]\nAlém disso,\n\\[\n\\mathbf{X}^\\top \\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{0},\n\\]\no que garante que os resíduos são ortogonais às colunas de \\(\\mathbf{X}\\).\nEssa ortogonalidade fundamenta:\n\na decomposição das somas de quadrados;\na independência geométrica entre componentes ajustadas e residuais;\na contagem de graus de liberdade.\n\n\n\n14.6.3 Somas de Quadrados em Forma Matricial\nA soma de quadrados total pode ser escrita como\n\\[\n\\mathbf{Y}^\\top \\mathbf{Y}.\n\\]\nA soma de quadrados explicada pelo modelo é\n\\[\n\\mathbf{Y}^\\top \\mathbf{H} \\mathbf{Y}.\n\\]\nA soma de quadrados residual é\n\\[\n\\mathbf{Y}^\\top \\mathbf{M} \\mathbf{Y}.\n\\]\nComo \\(\\mathbf{H}\\) e \\(\\mathbf{M}\\) projetam sobre subespaços ortogonais,\n\\[\n\\mathbf{Y}^\\top \\mathbf{Y}\n=\n\\mathbf{Y}^\\top \\mathbf{H} \\mathbf{Y}\n+\n\\mathbf{Y}^\\top \\mathbf{M} \\mathbf{Y}.\n\\]\nEssa identidade é puramente algébrica e independe de qualquer suposição probabilística.\n\n\n14.6.4 Postos e Graus de Liberdade\nComo visto anteriormente,\n\\[\n\\mathrm{tr}(\\mathbf{H}) = p+1,\n\\qquad\n\\mathrm{tr}(\\mathbf{M}) = n - p - 1.\n\\]\nPara matrizes simétricas idempotentes, o traço coincide com o posto. Portanto:\n\no subespaço ajustado tem dimensão \\(p+1\\);\no subespaço residual tem dimensão \\(n-p-1\\).\n\nEssas dimensões serão reinterpretadas, no contexto probabilístico, como graus de liberdade associados às somas de quadrados.\n\n\n\n\nAbadir, Karim M., e Jan R. Magnus. 2005. Matrix Algebra. Cambridge: Cambridge University Press.\n\n\nGolub, Gene H., e Charles F. Van Loan. 2013. Matrix Computations. 4º ed. Baltimore: Johns Hopkins University Press.\n\n\nHarville, David A. 1997. Matrix Algebra From a Statistician’s Perspective. New York: Springer.\n\n\nRencher, Alvin C., e William F. Christensen. 2012. Methods of Multivariate Analysis. 3º ed. Hoboken: Wiley.\n\n\nWeisberg, Sanford. 2005. Applied Linear Regression. New York: Wiley.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Estrutura Matricial dos Modelos de Regressão Linear</span>"
    ]
  },
  {
    "objectID": "ap_normal.html",
    "href": "ap_normal.html",
    "title": "15  Distribuição Normal",
    "section": "",
    "text": "15.1 Distribuição Normal Univariada\nEste apêndice tem um papel estrutural na fundamentação matemática dos modelos de regressão linear. A Distribuição Normal, especialmente em sua forma multivariada, fornece a base probabilística que torna possível derivar distribuições amostrais exatas para estimadores, contrastes lineares e estatísticas de teste em amostras finitas. Uma exposição formal e sistemática dessas propriedades pode ser encontrada em Anderson (2003) e Casella e Berger (2002).\nA ideia central que deve acompanhar o leitor ao longo deste apêndice é a seguinte:\nEssa perspectiva vetorial não é apenas notacional. Ela altera profundamente a forma de pensar sobre variabilidade, dependência e inferência.\nUma variável aleatória \\(Y\\) tem distribuição Normal univariada com média \\(\\mu \\in \\mathbb{R}\\) e variância \\(\\sigma^2 &gt; 0\\) se sua função densidade (fpd) é\n\\[\nf(y; \\mu, \\sigma^2)\n=\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\exp\\!\\left(\n-\\frac{(y - \\mu)^2}{2\\sigma^2}\n\\right),\n\\quad y \\in \\mathbb{R}.\n\\] Essa distribuição surge com frequência em modelagem estatística porque aparece como distribuição limite em muitos contextos, especialmente quando uma quantidade observada pode ser representada como a soma (ou média) de um grande número de contribuições aleatórias.\nO Teorema Central do Limite estabelece que, sob condições adequadas, a soma devidamente padronizada de variáveis aleatórias independentes, ou fracamente dependentes, converge em distribuição para a Normal, independentemente da forma das distribuições individuais; essa fundamentação é essencialmente assintótica e aproximada, não constituindo uma identidade estrutural exata. Uma formulação rigorosa desse resultado pode ser consultada em Casella e Berger (2002).\nNo contexto de modelos de regressão, a suposição de Normalidade não é obrigatória nem define o modelo em si. Um modelo de regressão linear pode ser formulado sem qualquer hipótese distributiva explícita sobre o erro, bastando condições sobre esperança, variância e independência.\nA Normalidade é frequentemente adotada porque constitui o caso mais simples e matematicamente tratável, permitindo obter distribuições exatas para estimadores, estatísticas de teste e intervalos de confiança em amostras finitas. Em outros contextos, distribuições alternativas podem ser mais adequadas, levando a extensões naturais da regressão linear, como os modelos lineares generalizados.\nOs parâmetros da Normal univariada admitem interpretações diretas, mas é importante compreendê-las com precisão estatística.\nO parâmetro \\(\\mu\\) representa o valor esperado teórico da variável aleatória \\(Y\\), isto é, o ponto em torno do qual a distribuição se concentra em média. Trata-se de uma quantidade populacional, definida independentemente de qualquer amostra específica, e que resume a tendência central do fenômeno sob o modelo probabilístico adotado.\nO parâmetro \\(\\sigma^2\\) representa a variância populacional da variável aleatória, quantificando a dispersão em torno de \\(\\mu\\). Essa variabilidade reflete a incerteza inerente ao fenômeno modelado e não carrega, nesse estágio, qualquer interpretação ligada a explicação ou não explicação por covariáveis. Essa distinção só surgirá no contexto de modelos condicionais, como a regressão.\nEssas interpretações ficam claras ao observarmos duas propriedades fundamentais da distribuição Normal:\nEssas igualdades não são meras convenções, mas decorrem da integração direta da densidade.\nUma característica estrutural importante da Normal é sua estabilidade por transformações lineares. Se \\(Y \\sim N(\\mu,\\sigma^2)\\) e definimos\n\\[\nZ = aY + b,\n\\]\ncom \\(a \\neq 0\\), então\n\\[\nZ \\sim N(a\\mu + b, a^2\\sigma^2).\n\\]\nEssa propriedade, demonstrada em Casella e Berger (2002), é o primeiro indício da importância da Normal na regressão: combinações lineares preservam a forma distributiva.\nUma transformação particularmente importante, tanto do ponto de vista teórico quanto prático, é a padronização. Definindo\n\\[\nZ = \\frac{Y - \\mu}{\\sigma},\n\\]\nobtém-se uma nova variável aleatória com distribuição\n\\[\nZ \\sim N(0,1),\n\\]\nconhecida como Normal padrão.\nA padronização desempenha um papel central em inferência estatística porque remove as unidades de medida e a escala original da variável, permitindo comparar desvios em termos relativos. Em modelos de regressão, essa ideia reaparece de forma sistemática: estatísticas de teste, resíduos padronizados e intervalos de confiança são construídos a partir de quantidades que mensuram desvios em relação a uma média teórica, expressos em unidades de desvio-padrão.\nAssim, compreender profundamente o significado de \\(\\mu\\), \\(\\sigma^2\\) e da padronização é essencial para interpretar corretamente os resultados inferenciais que surgirão nos modelos de regressão.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Distribuição Normal</span>"
    ]
  },
  {
    "objectID": "ap_normal.html#distribuição-normal-univariada",
    "href": "ap_normal.html#distribuição-normal-univariada",
    "title": "15  Distribuição Normal",
    "section": "",
    "text": "Esperança: \\[\n\\mathbb{E}[Y] = \\mu\n\\]\nVariância: \\[\n\\mathrm{Var}(Y) = \\sigma^2\n\\]",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Distribuição Normal</span>"
    ]
  },
  {
    "objectID": "ap_normal.html#distribuição-normal-bivariada",
    "href": "ap_normal.html#distribuição-normal-bivariada",
    "title": "15  Distribuição Normal",
    "section": "15.2 Distribuição Normal Bivariada",
    "text": "15.2 Distribuição Normal Bivariada\nAo avançarmos para o caso bivariado, deixamos de estudar variáveis aleatórias isoladas e passamos a lidar explicitamente com dependência entre variáveis aleatórias. Esse é um passo conceitual fundamental, pois modelos estatísticos mais complexos e abrangentes, incluindo os modelos de regressão, são construídos exatamente a partir de relações entre variáveis.\nConsidere o vetor aleatório \\[\n\\mathbf{Y} = (Y_1, Y_2)^\\top.\n\\]\nDizemos que \\(\\mathbf{Y}\\) segue uma Distribuição Normal bivariada se\n\\[\n\\mathbf{Y} \\sim N_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}),\n\\]\nonde o vetor de médias é dado por\n\\[\n\\boldsymbol{\\mu} =\n\\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2\n\\end{bmatrix},\n\\]\ne a matriz de covariância é\n\\[\n\\boldsymbol{\\Sigma} =\n\\begin{bmatrix}\n\\sigma_1^2 & \\rho\\,\\sigma_1\\sigma_2 \\\\\n\\rho\\,\\sigma_1\\sigma_2 & \\sigma_2^2\n\\end{bmatrix}.\n\\]\nNeste ponto ocorre uma mudança conceitual importante. Enquanto no caso univariado a variância era um único número, agora a matriz \\(\\boldsymbol{\\Sigma}\\) passa a concentrar toda a informação sobre dispersão e dependência:\n\nos termos da diagonal (\\(\\sigma_1^2\\) e \\(\\sigma_2^2\\)) descrevem a variabilidade individual de cada componente;\nos termos fora da diagonal descrevem a associação linear entre as variáveis, resumida pelo coeficiente de correlação \\(\\rho\\).\n\nAssim, a estrutura de dependência entre \\(Y_1\\) e \\(Y_2\\) não é um elemento acessório, mas parte integrante da própria definição da distribuição conjunta.\nA função densidade de probabilidade conjunta, como apresentado em Anderson (2003), pode ser escrita de forma compacta como\n\\[\nf(\\mathbf{y})\n=\n\\frac{1}{2\\pi |\\boldsymbol{\\Sigma}|^{1/2}}\n\\exp\\!\\left\\{\n-\\frac{1}{2}\n(\\mathbf{y} - \\boldsymbol{\\mu})^\\top\n\\boldsymbol{\\Sigma}^{-1}\n(\\mathbf{y} - \\boldsymbol{\\mu})\n\\right\\}.\n\\]\nEssa expressão merece uma leitura cuidadosa. O termo que aparece no expoente, \\[\n(\\mathbf{y} - \\boldsymbol{\\mu})^\\top\n\\boldsymbol{\\Sigma}^{-1}\n(\\mathbf{y} - \\boldsymbol{\\mu}),\n\\] é um escalar obtido a partir de vetores e matrizes, resultado de uma operação que combina transposição, multiplicação matricial e produto interno.\nNeste momento, não é necessário compreender formalmente esse termo como uma “forma quadrática” essa noção será estudada com cuidado em um apêndice específico.\nIntuitivamente, essa quantidade mede quão distante o vetor \\(\\mathbf{y}\\) está do centro \\(\\boldsymbol{\\mu}\\), mas não usando a distância euclidiana usual. Em vez disso, a distância é avaliada levando em conta a estrutura de variabilidade e dependência entre as componentes do vetor, codificada na matriz de covariância \\(\\boldsymbol{\\Sigma}\\).\nDessa forma, desvios ao longo de direções em que há maior variabilidade conjunta são penalizados de maneira diferente de desvios ao longo de direções com menor variabilidade. É essa ponderação que faz com que a distribuição apresente contornos elípticos, em vez de circulares.\nA formalização matemática desse tipo de expressão, bem como seu papel central na regressão, nas somas de quadrados e nas estatísticas de teste, será apresentada posteriormente, quando estudarmos explicitamente as distribuições associadas a expressões desse tipo.\nGeometricamente, isso se traduz no fato de que as curvas de mesma densidade dessa distribuição são elipses centradas em \\(\\boldsymbol{\\mu}\\).\nA forma, o tamanho e a orientação dessas elipses dependem diretamente de \\(\\boldsymbol{\\Sigma}\\):\n\nquando \\(\\rho = 0\\), as elipses são alinhadas com os eixos coordenados;\nquando \\(\\rho \\neq 0\\), as elipses tornam-se inclinadas, refletindo a associação linear entre \\(Y_1\\) e \\(Y_2\\).\n\nEssa interpretação geométrica será essencial mais adiante, quando discutirmos projeções, decomposições ortogonais e ajuste de modelos de regressão, nos quais a ideia de “direções relevantes” no espaço dos dados desempenha papel central.\nMesmo nesse cenário conjunto, algumas propriedades permanecem familiares e ajudam a consolidar a intuição:\n\nAs distribuições marginais continuam sendo Normais univariadas: \\[\nY_1 \\sim N(\\mu_1, \\sigma_1^2),\n\\qquad\nY_2 \\sim N(\\mu_2, \\sigma_2^2).\n\\]\n\nEssas marginais mostram que, marginalmente, cada componente do vetor se comporta como uma variável Normal comum, mas isso não elimina a possibilidade de dependência entre elas quando observadas conjuntamente.\n\nAs distribuições condicionais também são Normais: \\[\nY_1 \\mid Y_2 = y_2\n\\sim\nN\\!\\left(\n\\mu_1 + \\rho\\frac{\\sigma_1}{\\sigma_2}(y_2 - \\mu_2),\n\\,\n(1 - \\rho^2)\\sigma_1^2\n\\right).\n\\]\n\nAqui aparece uma ideia conceitualmente profunda e extremamente importante para o que virá depois: a média condicional de uma variável Normal é uma função linear da variável condicionante.\nEssa linearidade não é um artifício do modelo, nem uma escolha conveniente; ela é uma consequência direta da estrutura da Normalidade conjunta. Em modelos de regressão, essa propriedade será reinterpretada como a relação entre a resposta e as covariáveis, agora formulada de maneira explícita e sistemática.\nPortanto, compreender a Normal bivariada é compreender, em um cenário simples, a origem probabilística da ideia de regressão como relação média condicional.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Distribuição Normal</span>"
    ]
  },
  {
    "objectID": "ap_normal.html#distribuição-normal-multivariada",
    "href": "ap_normal.html#distribuição-normal-multivariada",
    "title": "15  Distribuição Normal",
    "section": "15.3 Distribuição Normal Multivariada",
    "text": "15.3 Distribuição Normal Multivariada\nNo caso geral, consideramos um vetor aleatório \\[\n\\mathbf{Y} \\in \\mathbb{R}^n,\n\\] que segue uma Distribuição Normal multivariada se\n\\[\n\\mathbf{Y} \\sim N_n(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}),\n\\]\nonde \\(\\boldsymbol{\\mu}\\) é o vetor de médias e \\(\\boldsymbol{\\Sigma}\\) é a matriz de covariância, simétrica e definida positiva.\nA função densidade associada é\n\\[\nf(\\mathbf{y})\n=\n\\frac{1}{(2\\pi)^{n/2} |\\boldsymbol{\\Sigma}|^{1/2}}\n\\exp\\!\\left\\{\n-\\frac{1}{2}\n(\\mathbf{y} - \\boldsymbol{\\mu})^\\top\n\\boldsymbol{\\Sigma}^{-1}\n(\\mathbf{y} - \\boldsymbol{\\mu})\n\\right\\}.\n\\]\nNeste ponto, é importante fazer uma mudança consciente na forma de pensar. Não estamos mais lidando com observações isoladas, mas com vetores aleatórios, e a incerteza passa a ser descrita por estruturas geométricas em espaços de dimensão maior.\nO vetor \\(\\boldsymbol{\\mu}\\) representa o centro da distribuição no espaço \\(\\mathbb{R}^n\\), enquanto a matriz \\(\\boldsymbol{\\Sigma}\\) determina como a variabilidade se organiza em torno desse centro. Mais especificamente, \\(\\boldsymbol{\\Sigma}\\) define:\n\ndireções ao longo das quais a variabilidade conjunta é maior;\ndireções ao longo das quais a variabilidade conjunta é menor;\ndependências lineares entre as componentes do vetor.\n\nEssas direções não precisam coincidir com os eixos coordenados originais, e essa observação será fundamental quando discutirmos projeções e decomposições em regressão múltipla.\nA expressão que aparece no expoente da densidade envolve novamente uma quantidade do tipo\n\\[\n(\\mathbf{y} - \\boldsymbol{\\mu})^\\top\n\\boldsymbol{\\Sigma}^{-1}\n(\\mathbf{y} - \\boldsymbol{\\mu}),\n\\]\nque produz um escalar a partir de vetores e matrizes. Assim como no caso bivariado, não é necessário, neste momento, compreender formalmente essa expressão como uma forma quadrática. Por ora, basta interpretar essa quantidade como uma medida de distância multivariada entre \\(\\mathbf{y}\\) e o centro \\(\\boldsymbol{\\mu}\\), ajustada pela estrutura de covariância.\nEssa forma de medir distância explica por que as regiões de maior densidade da Normal multivariada são elipsoides em \\(\\mathbb{R}^n\\), generalizando as elipses vistas no caso bivariado.\nAlgumas propriedades fundamentais seguem diretamente dessa definição e merecem ser destacadas, pois reaparecerão continuamente ao longo do estudo de modelos de regressão.\nA esperança e a covariância do vetor aleatório são dadas por\n\\[\n\\mathbb{E}[\\mathbf{Y}] = \\boldsymbol{\\mu},\n\\qquad\n\\mathrm{Cov}(\\mathbf{Y}) = \\boldsymbol{\\Sigma}.\n\\]\nEssas expressões formalizam a interpretação de \\(\\boldsymbol{\\mu}\\) como centro da distribuição e de \\(\\boldsymbol{\\Sigma}\\) como descrição completa da variabilidade conjunta.\nUma propriedade absolutamente central da Normal multivariada é sua estabilidade por transformações lineares. Se tomarmos uma transformação do tipo\n\\[\n\\mathbf{Z} = \\mathbf{A}\\mathbf{Y} + \\mathbf{a},\n\\]\nentão a variável transformada também segue uma distribuição Normal multivariada:\n\\[\n\\mathbf{Z} \\sim\nN_m(\\mathbf{A}\\boldsymbol{\\mu} + \\mathbf{a},\\,\n\\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^\\top).\n\\]\nEssa propriedade merece atenção especial. Ela afirma que qualquer combinação linear de um vetor Normal multivariado continua sendo Normal, independentemente da dimensão envolvida.\nEsse resultado será a pedra angular da teoria de regressão linear. Quando estudarmos regressão, veremos que os estimadores dos coeficientes, os valores ajustados e diversos contrastes estatísticos são obtidos exatamente como transformações lineares do vetor de respostas. A Normalidade dessas quantidades decorre diretamente desta propriedade, e não de argumentos ad hoc.\nOutra quantidade natural que surge no contexto da Normal multivariada é a chamada distância de Mahalanobis:\n\\[\nQ =\n(\\mathbf{Y} - \\boldsymbol{\\mu})^\\top\n\\boldsymbol{\\Sigma}^{-1}\n(\\mathbf{Y} - \\boldsymbol{\\mu}),\n\\]\npara o qual vale\n\\[\nQ \\sim \\chi^2_n.\n\\]\nMais uma vez, não é necessário aprofundar formalmente esse resultado neste momento. Conceitualmente, ele afirma que a distância multivariada entre \\(\\mathbf{Y}\\) e seu centro, quando devidamente padronizada pela matriz de covariância, possui uma distribuição conhecida.\nEsse fato será explorado de forma sistemática em regressão, onde somas de quadrados, estatísticas de teste e medidas de ajuste surgirão como casos particulares desse tipo de expressão.\nAssim, a Distribuição Normal multivariada fornece não apenas um modelo probabilístico para vetores de dados, mas também a base matemática para compreender por que as quantidades centrais da regressão admitem distribuições explícitas e interpretáveis.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Distribuição Normal</span>"
    ]
  },
  {
    "objectID": "ap_normal.html#partição-da-normal-multivariada",
    "href": "ap_normal.html#partição-da-normal-multivariada",
    "title": "15  Distribuição Normal",
    "section": "15.4 Partição da Normal Multivariada",
    "text": "15.4 Partição da Normal Multivariada\nUm dos recursos mais poderosos da Normal multivariada é a possibilidade de particionar o vetor aleatório em blocos menores e ainda assim manter uma descrição probabilística completa e explícita.\nConsidere o vetor aleatório particionado como\n\\[\n\\mathbf{Y} =\n\\begin{bmatrix}\n\\mathbf{Y}_1 \\\\\n\\mathbf{Y}_2\n\\end{bmatrix}\n\\sim\nN_n\\!\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_1 \\\\\n\\boldsymbol{\\mu}_2\n\\end{bmatrix},\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_{11} & \\boldsymbol{\\Sigma}_{12} \\\\\n\\boldsymbol{\\Sigma}_{21} & \\boldsymbol{\\Sigma}_{22}\n\\end{bmatrix}\n\\right).\n\\]\nAqui, a partição é puramente conceitual: estamos apenas reorganizando o vetor em dois blocos, sem alterar o modelo probabilístico subjacente. Ainda assim, essa simples reorganização permite responder a perguntas fundamentais sobre o comportamento do vetor aleatório.\nEm particular, ela nos permite distinguir claramente dois tipos de informação:\n\ncomportamento marginal, isto é, como cada bloco se distribui quando considerado isoladamente;\ncomportamento condicional, isto é, como um bloco se distribui quando o outro é observado.\n\nAs distribuições marginais seguem diretamente da definição da Normal multivariada:\n\\[\n\\mathbf{Y}_1 \\sim N_{n_1}(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_{11}),\n\\qquad\n\\mathbf{Y}_2 \\sim N_{n_2}(\\boldsymbol{\\mu}_2, \\boldsymbol{\\Sigma}_{22}).\n\\]\nEssas expressões mostram que, ao “olharmos apenas para uma parte do vetor”, o comportamento probabilístico dessa parte continua sendo Normal, com média e covariância correspondentes aos blocos apropriados de \\(\\boldsymbol{\\mu}\\) e \\(\\boldsymbol{\\Sigma}\\) (Anderson (2003); Casella e Berger (2002)). No entanto, essa visão marginal ignora completamente a dependência entre os blocos.\nA riqueza da Normal multivariada aparece de forma ainda mais clara ao analisarmos o comportamento condicional. A distribuição de \\(\\mathbf{Y}_1\\) dado que \\(\\mathbf{Y}_2 = \\mathbf{y}_2\\) é\n\\[\n\\mathbf{Y}_1 \\mid \\mathbf{Y}_2 = \\mathbf{y}_2\n\\sim\nN_{n_1}\\!\\left(\n\\boldsymbol{\\mu}_1 +\n\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\n(\\mathbf{y}_2 - \\boldsymbol{\\mu}_2),\n\\,\n\\boldsymbol{\\Sigma}_{11} -\n\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\n\\boldsymbol{\\Sigma}_{21}\n\\right).\n\\]\nEssa expressão concentra vários conceitos importantes em um único resultado.\nPrimeiro, observe que a média condicional de \\(\\mathbf{Y}_1\\) não é simplesmente \\(\\boldsymbol{\\mu}_1\\). Ela é ajustada pelo termo \\[\n\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\n(\\mathbf{y}_2 - \\boldsymbol{\\mu}_2),\n\\] que incorpora a informação trazida pela observação de \\(\\mathbf{Y}_2\\). Esse ajuste depende exclusivamente da estrutura de covariância entre os blocos, e não de escolhas arbitrárias de modelagem.\nSegundo, note que a matriz de covariância condicional \\[\n\\boldsymbol{\\Sigma}_{11} -\n\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\n\\boldsymbol{\\Sigma}_{21}\n\\] é sempre menor, no sentido de variância, do que a covariância marginal \\(\\boldsymbol{\\Sigma}_{11}\\). Isso formaliza matematicamente uma ideia intuitiva: ao observar parte do vetor, reduzimos a incerteza sobre o restante.\nEsse resultado mostra que, na Normal multivariada, o condicionamento produz dois efeitos simultâneos e bem definidos:\n\na média é deslocada de forma linear em função da parte observada;\na variabilidade é reduzida de maneira controlada pela estrutura de dependência.\n\nEssas duas propriedades; linearidade da média condicional e redução da variância, não são hipóteses adicionais nem aproximações, elas são consequências diretas da Normalidade conjunta.\nEmbora ainda não estejamos estudando modelos de regressão, é importante registrar que essa lógica será reinterpretada mais adiante quando os coeficientes de um modelo passarem a ser entendidos como efeitos condicionais, isto é, como variações esperadas em uma componente do vetor quando outras são mantidas fixas.\nAssim, a partição da Normal multivariada fornece o arcabouço probabilístico que sustenta a noção de regressão como estudo de relações condicionais, mesmo antes de qualquer equação de regressão ser escrita explicitamente.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Distribuição Normal</span>"
    ]
  },
  {
    "objectID": "ap_normal.html#covariância-zero-implica-independência",
    "href": "ap_normal.html#covariância-zero-implica-independência",
    "title": "15  Distribuição Normal",
    "section": "15.5 Covariância zero implica independência",
    "text": "15.5 Covariância zero implica independência\nEm geral, para vetores aleatórios arbitrários, a condição de covariância nula não implica independência. Isto é, pode ocorrer que duas variáveis tenham covariância igual a zero e, ainda assim, sejam dependentes.\nEntretanto, a família Normal possui uma propriedade estrutural especial:\n\nSe um vetor aleatório é Normal multivariado, então quaisquer componentes (ou combinações lineares de componentes) que tenham covariância zero são independentes.\n\nMais precisamente, se \\[\n\\mathbf{Y} \\sim N_n(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\n\\] e \\(\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^n\\), então \\[\n\\mathrm{Cov}(\\mathbf{a}^\\top \\mathbf{Y},\\, \\mathbf{b}^\\top \\mathbf{Y}) = 0\n\\quad \\Longrightarrow \\quad\n\\mathbf{a}^\\top \\mathbf{Y}\n\\text{ e }\n\\mathbf{b}^\\top \\mathbf{Y}\n\\text{ são independentes.}\n\\]\nEssa propriedade é específica da distribuição Normal e não vale em geral para outras distribuições multivariadas. Uma demonstração pode ser encontrada em Anderson (2003) e em Casella e Berger (2002).\nEssa característica será essencial na regressão linear, pois permite concluir, sob Normalidade dos erros, que:\n\no estimador dos coeficientes é independente do vetor de resíduos;\ndiferentes somas de quadrados associadas a projeções ortogonais são independentes;\nestatísticas baseadas em decomposições ortogonais possuem distribuições independentes.\n\nA independência decorre da ortogonalidade geométrica no espaço das observações, combinada com a estrutura da Normal multivariada.\n\n15.5.1 Independência no caso bivariado\nNo caso particular bivariado, seja \\[\n\\mathbf{Y} =\n\\begin{bmatrix}\nY_1 \\\\\nY_2\n\\end{bmatrix}\n\\sim N_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}),\n\\] com \\[\n\\boldsymbol{\\Sigma} =\n\\begin{bmatrix}\n\\sigma_1^2 & \\rho \\sigma_1 \\sigma_2 \\\\\n\\rho \\sigma_1 \\sigma_2 & \\sigma_2^2\n\\end{bmatrix}.\n\\]\nEntão vale a equivalência:\n\\[\n\\rho = 0\n\\quad \\Longleftrightarrow \\quad\nY_1 \\text{ e } Y_2 \\text{ são independentes.}\n\\]\nEssa equivalência é uma consequência direta da forma explícita da densidade conjunta e constitui uma propriedade distintiva da Normal bivariada. Em distribuições gerais, correlação zero não implica independência.\n\n\n15.5.2 Caso marginal\nSe \\[\n\\mathbf{Y} \\sim N_n(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}),\n\\] então qualquer subconjunto de componentes de \\(\\mathbf{Y}\\) também possui distribuição Normal multivariada.\nFormalmente, se particionarmos \\[\n\\mathbf{Y} =\n\\begin{bmatrix}\n\\mathbf{Y}_1 \\\\\n\\mathbf{Y}_2\n\\end{bmatrix},\n\\] então as distribuições marginais são\n\\[\n\\mathbf{Y}_1 \\sim N_{n_1}(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_{11}),\n\\qquad\n\\mathbf{Y}_2 \\sim N_{n_2}(\\boldsymbol{\\mu}_2, \\boldsymbol{\\Sigma}_{22}).\n\\]\nEssa estabilidade marginal é consequência direta da definição da Normal multivariada e pode ser verificada integrando-se a densidade conjunta ou utilizando o resultado de que combinações lineares preservam Normalidade.\n\n\n15.5.3 Partições e Independência entre Blocos\nConsidere novamente a partição\n\\[\n\\mathbf{Y} =\n\\begin{bmatrix}\n\\mathbf{Y}_1 \\\\\n\\mathbf{Y}_2\n\\end{bmatrix}\n\\sim\nN_n\\!\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_1 \\\\\n\\boldsymbol{\\mu}_2\n\\end{bmatrix},\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_{11} & \\boldsymbol{\\Sigma}_{12} \\\\\n\\boldsymbol{\\Sigma}_{21} & \\boldsymbol{\\Sigma}_{22}\n\\end{bmatrix}\n\\right).\n\\]\nEntão:\n\\[\n\\boldsymbol{\\Sigma}_{12} = \\mathbf{0}\n\\quad \\Longleftrightarrow \\quad\n\\mathbf{Y}_1 \\text{ e } \\mathbf{Y}_2 \\text{ são independentes.}\n\\]\nIsto é, na Normal multivariada, blocos são independentes se, e somente se, sua matriz de covariância cruzada for nula.\nEssa propriedade tem papel central na teoria da regressão linear clássica. Quando se demonstra que duas quantidades são obtidas por projeções ortogonais e que a matriz de covariância cruzada entre elas é nula, a Normalidade garante automaticamente independência.\nEssa combinação entre:\n\nortogonalidade algébrica,\ncovariância nula,\nestrutura Normal,\n\né o mecanismo matemático que sustenta a independência entre soma de quadrados do modelo e soma de quadrados do erro, fundamento da estatística \\(F\\).",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Distribuição Normal</span>"
    ]
  },
  {
    "objectID": "ap_normal.html#papel-da-distribuição-normal-na-fundamentação-dos-modelos-de-regressão",
    "href": "ap_normal.html#papel-da-distribuição-normal-na-fundamentação-dos-modelos-de-regressão",
    "title": "15  Distribuição Normal",
    "section": "15.6 Papel da Distribuição Normal na fundamentação dos modelos de regressão",
    "text": "15.6 Papel da Distribuição Normal na fundamentação dos modelos de regressão\nOs resultados apresentados neste apêndice fornecem uma base probabilística razoável para a formulação e a análise dos modelos clássicos de regressão linear. O objetivo aqui é explicitar as estruturas matemáticas que a tornam analisável de forma rigorosa.\nEm modelos de regressão linear com erros normalmente distribuídos, considera-se que o vetor de respostas pode ser escrito como\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\n\\qquad\n\\boldsymbol{\\varepsilon} \\sim N_n(\\mathbf{0}, \\sigma^2\\mathbf{I}_n),\n\\]\no que implica diretamente que\n\\[\n\\mathbf{Y} \\sim N_n(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathbf{I}_n).\n\\]\ncomo discutido em Kutner et al. (2005). Essa especificação não define o modelo de regressão em si, que pode ser formulado sob hipóteses mais gerais, mas estabelece um caso fundamental no qual resultados exatos de inferência podem ser obtidos em amostras finitas.\nA partir dessa estrutura probabilística decorrem, de forma sistemática, várias propriedades centrais da regressão linear clássica:\n\nos estimadores dos coeficientes surgem como transformações lineares do vetor aleatório \\(\\mathbf{Y}\\);\nos resíduos e as somas de quadrados associadas ao ajuste do modelo surgem como expressões quadráticas em \\(\\mathbf{Y}\\);\nas distribuições amostrais das estatísticas utilizadas para inferência são obtidas a partir das distribuições dessas transformações lineares e quadráticas.\n\nDeste modo, estatísticas do tipo \\(t\\) e \\(F\\) não são introduzidas de maneira ad hoc, mas emergem naturalmente da combinação entre a Normal multivariada e as operações algébricas realizadas sobre o vetor de respostas.\nOutros apêndices exploram explicitamente essas estruturas, estudando as distribuições associadas a transformações lineares e quadráticas de vetores aleatórios conjuntamente normais. Esse desenvolvimento permitirá compreender, de forma unificada, a origem das principais ferramentas inferenciais utilizadas em modelos de regressão linear.\n\n\n\n\nAnderson, T. W. 2003. An Introduction to Multivariate Statistical Analysis. 3º ed. New York: Wiley.\n\n\nCasella, George, e Roger L. Berger. 2002. Statistical Inference. 2º ed. Pacific Grove: Duxbury.\n\n\nKutner, Michael H., Christopher J. Nachtsheim, John Neter, e William Li. 2005. Applied Linear Statistical Models. 5º ed. New York: McGraw-Hill.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Distribuição Normal</span>"
    ]
  },
  {
    "objectID": "ap_forma_linear.html",
    "href": "ap_forma_linear.html",
    "title": "16  Formas Lineares e Quadráticas na Normal Multivariada",
    "section": "",
    "text": "16.1 Preliminares: Normal multivariada e transformações lineares\nEm regressão linear clássica, muitos estimadores e estatísticas fundamentais são formas lineares e formas quadráticas de um vetor aleatório Normal multivariado. Essa conexão não é meramente técnica: ela é estrutural e explica por que distribuições exatas em amostras finitas podem ser obtidas sob normalidade. A fundamentação matricial e geométrica desses resultados pode ser encontrada, em diferentes níveis de formalidade, em Harville (1997) e Anderson (2003).\nSeja \\[\n\\mathbf{Y}\\sim N_n(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}),\n\\] com \\(\\boldsymbol{\\Sigma}\\) simétrica definida positiva.\nUma propriedade central da Normal multivariada é sua estabilidade por transformações lineares: se \\(\\mathbf{A}\\) é uma matriz fixa \\(m\\times n\\) e \\(\\mathbf{a}\\in\\mathbb{R}^m\\), então \\[\n\\mathbf{Z}=\\mathbf{A}\\mathbf{Y}+\\mathbf{a}\n\\sim\nN_m(\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{a},\\, \\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^\\top).\n\\] Esse resultado é apresentado de forma sistemática em Anderson (2003) e constitui a base probabilística da teoria da regressão linear.\nComo caso particular, para um vetor fixo \\(\\mathbf{c}\\in\\mathbb{R}^n\\), \\[\nL=\\mathbf{c}^\\top\\mathbf{Y}\n\\sim\nN\\!\\left(\\mathbf{c}^\\top\\boldsymbol{\\mu},\\, \\mathbf{c}^\\top\\boldsymbol{\\Sigma}\\mathbf{c}\\right).\n\\] A dedução segue diretamente da propriedade anterior e também pode ser vista como aplicação do fato de que combinações lineares de vetores Normais permanecem Normais, como discutido em Casella e Berger (2002).\nEssa estrutura será aplicada diretamente a:",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Formas Lineares e Quadráticas na Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "ap_forma_linear.html#preliminares-normal-multivariada-e-transformações-lineares",
    "href": "ap_forma_linear.html#preliminares-normal-multivariada-e-transformações-lineares",
    "title": "16  Formas Lineares e Quadráticas na Normal Multivariada",
    "section": "",
    "text": "\\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{Y}\\);\ncontrastes \\(\\mathbf{C}\\hat{\\boldsymbol{\\beta}}\\);\npredições lineares e combinações de coeficientes.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Formas Lineares e Quadráticas na Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "ap_forma_linear.html#padronização-multivariada-e-redução-ao-caso-mathbfi",
    "href": "ap_forma_linear.html#padronização-multivariada-e-redução-ao-caso-mathbfi",
    "title": "16  Formas Lineares e Quadráticas na Normal Multivariada",
    "section": "16.2 Padronização multivariada e redução ao caso \\(\\mathbf{I}\\)",
    "text": "16.2 Padronização multivariada e redução ao caso \\(\\mathbf{I}\\)\nPara estudar formas quadráticas, é útil reduzir o problema ao caso esférico.\nComo \\(\\boldsymbol{\\Sigma}\\) é definida positiva, existe uma matriz simétrica definida positiva \\(\\boldsymbol{\\Sigma}^{1/2}\\) tal que \\[\n\\boldsymbol{\\Sigma}^{1/2}\\boldsymbol{\\Sigma}^{1/2}=\\boldsymbol{\\Sigma},\n\\qquad\n(\\boldsymbol{\\Sigma}^{1/2})^{-1}=\\boldsymbol{\\Sigma}^{-1/2}.\n\\] A existência dessa raiz quadrada decorre da decomposição espectral de matrizes simétricas definidas positivas, tratada em detalhe em Harville (1997).\nDefina \\[\n\\mathbf{Z}=\\boldsymbol{\\Sigma}^{-1/2}(\\mathbf{Y}-\\boldsymbol{\\mu}).\n\\]\nEntão \\[\n\\mathbf{Z}\\sim N_n(\\mathbf{0},\\mathbf{I}_n).\n\\]\nEssa transformação separa de maneira conceitualmente limpa os papéis de média e dispersão, reduzindo a análise probabilística ao caso esférico. Além disso, ela mostra que a chamada distância de Mahalanobis \\[\n(\\mathbf{Y}-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\mathbf{Y}-\\boldsymbol{\\mu})\n\\] é simplesmente a norma euclidiana ao quadrado de um vetor Normal padrão: \\[\n(\\mathbf{Y}-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\mathbf{Y}-\\boldsymbol{\\mu})\n=\n\\mathbf{Z}^\\top\\mathbf{Z}.\n\\] Essa ponte entre geometria (normas e projeções) e distribuição (Qui-quadrado) é central na teoria da inferência sob normalidade.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Formas Lineares e Quadráticas na Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "ap_forma_linear.html#forma-linear-distribuição-interpretação-e-conexão-com-regressão",
    "href": "ap_forma_linear.html#forma-linear-distribuição-interpretação-e-conexão-com-regressão",
    "title": "16  Formas Lineares e Quadráticas na Normal Multivariada",
    "section": "16.3 Forma linear: distribuição, interpretação e conexão com regressão",
    "text": "16.3 Forma linear: distribuição, interpretação e conexão com regressão\nUma forma linear de um vetor aleatório é uma expressão do tipo \\[\nL=\\mathbf{c}^\\top\\mathbf{Y},\n\\] onde \\(\\mathbf{c}\\) é fixo.\nSe \\(\\mathbf{Y}\\sim N_n(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\), então:\n\n\\(L\\) é Normal;\n\\(\\mathbb{E}(L)=\\mathbf{c}^\\top\\boldsymbol{\\mu}\\);\n\\(\\mathrm{Var}(L)=\\mathbf{c}^\\top\\boldsymbol{\\Sigma}\\mathbf{c}\\).\n\nNa regressão linear, quando se assume normalidade dos erros, tanto \\(\\hat{\\boldsymbol{\\beta}}\\) quanto qualquer contraste \\(\\mathbf{C}\\hat{\\boldsymbol{\\beta}}\\) são formas lineares em \\(\\mathbf{Y}\\). Por isso, possuem distribuição Normal exata em amostras finitas.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Formas Lineares e Quadráticas na Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "ap_forma_linear.html#forma-quadrática-definição-e-interpretação",
    "href": "ap_forma_linear.html#forma-quadrática-definição-e-interpretação",
    "title": "16  Formas Lineares e Quadráticas na Normal Multivariada",
    "section": "16.4 Forma quadrática: definição e interpretação",
    "text": "16.4 Forma quadrática: definição e interpretação\nUma forma quadrática em \\(\\mathbf{Y}\\) é uma expressão do tipo \\[\nQ=\\mathbf{Y}^\\top\\mathbf{A}\\mathbf{Y},\n\\] onde \\(\\mathbf{A}\\) é uma matriz fixa \\(n\\times n\\).\nPrimeira observação fundamental: apenas a parte simétrica de \\(\\mathbf{A}\\) influencia \\(Q\\). De fato, \\[\n\\mathbf{Y}^\\top\\mathbf{A}\\mathbf{Y}\n=\n\\mathbf{Y}^\\top\\left(\\frac{\\mathbf{A}+\\mathbf{A}^\\top}{2}\\right)\\mathbf{Y}.\n\\] Logo, pode-se assumir \\(\\mathbf{A}\\) simétrica sem perda de generalidade, fato discutido na literatura matricial estatística como em Harville (1997).\nEm regressão, as somas de quadrados são precisamente formas quadráticas: \\[\n\\mathrm{SQReg}=\\mathbf{Y}^\\top\\mathbf{H}\\mathbf{Y},\n\\qquad\n\\mathrm{SQRes}=\\mathbf{Y}^\\top\\mathbf{M}\\mathbf{Y}.\n\\]\nO comportamento probabilístico de \\(Q\\) depende criticamente das propriedades estruturais de \\(\\mathbf{A}\\). Quando \\(\\mathbf{A}\\) é uma projeção ortogonal (simétrica e idempotente), a forma quadrática se reduz a uma soma de quadrados de componentes Normais padrão em um subespaço.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Formas Lineares e Quadráticas na Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "ap_forma_linear.html#o-caso-central-mathbfztopmathbfz-e-a-distribuição-chi2",
    "href": "ap_forma_linear.html#o-caso-central-mathbfztopmathbfz-e-a-distribuição-chi2",
    "title": "16  Formas Lineares e Quadráticas na Normal Multivariada",
    "section": "16.5 O caso central: \\(\\mathbf{Z}^\\top\\mathbf{Z}\\) e a distribuição \\(\\chi^2\\)",
    "text": "16.5 O caso central: \\(\\mathbf{Z}^\\top\\mathbf{Z}\\) e a distribuição \\(\\chi^2\\)\nSe \\(\\mathbf{Z}\\sim N_n(\\mathbf{0},\\mathbf{I}_n)\\), então suas componentes são independentes e \\[\nZ_i\\sim N(0,1).\n\\]\nConsequentemente, \\[\n\\mathbf{Z}^\\top\\mathbf{Z}\n=\n\\sum_{i=1}^n Z_i^2\n\\sim\n\\chi^2_n.\n\\]\nEsse resultado é a base de todas as somas de quadrados na regressão sob normalidade, como apresentado em textos de inferência como Casella e Berger (2002).",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Formas Lineares e Quadráticas na Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "ap_forma_linear.html#projeções-ortogonais-e-qui-quadrado",
    "href": "ap_forma_linear.html#projeções-ortogonais-e-qui-quadrado",
    "title": "16  Formas Lineares e Quadráticas na Normal Multivariada",
    "section": "16.6 Projeções ortogonais e Qui-quadrado",
    "text": "16.6 Projeções ortogonais e Qui-quadrado\nSeja \\(\\mathbf{Z}\\sim N_n(\\mathbf{0},\\mathbf{I}_n)\\) e seja \\(\\mathbf{A}\\) simétrica e idempotente: \\[\n\\mathbf{A}^\\top=\\mathbf{A},\n\\qquad\n\\mathbf{A}^2=\\mathbf{A}.\n\\]\nSe \\(r=\\mathrm{rank}(\\mathbf{A})=\\mathrm{tr}(\\mathbf{A})\\), então \\[\n\\mathbf{Z}^\\top\\mathbf{A}\\mathbf{Z}\\sim \\chi^2_r.\n\\] Esse resultado pode ser deduzido via decomposição espectral de \\(\\mathbf{A}\\) e é tratado com rigor em Harville (1997) e Rencher e Christensen (2012).\nGeometricamente, \\(\\mathbf{A}\\) projeta sobre um subespaço \\(S\\) de dimensão \\(r\\). Assim, \\(\\mathbf{Z}^\\top\\mathbf{A}\\mathbf{Z}\\) mede o comprimento ao quadrado da componente projetada de \\(\\mathbf{Z}\\) em \\(S\\), que se comporta como soma de quadrados de \\(r\\) Normais padrão.\nEsse resultado aplica-se diretamente às matrizes \\(\\mathbf{H}\\) (posto \\(p+1\\)) e \\(\\mathbf{M}\\) (posto \\(n-p-1\\)). Quando \\(\\mathbf{Z}\\) representa a versão padronizada do vetor de erros, as somas de quadrados do modelo e do resíduo assumem distribuições Qui-quadrado com graus de liberdade dados por esses postos.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Formas Lineares e Quadráticas na Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "ap_forma_linear.html#independência-via-ortogonalidade",
    "href": "ap_forma_linear.html#independência-via-ortogonalidade",
    "title": "16  Formas Lineares e Quadráticas na Normal Multivariada",
    "section": "16.7 Independência via ortogonalidade",
    "text": "16.7 Independência via ortogonalidade\nSe \\(\\mathbf{Z}\\sim N_n(\\mathbf{0},\\mathbf{I}_n)\\) e \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) são simétricas idempotentes tais que \\[\n\\mathbf{A}\\mathbf{B}=\\mathbf{0},\n\\] então \\[\n\\mathbf{Z}^\\top\\mathbf{A}\\mathbf{Z}\n\\ \\perp\\\n\\mathbf{Z}^\\top\\mathbf{B}\\mathbf{Z}.\n\\] A independência decorre da decomposição ortogonal do espaço combinada com a simetria esférica da Normal padrão.\nCritério análogo vale para independência entre forma linear \\(L=\\mathbf{a}^\\top\\mathbf{Z}\\) e forma quadrática \\(Q=\\mathbf{Z}^\\top\\mathbf{A}\\mathbf{Z}\\): se \\[\n\\mathbf{A}\\mathbf{a}=\\mathbf{0},\n\\] então \\(L\\) e \\(Q\\) são independentes.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Formas Lineares e Quadráticas na Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "ap_forma_linear.html#mrlm-normal",
    "href": "ap_forma_linear.html#mrlm-normal",
    "title": "16  Formas Lineares e Quadráticas na Normal Multivariada",
    "section": "16.8 MRLM Normal",
    "text": "16.8 MRLM Normal\nConsidere o modelo de regressão linear múltipla sob normalidade e homocedasticidade:\n\\[\n\\mathbf{Y}=\\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon},\n\\qquad\n\\boldsymbol{\\varepsilon}\\sim N_n(\\mathbf{0},\\sigma^2\\mathbf{I}_n).\n\\]\nComo consequência direta da estabilidade da Normal multivariada por transformações lineares, tem-se\n\\[\n\\mathbf{Y}\\sim N_n(\\mathbf{X}\\boldsymbol{\\beta},\\sigma^2\\mathbf{I}_n).\n\\]\nAs quantidades centrais do modelo podem ser classificadas estruturalmente da seguinte forma, destacando-se suas distribuições quando há uma forma conhecida.\n\n16.8.1 Estimador como forma linear em \\(\\mathbf{Y}\\)\n\\[\n\\hat{\\boldsymbol{\\beta}}\n=\n(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{Y}.\n\\]\nTrata-se de uma transformação linear do vetor aleatório \\(\\mathbf{Y}\\). Logo,\n\\[\n\\hat{\\boldsymbol{\\beta}}\n\\sim\nN_{p+1}\n\\!\\left(\n\\boldsymbol{\\beta},\n\\sigma^2(\\mathbf{X}^\\top\\mathbf{X})^{-1}\n\\right).\n\\]\nEm particular, para cada componente \\(\\hat{\\beta}_j\\),\n\\[\n\\frac{\\hat{\\beta}_j-\\beta_j}\n{\\sigma\\sqrt{[(\\mathbf{X}^\\top\\mathbf{X})^{-1}]_{jj}}}\n\\sim N(0,1).\n\\]\n\n\n16.8.2 Resíduos como transformação linear\n\\[\n\\hat{\\boldsymbol{\\varepsilon}}\n=\n\\mathbf{M}\\mathbf{Y},\n\\qquad\n\\mathbf{M}=\\mathbf{I}_n-\\mathbf{H}.\n\\]\nComo \\(\\hat{\\boldsymbol{\\varepsilon}}\\) é transformação linear de um vetor Normal multivariado, segue que\n\\[\n\\hat{\\boldsymbol{\\varepsilon}}\n\\sim\nN_n\\!\\left(\n\\mathbf{M}\\mathbf{X}\\boldsymbol{\\beta},\n\\,\\sigma^2\\mathbf{M}\n\\right).\n\\]\nComo \\(\\mathbf{M}\\mathbf{X}=\\mathbf{0}\\), obtém-se\n\\[\n\\hat{\\boldsymbol{\\varepsilon}}\n\\sim\nN_n(\\mathbf{0},\\,\\sigma^2\\mathbf{M}).\n\\]\nAssim, os resíduos possuem estrutura Normal degenerada em um subespaço de dimensão \\(n-p-1\\).\n\n\n16.8.3 Soma de quadrados residual como forma quadrática\n\\[\n\\mathrm{SQRes}\n=\n\\hat{\\boldsymbol{\\varepsilon}}^\\top\\hat{\\boldsymbol{\\varepsilon}}\n=\n\\mathbf{Y}^\\top\\mathbf{M}\\mathbf{Y}.\n\\]\nComo \\(\\mathbf{M}\\) é simétrica idempotente com\n\\[\n\\mathrm{rank}(\\mathbf{M})=n-p-1,\n\\]\nsegue que, após padronização por \\(\\sigma^2\\),\n\\[\n\\frac{\\mathrm{SQRes}}{\\sigma^2}\n=\n\\frac{\\mathbf{Y}^\\top\\mathbf{M}\\mathbf{Y}}{\\sigma^2}\n\\sim\n\\chi^2_{\\,n-p-1}.\n\\]\nEquivalentemente,\n\\[\n\\mathrm{SQRes}\n\\sim\n\\sigma^2\\,\\chi^2_{\\,n-p-1}.\n\\]\nAlém disso, definindo\n\\[\n\\hat{\\sigma}^2=\\frac{\\mathrm{SQRes}}{n-p-1},\n\\]\ntem-se\n\\[\n\\frac{(n-p-1)\\hat{\\sigma}^2}{\\sigma^2}\n\\sim\n\\chi^2_{\\,n-p-1}.\n\\]\n\n\n16.8.4 Decomposição ortogonal da soma de quadrados total\n\\[\n\\mathbf{Y}^\\top\\mathbf{Y}\n=\n\\mathbf{Y}^\\top\\mathbf{H}\\mathbf{Y}\n+\n\\mathbf{Y}^\\top\\mathbf{M}\\mathbf{Y}.\n\\]\nSob o modelo Normal, após padronização por \\(\\sigma^2\\), as duas parcelas tornam-se formas quadráticas associadas a projeções ortogonais complementares.\nEscrevendo \\(\\boldsymbol{\\mu}=\\mathbf{X}\\boldsymbol{\\beta}\\), obtém-se:\n\nParte ajustada (forma quadrática não-central)\n\n\\[\n\\frac{\\mathbf{Y}^\\top\\mathbf{H}\\mathbf{Y}}{\\sigma^2}\n\\sim\n\\chi^2_{\\,p+1}(\\lambda),\n\\qquad\n\\lambda=\\frac{\\boldsymbol{\\mu}^\\top\\boldsymbol{\\mu}}{\\sigma^2}.\n\\]\n\nParte residual (forma quadrática central)\n\n\\[\n\\frac{\\mathbf{Y}^\\top\\mathbf{M}\\mathbf{Y}}{\\sigma^2}\n\\sim\n\\chi^2_{\\,n-p-1}.\n\\]\nComo \\(\\mathbf{H}\\mathbf{M}=\\mathbf{0}\\) e ambas são projeções ortogonais, essas duas formas quadráticas são independentes:\n\\[\n\\frac{\\mathbf{Y}^\\top\\mathbf{H}\\mathbf{Y}}{\\sigma^2}\n\\ \\perp\\\n\\frac{\\mathbf{Y}^\\top\\mathbf{M}\\mathbf{Y}}{\\sigma^2}.\n\\]\n\n\n16.8.5 Surgimento das distribuições \\(t\\) e \\(F\\)\nA partir dessas distribuições, emergem naturalmente as estatísticas de inferência.\nPara cada coeficiente,\n\\[\nT_j\n=\n\\frac{\\hat{\\beta}_j-\\beta_j}\n{\\hat{\\sigma}\\sqrt{[(\\mathbf{X}^\\top\\mathbf{X})^{-1}]_{jj}}}\n=\n\\frac{\n\\displaystyle\n\\frac{\\hat{\\beta}_j-\\beta_j}\n{\\sigma\\sqrt{[(\\mathbf{X}^\\top\\mathbf{X})^{-1}]_{jj}}}\n}\n{\n\\sqrt{\\frac{\\mathrm{SQRes}/\\sigma^2}{\\,n-p-1\\,}}\n}\n\\sim t_{\\,n-p-1}.\n\\]\nA estatística \\(t\\) surge como razão entre uma Normal padrão e a raiz de uma Qui-quadrado independente dividida por seus graus de liberdade.\nDe modo análogo, a estatística global de significância do modelo pode ser escrita como\n\\[\nF\n=\n\\frac{\n\\left(\\mathbf{Y}^\\top\\mathbf{H}\\mathbf{Y}\\right)/(p+1)\n}{\n\\left(\\mathbf{Y}^\\top\\mathbf{M}\\mathbf{Y}\\right)/(n-p-1)\n}\n\\sim\nF_{\\,p+1,\\;n-p-1},\n\\]\nno caso central sob a hipótese nula apropriada.\nAssim, as distribuições \\(t\\) e \\(F\\) não são introduzidas ad hoc: elas emergem diretamente da estrutura geométrica das projeções ortogonais combinada com a Normalidade multivariada do vetor de respostas.\n\n\n\n\nAnderson, T. W. 2003. An Introduction to Multivariate Statistical Analysis. 3º ed. New York: Wiley.\n\n\nCasella, George, e Roger L. Berger. 2002. Statistical Inference. 2º ed. Pacific Grove: Duxbury.\n\n\nHarville, David A. 1997. Matrix Algebra From a Statistician’s Perspective. New York: Springer.\n\n\nRencher, Alvin C., e William F. Christensen. 2012. Methods of Multivariate Analysis. 3º ed. Hoboken: Wiley.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Formas Lineares e Quadráticas na Normal Multivariada</span>"
    ]
  },
  {
    "objectID": "ap_tratamento.html",
    "href": "ap_tratamento.html",
    "title": "17  Tratamento de dados para regressão (pré-modelagem)",
    "section": "",
    "text": "17.1 Exemplos do dia a dia e Checklist rápido\nEste apêndice organiza, em forma de roteiro estruturado, os procedimentos que devem ser realizados antes do ajuste de qualquer modelo de regressão. O objetivo é preparar bases de dados tabulares (em que linhas representam observações e colunas representam variáveis) para que estejam coerentes, consistentes, completas e adequadas à modelagem estatística.\nOs métodos de regressão (lineares, generalizados ou penalizados) pressupõem que a variabilidade observada nos dados represente o fenômeno real sob estudo, e não erros de registro, inconsistências de escala ou problemas de codificação. O tratamento pré-modelagem constitui, portanto, o primeiro passo do raciocínio estatístico aplicado: garantir que o modelo descreva o mundo observado e não artefatos do processo de coleta ou organização dos dados.\nPrincípios norteadores da preparação de dados\nFerramentas e escopo\nFluxo geral do processo analítico\ncoleta → organização → tratamento → (entregável: base tratada + dicionário + log de decisões) → modelagem → diagnóstico → interpretação\nO tratamento de dados é, portanto, uma etapa intermediária e estruturante, que conecta a coleta bruta à modelagem estatística.\nExemplos rápidos do dia a dia\nObserve que, assim como a cozinha precisa estar organizada para a receita “dar certo”, a base precisa estar coerente, completa e padronizada para que a modelagem produza resultados interpretáveis e estatisticamente válidos.\nChecklist rápido: Faça antes de modelar\nCartões de decisão\n##Leitura e organização dos dados\nLer corretamente um arquivo de dados (formato, delimitador, encoding, símbolo decimal) é a primeira barreira contra erros que podem comprometer toda a análise. Uma leitura incorreta pode fazer com que números sejam interpretados como texto, datas como caracteres ou colunas inteiras sejam deslocadas. Esses problemas, se não detectados no início, propagam-se até a modelagem e podem invalidar inferências.\nEm regressão, erros de tipagem e leitura afetam diretamente a construção da matriz de projeto \\(\\mathbf{X}\\) e da variável resposta \\(\\mathbf{y}\\). Portanto, esta etapa é estrutural, não meramente operacional.\nConexões com a literatura e boas práticas\nAutores como Charnet et al. (2008) e Sheather (2009) enfatizam que a qualidade dos dados influencia diretamente as estimativas, testes e intervalos de confiança. Essa etapa integra o que se denomina análise exploratória de dados (AED): conhecer o material empírico antes de ajustar qualquer modelo.\nExplorar não é modelar, é compreender a estrutura do dado.\nFunções úteis no R\nPacotes e funções frequentemente utilizados nessa etapa incluem:\nO objetivo não é “usar funções”, mas garantir que a base esteja corretamente interpretada pelo R antes de qualquer transformação.\nO que fazer\nBoas práticas\nErros comuns (e como evitar)\nChecklist rápido\nAntes de seguir para qualquer transformação ou modelagem, verifique:\nSe qualquer resposta for negativa, o tratamento ainda não começou e a leitura ainda não terminou.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tratamento de dados para regressão (pré-modelagem)</span>"
    ]
  },
  {
    "objectID": "ap_tratamento.html#exemplos-do-dia-a-dia-e-checklist-rápido",
    "href": "ap_tratamento.html#exemplos-do-dia-a-dia-e-checklist-rápido",
    "title": "17  Tratamento de dados para regressão (pré-modelagem)",
    "section": "",
    "text": "CSV que não abre “corretamente”: o arquivo foi salvo com ; em vez de , como separador, e o decimal usa , (ex.: 3,14). Resultado: números são interpretados como texto.\nTratamento: especificar corretamente o delimitador e o símbolo decimal na leitura; padronizar separadores.\nAcentos e caracteres estranhos: colunas aparecem como ação, aÃ§Ã£o ou nomes quebrados.\nTratamento: ajustar o encoding (ex.: UTF-8 vs latin-1) e padronizar nomes de variáveis.\nTipos incorretos: idades lidas como texto (\"21\"), datas como strings (\"2025-10-24\") e variáveis 0/1 lidas como numéricas quando deveriam ser categóricas. Tratamento: converter explicitamente para os tipos adequados (numérico, data/hora, fator) e validar o resultado.\nCódigos de “faltante” disfarçados: valores como 999, -1, NA, \"\" representam ausência, mas estão misturados com dados válidos. (Wickham et al. (2024))\nTratamento: recodificar para ausentes padronizados e decidir entre excluir, imputar ou manter com justificativa.\nDuplicatas e chaves quebradas: a mesma unidade amostral aparece repetida; totais e médias ficam incorretos.\nTratamento: identificar chaves únicas, remover ou conciliar duplicatas e documentar a decisão.\nCategorias inconsistentes: Masculino, M, masc e male aparecem como níveis distintos.\nTratamento: unificar rótulos, definir categoria de referência substantiva e gerar dummies adequadamente.\nDatas em formatos mistos: 31/12/2025 e 12-31-2025 na mesma coluna.\nTratamento: normalizar formato, verificar timezone quando relevante e extrair componentes (ano/mês/dia) se necessário.\nValores impossíveis: altura = -5, proporção &gt; 1, idade = 300.\nTratamento: definir faixas válidas segundo o domínio e corrigir ou descartar observações inconsistentes.\nEscalas heterogêneas: receita em reais e custo em milhares de reais; área em m² e km².\nTratamento: padronizar unidades ou aplicar reescala/padronização (ex.: z-score).\nOutliers evidentes: uma observação muito superior às demais; zero estrutural inesperado.\nTratamento: verificar plausibilidade no contexto, decidir entre correção, winsorização, transformação ou manutenção justificada.\nResposta incompatível com o objetivo: deseja-se regressão linear contínua, mas a variável resposta é binária ou contagem.\nTratamento: verificar se o tipo de resposta é compatível com o modelo pretendido (linear, logístico, Poisson etc.).\n\n\n\n\nDefina o objetivo e o tipo de resposta: contínua, binária ou contagem; confirme que a variável resposta é compatível com o modelo pretendido.\nGaranta leitura correta: verifique separador, encoding e símbolo decimal; confirme que variáveis numéricas não foram importadas como texto.\nAcerte tipos e unidades: converta datas, inteiros e reais; padronize unidades e nomes de variáveis.\nMapeie e trate dados faltantes: identifique NA, vazios e códigos artificiais (ex.: 999); decida entre excluir, imputar ou manter com justificativa. (Wickham et al. (2024))\nElimine inconsistências: remova ou una duplicatas, valide chaves, corrija valores impossíveis.\nCuide de outliers: identifique pontos extremos; corrija, transforme ou mantenha. Todas as ações com outliers devem ser realizadas com justificativa documentada.\nCodifique variáveis qualitativas: unifique rótulos, defina categoria de referência e evite colinearidade perfeita na matriz de projeto.\nCheque condições mínimas para regressão:\n\nVariabilidade das covariáveis;\n\nAusência de colinearidade perfeita (\\(X^\\top X\\) não singular);\n\nNúmero de observações maior que o número de parâmetros (\\(n &gt; p + 1\\));\n\nGere sumários e gráficos básicos;\n\nSalve a versão tratada com dicionário de variáveis e log de decisões.\n\n\n\n\nFaltantes:\nQual a proporção? A variável é essencial? Qual o mecanismo provável (MCAR/MAR/MNAR)? (Little e Rubin (2019))\n→ excluir / imputar (média, mediana, por grupo, métodos múltiplos) / manter com justificativa. (Kuhn et al. (2024); Buuren e Groothuis-Oudshoorn (2024))\nOutliers:\nErro de digitação ou unidade? Valor plausível no domínio?\n→ corrigir / winsorizar / transformar / manter e justificar.\nCategóricas:\nExistem níveis raros (alta cardinalidade)?\n→ agrupar em “outros”; escolher referência substantiva; padronizar rótulos.\nEscalas:\nMagnitudes muito diferentes entre covariáveis?\n→ aplicar padronização (z-score) ou reescala; revisar unidades.\nReprodutibilidade:\nToda decisão foi registrada no log de tratamento?\n\n\n\n\n\n\n\n\n\n\nreadr: read_csv(), read_delim(), guess_encoding(), locale()\nreadxl: read_excel()\ndplyr: glimpse(), summarise(), count()\nfs e here: organização de diretórios e caminhos reproduzíveis\nvroom: leitura rápida de arquivos grandes\nstringi: diagnóstico e conversão de encoding\n\n\n\n\nMapear a origem dos dados (site, repositório, disciplina, experimento) e sua licença de uso.\nConferir o formato do arquivo (CSV, XLSX, SAV, DTA etc.) e o delimitador utilizado (,, ;, tabulação).\nAjustar corretamente o encoding (UTF-8, latin-1) e o símbolo decimal (, ou .).\nDefinir um esquema de pastas do projeto (por exemplo: dados_brutos/, dados_interinos/, dados_tratados/) e convenções padronizadas de nomes.\n\n\n\nManter a pasta dados_brutos/ imutável. Nunca sobrescrever dados originais.\nRealizar todas as modificações em cópias armazenadas em dados_tratados/.\nPadronizar nomes de colunas: minúsculas, sem acento, em formato snake_case.\nCriar um arquivo README_dados.md com:\n\nFonte dos dados\nData de download\nResponsável\nDescrição geral das variáveis\n\n\n\n\nArquivo CSV com ; como separador lido como se fosse , → verificar explicitamente o delimitador.\nValores como 1,23 interpretados como texto → ajustar locale(decimal_mark = \",\") ou converter adequadamente.\nDatas ambíguas (ex.: 01/02/2020) → padronizar formato (preferencialmente ISO 8601: 2020-02-01).\nColunas numéricas importadas como character → converter explicitamente e validar.\n\n\n\n\nColunas numéricas estão realmente no tipo numérico?\nDatas foram reconhecidas como datas?\nHá duplicatas segundo a chave da base?\nNomes de colunas estão padronizados?\nA leitura preservou o número esperado de linhas e colunas?",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tratamento de dados para regressão (pré-modelagem)</span>"
    ]
  },
  {
    "objectID": "ap_tratamento.html#estrutura-e-tipagem-das-variáveis",
    "href": "ap_tratamento.html#estrutura-e-tipagem-das-variáveis",
    "title": "17  Tratamento de dados para regressão (pré-modelagem)",
    "section": "17.2 Estrutura e tipagem das variáveis",
    "text": "17.2 Estrutura e tipagem das variáveis\nA estrutura e o tipo das variáveis determinam como a matriz de projeto \\(\\mathbf{X}\\) será construída. Uma tipagem incorreta não é apenas um erro técnico: ela altera o significado estatístico do modelo, pode introduzir colinearidade artificial e comprometer estimativas, testes e interpretações.\nDe acordo com Sheather (2009), a correta identificação do tipo de cada variável é condição essencial para que o modelo represente adequadamente a relação entre preditores e resposta. Uma variável categórica tratada como numérica impõe uma estrutura inexistente; uma variável numérica tratada como categórica multiplica desnecessariamente parâmetros.\nEm regressão, a tipagem define como cada coluna entra em \\(\\mathbf{X}\\): como valor contínuo, como conjunto de dummies ou como transformação temporal.\nFerramentas R úteis\nFunções base do R:\n\nstr() — inspeciona a estrutura da base\n\nclass() — verifica o tipo de objeto\n\nas.numeric(), as.integer() — conversões numéricas\n\nas.Date() — conversão de datas\n\nPacotes auxiliares:\n\nlubridate: ymd(), dmy(), ymd_hms()\n\ndplyr: mutate(), across()\n\nforcats: manipulação de fatores\n\nPara validação: - is.na() — valores ausentes\n- is.finite() — valores numéricos válidos\n- Verificações de faixa (ex.: idade ≥ 0)\nO objetivo não é apenas converter, mas verificar se a conversão preserva o significado substantivo da variável.\nClassifique as variáveis\nAntes de qualquer modelagem, identifique claramente:\n\nNuméricas contínuas (salário, temperatura, peso)\n\nNuméricas discretas (número de visitas, contagem de eventos)\n\nBinárias (0/1, sim/não)\n\nCategóricas nominais (sexo, região)\n\nCategóricas ordinais (baixo, médio, alto)\n\nTemporais (datas, horários)\n\nIdentificadores (IDs) que não devem entrar como preditores numéricos\n\nIdentificadores numéricos (ex.: matrícula, CPF, código do lote) não são variáveis quantitativas e não devem ser tratados como tal na regressão.\nAjustes típicos\n\nConverter strings numéricas para número de forma explícita.\nConverter datas para classe apropriada e, quando necessário, extrair componentes (ano, mês).\nTransformar variáveis 0/1 em fator quando a interpretação for categórica.\nDefinir fatores com níveis claros e ordenados quando houver estrutura ordinal.\nPadronizar rótulos inconsistentes (ex.: M, Masculino, male → um único padrão).\n\nApós os ajustes, reavalie a estrutura da base e confirme que cada coluna está no tipo esperado antes de avançar para o tratamento de faltantes ou criação de dummies.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tratamento de dados para regressão (pré-modelagem)</span>"
    ]
  },
  {
    "objectID": "ap_tratamento.html#tratamento-de-dados-faltantes-missing",
    "href": "ap_tratamento.html#tratamento-de-dados-faltantes-missing",
    "title": "17  Tratamento de dados para regressão (pré-modelagem)",
    "section": "17.3 Tratamento de dados faltantes (missing)",
    "text": "17.3 Tratamento de dados faltantes (missing)\nDados faltantes alteram o tamanho efetivo da amostra, modificam a estrutura da matriz de projeto \\(\\mathbf{X}\\) e podem introduzir viés nas estimativas quando o mecanismo de ausência não é completamente aleatório (MCAR). Em regressão, a presença de faltantes pode funcionar como um mecanismo implícito de seleção amostral, afetando tanto a estimação quanto a interpretação dos coeficientes.\nExcluir observações com valores ausentes equivale, muitas vezes, a analisar uma subamostra potencialmente não representativa.\nErros clássicos\n\nImputar zero indiscriminadamente (por exemplo, via replace_na) sem considerar o significado substantivo.\nRealizar junções (joins) entre tabelas sem validar chaves, criando duplicações artificiais.\nRemover colunas inteiras apenas por conterem NA, sem avaliar sua relevância analítica.\nImputar a variável resposta sem justificativa metodológica.\n\nSugestões da literatura\nSegundo Little e Rubin (2019), o tratamento adequado depende do mecanismo de ausência:\n\nMCAR (Missing Completely At Random): ausência independente de variáveis observadas e não observadas.\nMAR (Missing At Random): ausência depende apenas de variáveis observadas.\nMNAR (Missing Not At Random): ausência depende de informação não observada.\n\nEm situações simples e com baixa proporção de faltantes, imputações por média ou mediana podem ser aceitáveis como aproximação. Em contextos mais complexos, recomenda-se imputação múltipla ou métodos baseados em modelos, preservando a incerteza associada ao processo.\nComo minimizar problemas\n\nMapear sistematicamente os valores ausentes com is.na() e quantificar proporções por variável.\nIdentificar códigos artificiais de ausência (999, -1, \"\") e recodificá-los para NA.\nAvaliar a importância substantiva da variável antes de decidir pela exclusão.\nDocumentar cada remoção ou imputação realizada.\nPara duplicatas associadas a junções, utilizar verificações de chave e funções como duplicated() ou distinct().\n\nA decisão deve ser estatística e substantiva.\nBoas práticas\n\nManter um log explícito das decisões tomadas.\nComparar estatísticas descritivas antes e depois da imputação.\nEvitar alterar a distribuição da variável de forma não justificada.\nNão imputar automaticamente a variável resposta nesta etapa, salvo sob estratégia metodológica claramente definida.\n\nProcedimento recomendado\n\nCalcular a taxa de faltantes por coluna.\nIdentificar padrões estruturais de ausência.\nClassificar o possível mecanismo (MCAR/MAR/MNAR).\nDefinir estratégia:\n\nexcluir observações,\n\nimputar (média, mediana, por grupo ou múltipla),\n\nmanter e modelar posteriormente o mecanismo de ausência.\n\nRegistrar todas as decisões no log de tratamento.\n\nTratamento de dados faltantes não é apenas limpeza — é uma decisão inferencial que pode alterar os resultados do modelo.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tratamento de dados para regressão (pré-modelagem)</span>"
    ]
  },
  {
    "objectID": "ap_tratamento.html#detecção-de-valores-extremos-e-inconsistências",
    "href": "ap_tratamento.html#detecção-de-valores-extremos-e-inconsistências",
    "title": "17  Tratamento de dados para regressão (pré-modelagem)",
    "section": "17.4 Detecção de valores extremos e inconsistências",
    "text": "17.4 Detecção de valores extremos e inconsistências\nValores extremos (outliers) podem ser resultado de erro de digitação, inconsistência de unidade ou representar fenômenos reais raros. Em regressão, esses pontos podem influenciar de forma desproporcional os estimadores \\(\\widehat{\\boldsymbol{\\beta}}\\), os resíduos e os diagnósticos de ajuste.\nNem todo valor extremo é um erro, mas todo valor extremo exige investigação.\nErros clássicos\n\nRemover automaticamente qualquer ponto extremo sem verificar sua origem.\nEstabelecer cortes arbitrários sem registrar critérios e justificativas.\nTransformar a variável resposta sem considerar a nova interpretação dos coeficientes.\nIgnorar a possibilidade de que o ponto seja informativo para o fenômeno estudado.\n\nSegundo Charnet et al. (2008) e Sheather (2009), valores extremos podem distorcer estimativas, ampliar variâncias e comprometer testes de hipóteses.\nBarnett e Lewis (1994) oferecem fundamentos formais para detecção de outliers em análises univariadas e multivariadas, distinguindo entre:\n\nObservações extremas na distribuição marginal;\nPontos de alta alavancagem (leverage);\nObservações influentes (que alteram substancialmente o ajuste do modelo).\n\nEmbora diagnósticos formais de influência sejam discutidos em outros capítulos, a identificação preliminar já deve ocorrer nesta etapa.\nFerramentas R\nPara inspeção inicial:\n\nquantile() — limites interquartílicos\n\nsd() e scale() — padronização e z-score\n\nsummary() — inspeção geral\n\nVisualizações:\n\nggplot2::geom_boxplot()\n\nggplot2::geom_histogram()\n\nggplot2::geom_point()\n\nA visualização frequentemente revela padrões que estatísticas isoladas não mostram.\nComo resolver ou minimizar efeitos\n\nConfirmar se o valor resulta de erro de digitação ou unidade (ex.: centímetros vs metros).\nCorrigir inconsistências quando verificadas documentalmente.\nAplicar winsorização (DescTools::Winsorize) quando a estratégia for limitar extremos mantendo observações.\nUtilizar transformações (log, raiz quadrada) quando houver forte assimetria.\nManter a observação quando for substantivamente plausível e documentar a decisão.\n\nA remoção deve ser exceção, não regra.\nObservação importante\nEliminar valores extremos altera a distribuição da variável, o tamanho da amostra e potencialmente a matriz \\(\\mathbf{X}\\). Toda decisão deve ser registrada no log de tratamento.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tratamento de dados para regressão (pré-modelagem)</span>"
    ]
  },
  {
    "objectID": "ap_tratamento.html#padronização-e-transformações-numéricas",
    "href": "ap_tratamento.html#padronização-e-transformações-numéricas",
    "title": "17  Tratamento de dados para regressão (pré-modelagem)",
    "section": "17.5 Padronização e transformações numéricas",
    "text": "17.5 Padronização e transformações numéricas\nPadronizar variáveis numéricas torna os preditores comparáveis em escala e pode melhorar a estabilidade numérica de procedimentos de estimação. Transformações adequadas, por sua vez, reduzem assimetria, estabilizam variância e facilitam interpretações coerentes com o fenômeno estudado.\nEm regressão linear clássica, a padronização não altera o ajuste global do modelo nem o \\(R^2\\), mas modifica a escala dos coeficientes e sua interpretação. Já em métodos penalizados (Ridge e LASSO), a padronização é praticamente indispensável.\nCharnet et al. (2008) ressaltam que variáveis em escalas muito distintas podem gerar instabilidade numérica e dificultar a comparação entre efeitos.\nSheather (2009) destaca que reescalar variáveis pode facilitar a interpretação de coeficientes em regressões múltiplas, especialmente quando as unidades originais são muito grandes ou muito pequenas.\nMontgomery, Peck, e Vining (2021) enfatizam que diferenças extremas de magnitude entre covariáveis podem afetar diagnósticos e procedimentos computacionais.\nFerramentas R\n\nscale() — padronização pelo z-score (média zero e desvio padrão um).\nReescala min–max — pode ser feita manualmente via transformações aritméticas.\nMASS::boxcox() — identificação de transformações do tipo Box–Cox.\nlog() ou log1p() — transformações logarítmicas (úteis para assimetria positiva).\nTransformações via dplyr::mutate() para aplicação sistemática.\n\nA escolha da transformação deve ser guiada pelo comportamento empírico da variável e pelo contexto substantivo.\nSoluções recomendadas na literatura\n\nPara variáveis altamente assimétricas, aplicar transformações logarítmicas pode aproximar a normalidade e reduzir heterocedasticidade.\nPara contagens moderadas, Paula (2004) sugere transformação por raiz quadrada como alternativa simples.\nPara distribuições fortemente assimétricas, considerar Box–Cox quando apropriado.\nEvitar misturar variáveis em escalas radicalmente diferentes sem padronização prévia.\n\nQuando padronizar\n\nPara comparar magnitudes relativas entre preditores.\nPara métodos penalizados (Ridge, LASSO), em que a penalização depende da escala.\nPara algoritmos baseados em distância ou otimização numérica.\nQuando unidades originais dificultam interpretação direta.\n\nPadronizar não é obrigação universal; é uma decisão metodológica que deve preservar a interpretabilidade do modelo.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tratamento de dados para regressão (pré-modelagem)</span>"
    ]
  },
  {
    "objectID": "ap_tratamento.html#codificação-de-variáveis-categóricas-dummies",
    "href": "ap_tratamento.html#codificação-de-variáveis-categóricas-dummies",
    "title": "17  Tratamento de dados para regressão (pré-modelagem)",
    "section": "17.6 Codificação de variáveis categóricas (dummies)",
    "text": "17.6 Codificação de variáveis categóricas (dummies)\nUma variável categórica com \\(k\\) níveis não pode entrar diretamente como coluna numérica em \\(\\mathbf{X}\\). É necessário convertê-la em variáveis indicadoras (dummies), usualmente em número \\(k-1\\), para evitar colinearidade perfeita.\nA escolha da codificação determina a interpretação dos coeficientes estimados.\nErros comuns\n\nCriar \\(k\\) dummies para \\(k\\) categorias (armadilha da variável dummy), gerando singularidade em \\(\\mathbf{X}^\\top \\mathbf{X}\\).\nEscolher categoria de referência sem critério substantivo.\nManter níveis raros, produzindo colunas quase vazias e estimativas instáveis.\nUsar rótulos inconsistentes (acentos, abreviações, maiúsculas/minúsculas misturadas).\nTratar variável ordinal como nominal sem refletir sobre a estrutura de ordem.\n\nConexão com a modelagem\nQuando uma variável categórica é convertida corretamente em \\(k-1\\) dummies, cada coeficiente estimado representa a diferença média entre aquela categoria e a categoria de referência, mantendo os demais preditores constantes.\nSe todas as \\(k\\) dummies forem incluídas juntamente com o intercepto, ocorre dependência linear exata, tornando \\(\\mathbf{X}^\\top \\mathbf{X}\\) não invertível no caso clássico de MRLM.\nPortanto, a codificação correta não é apenas conveniência computacional, é condição para a existência do estimador de mínimos quadrados.\nFundamentação teórica\nCharnet et al. (2008) destacam a importância da escolha da categoria de referência para interpretação dos coeficientes.\nJames et al. (2013) discutem como alta cardinalidade pode gerar modelos instáveis e sobreajustados.\nPara variáveis ordinais, a estrutura de ordenação pode ser incorporada explicitamente, evitando perda de informação.\nFerramentas R\n\nmodel.matrix() — gera automaticamente a matriz de projeto com codificação apropriada.\nfastDummies — criação explícita de variáveis indicadoras.\nrecipes::step_dummy() — codificação sistemática em pipelines.\nforcats — manipulação e reorganização de níveis.\nFatores ordenados (ordered) para variáveis com hierarquia natural.\n\nO R, por padrão, utiliza codificação por tratamento (treatment contrasts), mas outras codificações podem ser especificadas conforme necessidade.\nBoas práticas\n\nDefinir categoria de referência com base em critério substantivo (grupo controle, baseline, padrão).\nAgrupar níveis raros quando apropriado.\nManter um dicionário de variáveis documentando níveis e significados.\nPadronizar rótulos antes da geração de dummies.\nVerificar o número final de parâmetros gerados após codificação.\n\nAtenção à alta cardinalidade\nVariáveis com muitos níveis distintos podem gerar dezenas ou centenas de colunas em \\(\\mathbf{X}\\), aumentando dimensionalidade e risco de sobreajuste.\nNesses casos, considere:\n\nAgrupamento por regras de negócio;\nSeleção de variáveis;\nMétodos penalizados (quando apropriado na etapa seguinte).\n\nCodificar corretamente é garantir que a estrutura qualitativa do fenômeno seja traduzida adequadamente para o modelo quantitativo.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tratamento de dados para regressão (pré-modelagem)</span>"
    ]
  },
  {
    "objectID": "ap_tratamento.html#verificação-de-condições-para-modelagem-linear",
    "href": "ap_tratamento.html#verificação-de-condições-para-modelagem-linear",
    "title": "17  Tratamento de dados para regressão (pré-modelagem)",
    "section": "17.7 Verificação de condições para modelagem linear",
    "text": "17.7 Verificação de condições para modelagem linear\nCumprir condições mínimas como variabilidade das covariáveis, ausência de colinearidade perfeita e tamanho amostral adequado (\\(n &gt; p + 1\\)) é o que permite aplicar os resultados teóricos da regressão linear e múltipla com segurança.\nNo modelo linear clássico, a existência do estimador de mínimos quadrados depende de \\(\\mathbf{X}^\\top \\mathbf{X}\\) ser invertível, o que exige que a matriz de projeto \\(\\mathbf{X}\\) tenha posto completo. Assim, esta verificação não é opcional — é estrutural. (Searle (2016))\nEssas checagens antecedem os diagnósticos formais e reduzem problemas posteriores na estimação e interpretação.\nFerramentas R úteis\n\ncor() ou pacote corrr — inspeção de associações entre preditores.\nqr() ou Matrix::rankMatrix() — verificação do posto da matriz de projeto.\ncar::vif() — cálculo do fator de inflação da variância (VIF). (Fox e Weisberg (2024))\nsummary() e inspeção de variância — identificação de variáveis constantes.\n\nEssas ferramentas ajudam a identificar:\n\nColinearidade perfeita ou quase perfeita;\nVariáveis constantes ou quase constantes;\nRelações lineares redundantes entre preditores.\n\nComo mitigar problemas\n\nRemover ou combinar variáveis altamente correlacionadas.\nRevisar a codificação de dummies para evitar dependência linear.\nPadronizar variáveis quando necessário.\nEliminar duplicatas estruturais na base.\nReduzir dimensionalidade quando \\(p\\) se aproxima de \\(n\\).\n\nToda verificação deve ser documentada no relatório de tratamento.\nCompatibilidade da variável resposta\nAntes da modelagem, é essencial verificar se o tipo da variável resposta é compatível com o modelo pretendido:\n\nContínua: variável em escala intervalar ou razão.\nBinária: 0/1 ou fator com dois níveis.\nContagem: inteiros não negativos.\nProporção: valores no intervalo \\([0,1]\\).\n\nEscolher modelo inadequado ao tipo de resposta gera inferências inválidas.\nTabela-guia: tipo de resposta e cuidados\n\n\n\n\n\n\n\n\n\nTipo de resposta\nExemplo\nTratamento pré-modelo\nModelo típico\n\n\n\n\nContínua\nPreço, temperatura\nVerificar outliers, padronização e unidades\nMRLS, MRLM\n\n\nBinária\nSucesso/fracasso\nConferir codificação 0/1, balanceamento e faltantes\nLogístico, Probit\n\n\nContagem\nNº de eventos\nAvaliar zeros estruturais e dispersão\nPoisson, Binomial Negativa\n\n\nProporção\nTaxa, share\nVerificar limites 0/1 e denominadores\nBeta, Quasi-binomial\n\n\n\nModelar sem verificar essas condições equivale a aplicar teoria sob premissas não verificadas. O tratamento adequado garante que a transição para a modelagem seja matemática e estatisticamente legítima.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tratamento de dados para regressão (pré-modelagem)</span>"
    ]
  },
  {
    "objectID": "ap_tratamento.html#sumários-e-visualizações-exploratórias",
    "href": "ap_tratamento.html#sumários-e-visualizações-exploratórias",
    "title": "17  Tratamento de dados para regressão (pré-modelagem)",
    "section": "17.8 Sumários e visualizações exploratórias",
    "text": "17.8 Sumários e visualizações exploratórias\nExplorar os dados antes da modelagem permite identificar padrões, inconsistências e relações estruturais que orientam decisões de limpeza, transformação e especificação do modelo.\nA visualização não substitui a modelagem, ela antecipa problemas e revela estruturas que podem afetar a construção de \\(\\mathbf{X}\\) e a escolha do modelo. (Tufte (2001))\nFerramentas R\nFunções e pacotes úteis nesta etapa:\n\nsummary() — estatísticas descritivas básicas.\ntable() ou dplyr::count() — frequências de variáveis categóricas.\nggplot2 — histogramas, boxplots, gráficos de dispersão.\nGGally::ggpairs() — matriz gráfica de dispersão.\ncorrplot ou ggcorrplot — visualização de matrizes de correlação.\nplotly — visualizações interativas (opcional).\n\nO objetivo é compreender estrutura, dispersão, assimetria e possíveis relações lineares preliminares.\nComo resolver dificuldades comuns\n\nDistribuições muito assimétricas → aplicar transformações (log, raiz quadrada) e reavaliar.\nCategorias vazias ou raras → reclassificar níveis ou agrupar.\nEscalas muito distintas → padronizar antes de comparar magnitudes.\nCorrelações elevadas entre preditores → revisar especificação do modelo.\n\nVisualizar é diagnosticar antes do diagnóstico formal.\nProdutos esperados\nAo final da etapa exploratória, espera-se:\n\nTabela de estatísticas descritivas por variável numérica (média, desvio padrão, p5, p50, p95).\nFrequência absoluta e relativa por variável categórica.\nHistogramas e boxplots para avaliar distribuição.\nGráficos de dispersão entre \\(Y\\) e cada \\(X\\).\nMatriz de correlação entre preditores numéricos.\n\nEsses produtos funcionam como evidência documental do entendimento da base antes da modelagem.\nPerguntas-guia\n\nAlguma variável apresenta assimetria extrema?\nExistem valores fora de faixa plausível?\nHá categorias quase vazias?\nQuais pares de \\(X\\) apresentam correlação elevada?\nA relação entre \\(Y\\) e \\(X\\) parece aproximadamente linear?\n\nResponder a essas perguntas reduz erros na etapa seguinte.\nMapa de funções em R (resumo operacional)\n\nLeitura: read_csv, read_delim, read_excel.\nInspeção: glimpse, summary, count.\nTransformação: mutate, across, scale.\nTipagem: as.numeric, as.Date, factor.\nCorrelação: cor.\nMatriz de projeto: model.matrix.\nDiagnóstico estrutural: qr, rankMatrix, vif.\n\nA exploração sistemática consolida a transição entre tratamento de dados e modelagem estatística.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tratamento de dados para regressão (pré-modelagem)</span>"
    ]
  },
  {
    "objectID": "ap_tratamento.html#salvamento-e-documentação-da-base-tratada",
    "href": "ap_tratamento.html#salvamento-e-documentação-da-base-tratada",
    "title": "17  Tratamento de dados para regressão (pré-modelagem)",
    "section": "17.9 Salvamento e documentação da base tratada",
    "text": "17.9 Salvamento e documentação da base tratada\nA documentação do tratamento (log + dicionário) e o versionamento adequado garantem reprodutibilidade, transparência metodológica e facilitam revisão por pares.\nEm regressão, a qualidade das inferências depende não apenas do modelo ajustado, mas da rastreabilidade das decisões tomadas antes da modelagem.\nO que entregar ao final do tratamento\n\nBase final tratada (formato padrão, por exemplo: CSV).\nDicionário de variáveis contendo:\n\nNome da variável\nTipo\nUnidade (quando aplicável)\nNíveis (para categóricas)\nOrigem ou transformação realizada\n\nLog de decisões documentando:\n\nO que foi modificado\nPor que foi modificado\nQuando foi modificado\n\nScript reproduzível contendo todas as etapas do tratamento (opcional, mas altamente recomendado).\n\nO objetivo é que qualquer outro pesquisador consiga reconstruir exatamente a base utilizada na modelagem.\nConvenções úteis\n\nUtilizar nome de arquivo com carimbo de data, por exemplo:\nbase_tratada_2025-10-26.csv\nManter script de preparo versionado e comentado.\nUtilizar estrutura organizada de pastas:\n\ndados_brutos/\ndados_interinos/\ndados_tratados/\n\n\nNunca sobrescrever dados brutos.\nPipelines modernos e reprodutibilidade\nEm aplicações contemporâneas, o tratamento de dados é frequentemente organizado em pipelines: sequências estruturadas de leitura → transformação → validação → saída tratada → modelagem.\nEsse fluxo reduz erros humanos, aumenta consistência e fortalece a reprodutibilidade científica. No ecossistema R, essa abordagem é facilitada por ferramentas como tidyverse, recipes e estruturas de modelagem integradas.\nMesmo quando não formalizado em código automatizado, o processo deve ser concebido como um fluxo explícito, sequencial e documentado.\nModelar é o passo visível; documentar é o passo que garante credibilidade.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tratamento de dados para regressão (pré-modelagem)</span>"
    ]
  },
  {
    "objectID": "ap_tratamento.html#checklist-técnico-condições-mínimas-para-seguir-à-modelagem",
    "href": "ap_tratamento.html#checklist-técnico-condições-mínimas-para-seguir-à-modelagem",
    "title": "17  Tratamento de dados para regressão (pré-modelagem)",
    "section": "17.10 Checklist técnico: Condições mínimas para seguir à modelagem",
    "text": "17.10 Checklist técnico: Condições mínimas para seguir à modelagem\nEste checklist consolida os critérios estruturais que devem ser verificados ao final do tratamento de dados, antes do ajuste de qualquer modelo de regressão.\n\nVariabilidade das covariáveis: nenhuma variável explicativa deve ser constante ou quase constante.\nAusência de colinearidade perfeita: a matriz \\(\\mathbf{X}\\) deve ter posto completo; logo, \\(\\mathbf{X}^\\top \\mathbf{X}\\) deve ser não singular.\nTamanho amostral adequado: \\(n &gt; p + 1\\) no caso do modelo linear com intercepto; caso contrário, considere reduzir dimensionalidade ou ampliar a amostra.\nEscalas compatíveis: quando necessário, variáveis reescaladas ou padronizadas para evitar instabilidade numérica.\nTipos de dados conferidos: variáveis numéricas, categóricas e temporais corretamente tipadas.\nAusência de erros estruturais: duplicatas removidas, chaves validadas e inconsistências corrigidas.\nBase salva e documentada: versão final armazenada com data, autor e dicionário de variáveis.\nPronta para modelagem: a base pode ser utilizada diretamente em MRLS, MRLM, MLG ou métodos penalizados sem retrabalho estrutural.\n\nO não atendimento a qualquer desses critérios compromete a legitimidade estatística do modelo. A qualidade de toda inferência estatística depende dessa distinção, e ela começa antes do ajuste do primeiro modelo.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tratamento de dados para regressão (pré-modelagem)</span>"
    ]
  },
  {
    "objectID": "ap_tratamento.html#bases-para-prática",
    "href": "ap_tratamento.html#bases-para-prática",
    "title": "17  Tratamento de dados para regressão (pré-modelagem)",
    "section": "17.11 Bases para prática",
    "text": "17.11 Bases para prática\nAo finalizar o tratamento de uma base, tente ajustar um modelo simples apenas para validar tipos e dummies (sem discutir resultados). Se rodar sem erros e os sumários fizerem sentido, a base está pronta para a próxima unidade.\nEscada de dificuldade das bases - Nível 1 (aquecimento): Online Shoppers: tipagem + dummies + sumários.\n- Nível 2 (intermediário): Ames/House Prices: faltantes moderados + reescala + dicionário.\n- Nível 3 (avançado): Student Failure (messy): chaves/duplicatas + integração + plano de imputação.\n- Extra (discreta): Bike Sharing: temporais + zeros estruturais + outliers climáticos.\nEsta seção apresenta bases públicas e didáticas para exercícios de tratamento pré‑modelagem. Cada item traz link de acesso, o que a base representa e situações‑problema que motivam o tratamento. Ao final de cada base há um bloco Tarefas sugeridas para orientar o estudo.\n\n17.11.1 House Prices: Advanced Regression Techniques (Kaggle)\n\nTipo de resposta: Contínua (SalePrice).\nLink: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques\nSobre a base: preços de casas em Ames (Iowa, EUA), com ~79 variáveis numéricas e categóricas.\nSituação:\n\nMuitas colunas com valores ausentes (por ex.: LotFrontage, Alley, PoolQC).\nVariáveis categóricas com codificação inconsistente e níveis raros.\nOutliers em preço e área; unidades e escalas heterogêneas.\n\nExcelente caso para um pipeline completo de limpeza (missing, tipagem, codificação, reescala) antes da regressão contínua.\nTarefas sugeridas:\n\nMapear porcentagens de faltantes por coluna e decidir estratégia (excluir, imputar, manter).\nPadronizar nomes e unidades; verificar outliers em SalePrice e GrLivArea.\nUnificar níveis categóricos raros e definir dummies com categoria de referência.\nSalvar uma versão tratada e documentar as decisões.\n\n\n\n\n17.11.2 Ames Housing Dataset\n\nTipo de resposta: Contínua (SalePrice).\nLink: https://github.com/data-doctors/kaggle-house-prices-advanced-regression-techniques\nSobre a base: variação/derivação do problema de habitação de Ames, amplamente usada em cursos.\nSituação:\n\nMistura de tipos (numéricos + categóricos), com valores ausentes e níveis raros.\nRecomendação frequente de transformação logarítmica da resposta.\nOutliers estruturais (ex.: casas muito acima da média).\n\nReforça o contraste de estratégias de tratamento em relação à 10.1.\nTarefas sugeridas:\n\nComparar duas abordagens de tratamento de faltantes (ex.: imputação por mediana vs. KNN) e registrar impactos em sumários.\nTestar padronização vs. não padronização nas variáveis contínuas.\nProduzir um dicionário de variáveis claro.\n\n\n\n\n17.11.3 Online Shoppers Purchasing Intention (UCI)\n\nTipo de resposta: Binária (Revenue: sim/não).\nLink: https://archive.ics.uci.edu/ml/datasets/Online%2BShoppers%2BPurchasing%2BIntention%2BDataset\nSobre a base: sessões de navegação em e‑commerce; objetivo é prever se a sessão termina em compra.\nSituação:\n\nVariáveis categóricas e temporais misturadas às numéricas; necessidade de codificação.\nPossível desbalanceamento da classe Revenue.\nDatas/temporais como texto exigindo normalização e extração de componentes.\n\nBom caso de tratamento moderado, contrastando com bases mais “sujas”.\nTarefas sugeridas:\n\nVerificar distribuição de Revenue (balanceamento).\nDefinir dummies consistentes e padronizar escalas numéricas.\nCriar variáveis derivadas temporais (mês, dia da semana) de forma reproducível.\n\n\n\n\n17.11.4 Student Failure (Messy) Dataset (Kaggle)\n\nTipo de resposta: Binária (fail = 1 se o aluno reprova/sai; 0 caso contrário).\nLink: https://www.kaggle.com/code/sashatarakanova/student-failure-modelling-with-a-messy-dataset\nSobre a base: dados educacionais com múltiplas tabelas heterogêneas para prever reprovação.\nSituação:\n\nMuitos valores ausentes; tabelas com chaves não padronizadas; duplicatas.\nCategorias inconsistentes para o mesmo conceito (ex.: formas distintas de escrever “curso”).\nNecessidade de unificação/integração de fontes (join/merge) com validação.\n\nÓtimo para treinar integração e saneamento antes de qualquer modelagem binária.\nTarefas sugeridas:\n\nReconstruir uma chave única estável e eliminar duplicatas.\nMapear e recodificar categorias equivalentes.\nDocumentar um plano de imputação apropriado por variável.\n\n\n\n\n17.11.5 Bike Sharing Dataset (UCI)\n\nTipo de resposta: Discreta (contagem de bicicletas alugadas por hora/dia).\nLink: https://archive.ics.uci.edu/dataset/275/bike%2Bsharing%2Bdataset\nSobre a base: uso de bicicletas compartilhadas, com variáveis meteorológicas, feriados, sazonalidade e efeitos de hora do dia.\nSituação:\n\nContagens com muitos zeros em horários de baixa demanda e picos em horários de pico; necessidade de identificar zeros estruturais.\nVariáveis temporais em formato de texto exigindo conversão e extração (hora, dia da semana, feriado).\nPossíveis outliers (eventos climáticos extremos) e variabilidade alta da resposta.\nPadronização de escalas e codificação consistente de feriados/sazonalidade.\nPrepara para o tratamento de resposta discreta (contagem), anterior à escolha de modelos como Poisson ou Binomial Negativa.\n\nTarefas sugeridas:\n\nNormalizar as variáveis temporais e criar indicadores (feriado, fim de semana, hora do rush).\nCaracterizar zeros estruturais vs. esparsidade aleatória e discutir implicações para a modelagem.\nDetectar outliers climáticos e decidir estratégia (transformação, winsorização ou justificativa de manutenção).\nEntregar uma versão tratada com dicionário e log de decisões.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tratamento de dados para regressão (pré-modelagem)</span>"
    ]
  },
  {
    "objectID": "ap_tratamento.html#glossario",
    "href": "ap_tratamento.html#glossario",
    "title": "17  Tratamento de dados para regressão (pré-modelagem)",
    "section": "17.12 Glossário",
    "text": "17.12 Glossário\nEncoding (codificação de caracteres)\nComo o computador guarda letras com acentos e símbolos. Exemplos: UTF‑8, latin‑1. Se o encoding está errado, aparecem “�” ou letras quebradas.\nDelimitador / Separador\nSímbolo que separa colunas em arquivos de texto. Exemplos: vírgula ,, ponto e vírgula ;, tab .\nDecimal (símbolo decimal)\nO símbolo que separa parte inteira e fracionária. Em pt‑BR, vírgula (ex.: 3,14); em en‑US, ponto (3.14).\nCSV, XLSX, SAV, DTA\nFormatos de planilha/tabela. CSV: texto simples; XLSX: Excel; SAV: SPSS; DTA: Stata.\nLicença (de uso dos dados)\nCondições legais de uso/compartilhamento do dataset (ex.: CC‑BY). Leia antes de usar.\nFaltante / Dados faltantes\nInformação ausente em uma célula. Pode aparecer como NA, NaN, vazio \"\" ou códigos como 999.\nMCAR, MAR, MNAR\nTipos de mecanismo de ausência: MCAR (ausência completamente ao acaso), MAR (ao acaso condicional a outras variáveis), MNAR (não ao acaso; depende do próprio valor ausente).\nDuplicatas e chaves\nDuplicata: linha repetida. Chave: coluna (ou combinação) que identifica exclusivamente cada linha (ex.: id).\nLog de duplicatas / Log de tratamento\nRegistro simples do que foi removido/alterado e por quê. Ajuda na reprodutibilidade.\nOutlier\nValor muito fora do padrão do conjunto. Pode ser erro, evento raro ou caso especial.\nWinsorização (winsorize)\nTécnica que limita valores extremos a um limite (ex.: truncar no percentil 1% e 99%) para reduzir impacto de outliers.\nReescala / Padronização (z‑score)\nColocar variáveis em escala comparável. z‑score: subtrai a média e divide pelo desvio‑padrão (fica média 0 e dp 1). min–max: leva para [0,1] pela fórmula (x−min)/(max−min).\nCardinalidade (de categorias)\nNúmero de níveis distintos de uma variável categórica. Alta cardinalidade = muitos níveis.\nPadronizar rótulos\nEscrever categorias de forma consistente (ex.: tudo minúsculo, sem acento, sem espaços extras), unificando sinônimos.\nDummies (one‑hot)\nTransformar uma variável categórica em colunas 0/1 (uma coluna a menos que o número de categorias, para evitar colinearidade perfeita).\nVariável ordinal\nCategorias que têm ordem (ex.: fundamental &lt; médio &lt; superior).\nZeros estruturais\nZeros esperados por construção (ex.: aluguel de bicicletas à 03h pode ser zero). Diferem de “zeros por acaso”.\nRegularização (Ridge, LASSO)\nTécnicas que penalizam coeficientes para lidar com muitas variáveis e reduzir sobreajuste; exigem atenção à escala dos preditores.\nJSON\nFormato de texto para dados estruturados em pares chave:valor (não será foco aqui; usamos tabelas).\nPipeline\nSequência organizada de passos de tratamento: leitura → limpeza → transformação → verificação → saída tratada.\n\n\n\n\nBarnett, Vic, e Toby Lewis. 1994. Outliers in Statistical Data. 3º ed. Chichester: Wiley.\n\n\nBuuren, Stef van, e Karin Groothuis-Oudshoorn. 2024. mice: Multivariate Imputation by Chained Equations. CRAN. https://cran.r-project.org/package=mice.\n\n\nCharnet, Reinaldo, Carlos Alberto Freire, Eliane M. R. Charnet, e Helio Bonvino. 2008. Análise de Modelos de Regressão Linear com Aplicações. 2º ed. Campinas: EDUNICAMP.\n\n\nFox, John, e Sanford Weisberg. 2024. vif: Variance Inflation Factors. car package documentation. https://search.r-project.org/CRAN/refmans/car/html/vif.html.\n\n\nIannone, Richard. 2024. pointblank: Data Validation. R package documentation. https://rstudio.github.io/pointblank/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, e Robert Tibshirani. 2013. An Introduction to Statistical Learning with Applications in R. New York: Springer.\n\n\nKuhn, Max et al. 2024. step_impute_mean: Imputation Steps. recipes package documentation. https://recipes.tidymodels.org/reference/step_impute_mean.html.\n\n\nLittle, Roderick J. A., e Donald B. Rubin. 2019. Statistical Analysis with Missing Data. 3º ed. Hoboken: Wiley.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, e G. Geoffrey Vining. 2021. Introduction to Linear Regression Analysis. 6º ed. Hoboken: John Wiley & Sons.\n\n\nPaula, Gilberto A. 2004. Modelos de Regressão com Apoio Computacional. São Paulo: IME-USP.\n\n\nSearle, Shayle R. 2016. Matrix Algebra Useful for Statistics. 2º ed. Hoboken: Wiley.\n\n\nSheather, Simon J. 2009. A Modern Approach to Regression with R. New York: Springer.\n\n\nTufte, Edward R. 2001. The Visual Display of Quantitative Information. 2º ed. Cheshire: Graphics Press.\n\n\nWickham, Hadley et al. 2024. Missing values. tidyr package documentation. https://tidyr.tidyverse.org/articles/missing-values.html.",
    "crumbs": [
      "Parte IV — Apêndices",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Tratamento de dados para regressão (pré-modelagem)</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referências",
    "section": "",
    "text": "Abadir, Karim M., and Jan R. Magnus. 2005. Matrix Algebra.\nCambridge: Cambridge University Press.\n\n\nAkaike, Hirotugu. 1974. “A New Look at the Statistical Model\nIdentification.” IEEE Transactions on Automatic Control\n19 (6): 716–23.\n\n\nAnderson, T. W. 2003. An Introduction to Multivariate Statistical\nAnalysis. 3rd ed. New York: Wiley.\n\n\nBarnett, Vic, and Toby Lewis. 1994. Outliers in Statistical\nData. 3rd ed. Chichester: Wiley.\n\n\nBelsley, David A., Edwin Kuh, and Roy E. Welsch. 1980. Regression\nDiagnostics: Identifying Influential Data and Sources of\nCollinearity. New York: John Wiley & Sons.\n\n\nBurnham, Kenneth P., and David R. Anderson. 2002. Model Selection\nand Multimodel Inference: A Practical Information-Theoretic\nApproach. 2nd ed. New York: Springer.\n\n\nBuuren, Stef van, and Karin Groothuis-Oudshoorn. 2024. Mice:\nMultivariate Imputation by Chained Equations. CRAN. https://cran.r-project.org/package=mice.\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical\nInference. 2nd ed. Pacific Grove: Duxbury.\n\n\nCharnet, Reinaldo, Carlos Alberto Freire, Eliane M. R. Charnet, and\nHelio Bonvino. 2008. Análise de Modelos de Regressão Linear Com\nAplicações. 2nd ed. Campinas: EDUNICAMP.\n\n\nDraper, Norman R., and Harry Smith. 1998. Applied Regression\nAnalysis. 3rd ed. New York: John Wiley & Sons.\n\n\nFox, John, and Sanford Weisberg. 2024. Vif: Variance Inflation\nFactors. car package documentation. https://search.r-project.org/CRAN/refmans/car/html/vif.html.\n\n\nGalton, Francis. 1886a. “Family Likeness in Stature.”\nProceedings of the Royal Society of London 40: 42–72.\n\n\n———. 1886b. “Regression Towards Mediocrity in Hereditary\nStature.” Journal of the Anthropological Institute 15:\n246–63.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki\nVehtari, and Donald B. Rubin. 2014. Bayesian Data Analysis. 3rd\ned. CRC Press.\n\n\nGiordano, Frank R., William P. Fox, and Steven B. Horton. 2013. A\nFirst Course in Mathematical Modeling. Cengage Learning.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix\nComputations. 4th ed. Baltimore: Johns Hopkins University Press.\n\n\nGujarati, Damodar N. 2006. Econometria Básica. 4th ed. Rio de\nJaneiro: Elsevier Campus.\n\n\nHarville, David A. 1997. Matrix Algebra from a Statistician’s\nPerspective. New York: Springer.\n\n\n———. 2000. Matrix Algebra from a Statistician’s Perspective.\nNew York: Springer.\n\n\nHoffmann, Rodolfo. 2006. Análise de Regressão: Uma Introdução à\nEconometria. 2nd ed. São Paulo: Hucitec.\n\n\n———. 2016. Análise de Regressão: Uma Introdução à Econometria.\n5th ed. Portal de Livros Abertos da USP. https://doi.org/10.11606/9788592105709.\n\n\nIannone, Richard. 2024. Pointblank: Data Validation. R package\ndocumentation. https://rstudio.github.io/pointblank/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2013. An Introduction to Statistical Learning with Applications in\nr. New York: Springer.\n\n\nKuhn, Max et al. 2024. Step_impute_mean: Imputation Steps.\nrecipes package documentation. https://recipes.tidymodels.org/reference/step_impute_mean.html.\n\n\nKutner, Michael H., Christopher J. Nachtsheim, John Neter, and William\nLi. 2005. Applied Linear Statistical Models. 5th ed. New York:\nMcGraw-Hill.\n\n\nLittle, Roderick J. A., and Donald B. Rubin. 2019. Statistical\nAnalysis with Missing Data. 3rd ed. Hoboken: Wiley.\n\n\nMeerschaert, Mark M. 2013. Mathematical Modeling. 4th ed.\nAcademic Press.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, and G. Geoffrey Vining. 2021.\nIntroduction to Linear Regression Analysis. 6th ed. Hoboken:\nJohn Wiley & Sons.\n\n\nPaula, Gilberto A. 2004. Modelos de Regressão Com Apoio\nComputacional. São Paulo: IME-USP.\n\n\nPearson, Karl, and Alice Lee. 1903. “On the Laws of\nInheritance.” Biometrika 2: 357–462.\n\n\nRencher, Alvin C., and William F. Christensen. 2012. Methods of\nMultivariate Analysis. 3rd ed. Hoboken: Wiley.\n\n\nSchwarz, Gideon. 1978. “Estimating the Dimension of a\nModel.” The Annals of Statistics 6 (2): 461–64.\n\n\nSearle, Shayle R. 2016. Matrix Algebra Useful for Statistics.\n2nd ed. Hoboken: Wiley.\n\n\nSheather, Simon J. 2009. A Modern Approach to Regression with\nr. New York: Springer.\n\n\nTufte, Edward R. 2001. The Visual Display of Quantitative\nInformation. 2nd ed. Cheshire: Graphics Press.\n\n\nWeisberg, Sanford. 2005. Applied Linear Regression. New York:\nWiley.\n\n\nWickham, Hadley et al. 2024. Missing Values. tidyr package\ndocumentation. https://tidyr.tidyverse.org/articles/missing-values.html.",
    "crumbs": [
      "Referências"
    ]
  }
]