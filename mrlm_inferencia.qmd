# Inferência Estatística no MRLM

## Base inferencial do MRLM Normal

Com os parâmetros estimados por MQO, passamos do **ajuste** para a **inferência**. Em regressão múltipla, três perguntas organizam quase toda a prática inferencial. 

- A primeira é sobre **efeitos individuais**: dado todo o restante do modelo, o coeficiente de $x_j$ é ou não é compatível com a suposição de ser igual à zero? Essa é a leitura “condicional” de significância, que isola o efeito marginal de cada preditor. 

- A segunda trata de **efeitos conjuntos**: grupos de variáveis, por exemplo, um bloco de dummies ou um conjunto de indicadores macroeconômicos, acrescentam explicação estatisticamente relevante quando considerados em conjunto? 

- Por fim, a terceira diz respeito à **precisão e previsão**: quão incertos são os coeficientes estimados, quão precisa é a média condicional $E(Y\mid x_0)$ em um novo ponto $x_0$, e quão amplo deve ser um intervalo para uma **nova observação** $Y_0$ gerada sob as mesmas condições?

Para responder a essas perguntas, trabalharemos no escopo do **MRLM Normal**, em que
$$
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon},\qquad
\boldsymbol{\varepsilon}\sim N_n(\mathbf{0},\sigma^2 I_n),
$$
e usaremos os resultados de projeção $\hat{\mathbf{Y}}=H\mathbf{Y}$ e $\hat{\boldsymbol{\varepsilon}}=(I_n-H)\mathbf{Y}$ para construir estatísticas $t$ e $F$, bem como intervalos de confiança e de predição com validade **exata** em amostras finitas. Nas subseções seguintes, desenvolvemos cada uma dessas peças com cuidado, sempre enfatizando a interpretação **condicional** típica do MRLM.


Para organizar o raciocínio, passamos a trabalhar com o **Modelo de Regressão Linear Múltipla Normal (MRLM Normal)**, que acrescenta a suposição de distribuição normal para os erros aleatórios:  

$$
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}, \qquad
\boldsymbol{\varepsilon}\sim N_n(\mathbf{0},\,\sigma^2 I_n).
$$  

Sob esse o MRLM Normal, temos que:  

- O vetor de respostas segue uma distribuição normal multivariada ($n-$variada):  
  $$
  \mathbf{Y}\sim N_n(\mathbf{X}\boldsymbol{\beta},\,\sigma^2 I_n).
  $$  

- O estimador de mínimos quadrados é:  
  $$
  \hat{\boldsymbol{\beta}}=(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{Y},
  $$
  e os valores ajustados e resíduos podem ser escritos como:  
  $$
  \hat{\mathbf{Y}}=\mathbf{H}\mathbf{Y}\qquad \text{e} \qquad
  \hat{\boldsymbol{\varepsilon}}=\mathbf{M}\mathbf{Y},
  $$
em que $\mathbf{H}=\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top$ é a matriz chapéu ("hat") e $\mathbf{M}=\mathbf{I}_n-\mathbf{H}$ é a matriz dos resíduos. Essas matrizes são simétricas, idempotentes e ortogonais entre si.

De forma expandida, a matriz $\mathbf{H}$ pode ser escrita como
$$
\mathbf{H} =
\begin{bmatrix}
h_{11} & h_{12} & \cdots & h_{1n} \\
h_{21} & h_{22} & \cdots & h_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
h_{n1} & h_{n2} & \cdots & h_{nn}
\end{bmatrix},
\qquad
h_{ij} = \mathbf{x}_i^\top(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{x}_j,
$$
onde $\mathbf{x}_i^\top$ é a $i$-ésima linha de $\mathbf{X}$ (observação $i$).

Analogamente, a matriz $\mathbf{M}$ é
$$
\mathbf{M} =
\begin{bmatrix}
m_{11} & m_{12} & \cdots & m_{1n} \\
m_{21} & m_{22} & \cdots & m_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
m_{n1} & m_{n2} & \cdots & m_{nn}
\end{bmatrix},
\qquad
m_{ij} = \delta_{ij} - h_{ij},
$$
em que $\delta_{ij}$ é o delta de Kronecker. Temos também que:

- As **diagonais de $\mathbf{H}$**, $h_{ii}$, são chamadas de **alavancas** (*leverages*):  
  $$
  h_{ii} = \mathbf{x}_i^\top(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{x}_i,
  $$
  e medem a influência potencial da observação $i$ sobre seu próprio ajustado.  

- As diagonais de $\mathbf{M}$ são $m_{ii}=1-h_{ii}$ e, por construção,  
  $$
  \sum_{i=1}^n h_{ii} = \operatorname{tr}(\mathbf{H}) = p+1,
  \qquad
  \sum_{i=1}^n m_{ii} = \operatorname{tr}(\mathbf{M}) = n-p-1.
  $$  

Assim como $\mathbf{X}$ organiza as covariáveis, $\mathbf{H}$ e $\mathbf{M}$ organizam as projeções em blocos matriciais, sendo peças centrais para interpretar e calcular ajustados, resíduos e diagnósticos no MRLM.


Desses resultados seguem três propriedades fundamentais:  

1. **Estimadores dos coeficientes normalmente distribuiídos**  
   $$
   \hat{\boldsymbol{\beta}} \mid \mathbf{X} \;\sim\; N_{p+1}\!\big(\boldsymbol{\beta},\,\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\big).
   $$  
   Ou seja, cada coeficiente ajustado é normal em torno do valor verdadeiro, com variância que depende da variabilidade dos erros e da estrutura da matriz de regressoras.  

2. **O quociente da soma de quadrados dos resíduos pela variância segue uma distribuição Qui-Quadrado**  
   $$
   SQ_{Res}=\hat{\boldsymbol{\varepsilon}}^\top\hat{\boldsymbol{\varepsilon}}=\mathbf{Y}^\top \mathbf{M}\mathbf{Y},
   $$
   que, sob o MRLM Normal, segue uma distribuição **qui-quadrado com $n-p$ graus de liberdade** após divisão por $\sigma^2$:  
   $$
   \frac{SQ_{Res}}{\sigma^2}\sim \chi^2_{n-p-1}.
   $$  

3. **O vetor de estimadores dos coeficientes e o estimador da variância dos erros são independentes**  
   O vetor $\hat{\boldsymbol{\beta}}$ e a soma de quadrados dos resíduos $SQ_{Res}$ são estatisticamente **independentes**. Esse resultado decorre da ortogonalidade das projeções em $\operatorname{col}(\mathbf{X})$ e seu complemento.  


Esses três pilares fornecem a base para a inferência estatística no MRLM. É graças a eles que conseguimos:  

- Construir **estatísticas $t$ e $F$ com distribuições exatas em amostras finitas** para testar restrições nos coeficientes;  
- Formular **intervalos de confiança** para os coeficientes e para a média condicional;  
- Estabelecer **intervalos de predição** para novas observações da variavel resposta.  

É importante lembrar que todas as conclusões devem ser interpretadas **condicionalmente às demais variáveis explicativas** incluídas no modelo — isto é, no sentido de **efeito marginal no MRLM**. Em particular, dizer que um efeito é “significativo” em regressão múltipla significa que, **mantidos os outros preditores**, a variação em $x_j$ está associada a uma mudança média em $Y$ que dificilmente se explica apenas pelo acaso. Isso difere da correlação simples entre $Y$ e $x_j$, que ignora o controle das demais variáveis.



## Testes individuais para coeficientes

O primeiro passo da inferência no MRLM é avaliar, para **cada** coeficiente, se há evidência de que o seu valor populacional difere de zero **condicionalmente às demais variáveis**. Em outras palavras, perguntamos: *“dado tudo o que já está no modelo, a variável $x_j$ ainda acrescenta um efeito linear sobre a média de $Y$?”* 

Formalmente, a hipótese testada é
$$
H_0:\ \beta_j=0
\qquad\text{vs.}\qquad
H_1:\ \beta_j\neq 0,
$$
para algum $j\in\{0,1,\dots,p-1\}$ (admitindo o intercepto na primeira coluna de $\mathbf{X}$). Para o intercepto, o raciocínio é análogo, ainda que a interpretação substantiva costume ser distinta.

 
Embora o teste $t$ para $j=0$ siga a mesma fórmula e tenha exatamente a mesma distribuição sob $H_0$ que os demais coeficientes, sua interpretação é particular. O parâmetro $\beta_0$ corresponde ao valor esperado de $Y$ quando todas as variáveis explicativas são iguais a zero. Isso pode ser informativo **em alguns contextos** (quando zero é um ponto de referência natural, como “nenhum ano de escolaridade”) ou **pouco relevante em outros** (quando zero não faz sentido, como “peso corporal zero”). 

Uma prática comum em análise aplicada é **centrar** as variáveis explicativas (subtraindo suas médias), de modo que $\beta_0$ represente a **média condicional** de $Y$ no ponto central do espaço das covariáveis. Essa estratégia melhora a interpretação substantiva e reduz correlações artificiais entre intercepto e demais coeficientes, **sem alterar** testes, previsões ou a qualidade do ajuste.

**Como construir a estatística $t$.**  
Considere $c_{jj}$ o elemento $j$-ésimo da diagonal de $(\mathbf{X}^\top\mathbf{X})^{-1}$. Sob o MRLM Normal, vale
$$
\operatorname{Var}(\hat{\beta}_j\mid \mathbf{X})=\sigma^2 c_{jj}.
$$
Como $\sigma^2$ é desconhecido, estimamos por
$$
\hat{\sigma}^2=\frac{SQ_{Res}}{n-p-1}
=\frac{\mathbf{Y}^\top(\mathbf{I}_n-\mathbf{H})\mathbf{Y}}{n-p-1},
\qquad \text{logo} \qquad 
\operatorname{EP}(\hat{\beta}_j)=\hat{\sigma}\sqrt{c_{jj}}.
$$
A estatística de teste é então
$$
t_j=\frac{\hat{\beta}_j-0}{\hat{\sigma}\sqrt{c_{jj}}}.
$$
Se $H_0$ é verdadeira e as hipóteses do MRLM Normal são satisfeitas, então $t_j\sim t_{n-p-1}$ (em amostra finita).

O denominador da estatística $t_j$ corresponde ao erro-padrão da estimativa de $\beta_j$, que mede a incerteza associada ao coeficiente e reflete a estrutura geométrica de $\mathbf{X}$ por meio de $c_{jj}$, elemento de $(\mathbf{X}^\top\mathbf{X})^{-1}$. Quando a variável $x_j$ apresenta forte colinearidade com outras colunas de $\mathbf{X}$, essa matriz torna-se mal-condicionada, inflando $c_{jj}$ e, consequentemente, o erro-padrão; isso reduz a estatística $t_j$, diminui o poder do teste e torna mais difícil rejeitar $H_0$, mesmo quando há de fato um efeito real de $x_j$ sobre a resposta.

**Como interpretar e tomar decisão.**  

No uso mais comum, aplicamos um teste **bicaudal** e rejeitamos $H_0$ se $|t_j|>t_{\alpha/2,\,n-p-1}$ (ou se o valor-$p<\alpha$). É essencial reforçar que esse teste é **condicional**: ele avalia o efeito de $x_j$ **mantendo fixas as demais variáveis do modelo**. Essa leitura é distinta da regressão simples entre $Y$ e $x_j$, na qual não há controle para os outros preditores. Na regressão simples, se $x_j$ estiver correlacionado com variáveis omitidas que também influenciam $Y$, o efeito estimado pode aparecer **inflado** (atribuindo a $x_j$ parte da variação que na verdade pertence a outro preditor) ou **atenuado** (subestimando sua contribuição real). 

No MRLM, ao contrário, o teste $t$ busca isolar o **efeito marginal** de $x_j$, garantindo que a conclusão sobre sua significância estatística esteja condicionada ao conjunto completo de variáveis incluídas no modelo.

**Testes individuais gerais e relação com a distribuição $F$.**  

Em testes mais gerais, podemos estar interessados em testar a hipótese de $\beta_j$ ser igual a um valor fixo e conhecido $d_j$. Neste caso, temos
$$
H_0:\ \beta_j=d_j
\qquad\text{vs.}\qquad
H_1:\ \beta_j\neq d_j,
$$
e a estatística de teste é
$$
t_j=\frac{\hat{\beta}_j-d_j}{\hat{\sigma}\sqrt{c_{jj}}}.
$$
Elevando ao quadrado,
$$
F_j=t_j^2=\left(\frac{\hat{\beta}_j-d_j}{\hat{\sigma}\sqrt{c_{jj}}}\right)^2 
= \frac{(\hat{\beta}_j-d_j)\left(c_{jj}\right)^{-1}(\hat{\beta}_j-d_j)}{\widehat{\sigma^2}}.
$$
Sob $H_0$, a estatística $F_j$ segue uma distribuição **$F$** com $1$ e $n-p-1$ graus de liberdade, isto é, $F_j \sim F_{1,\,n-p-1}$. Essa equivalência ($t^2 \equiv F$ para $q=1$) será particularmente útil ao conectar testes individuais com **testes conjuntos** (Seção 3.3.2).

**Cuidados essenciais.**

- **Multicolinearidade** pode inflar $c_{jj}$, aumentar erros-padrão e reduzir poder, obscurecendo efeitos reais.  
- **Omissão de variáveis relevantes** pode gerar viés sistemático, alterando tanto estimativas quanto significância.  
- **Testes múltiplos** sem controle adequado aumentam risco de falsos positivos.  

Esses pontos reforçam a importância de complementar a análise com diagnóstico de resíduos (Seção 3.8) e discussões sobre seleção de modelos (Seção 3.6).

**Ideias de demonstração.**  
(i) De $\hat{\boldsymbol{\beta}}\mid \mathbf{X}\sim N_{p+1}(\boldsymbol{\beta},\,\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1})$ segue $\hat{\beta}_j\sim N(\beta_j,\sigma^2 c_{jj})$.  
(ii) $SQ_{Res}/\sigma^2=\mathbf{Y}^\top(\mathbf{I}_n-\mathbf{H})\mathbf{Y}/\sigma^2\sim \chi^2_{n-p-1}$ (posto de $\mathbf{M}$).  
(iii) Sob normalidade, projeções ortogonais nos subespaços de $\mathbf{H}$ e $\mathbf{M}$ são independentes; assim, $\hat{\beta}_j$ é independente de $SQ_{Res}$.  
(iv) O pivô
$$
\frac{\hat{\beta}_j-\beta_j}{\sigma\sqrt{c_{jj}}}\sim N(0,1),
\qquad
\frac{SQ_{Res}}{\sigma^2}\sim \chi^2_{n-p-1}\ \text{independentes}
$$
leva, ao substituir $\sigma$ por $\hat{\sigma}$, à distribuição $t_{n-p-1}$.


## Testes conjuntos e hipóteses lineares gerais

Enquanto os testes $t$ verificam hipóteses sobre **coeficientes individuais**, na prática frequentemente nos interessa saber se **um grupo de variáveis explicativas, em conjunto, exerce efeito significativo sobre a resposta**.  

Por exemplo: “Todas as variáveis indicadoras (_dummies_) de escolaridade, tomadas em bloco, têm efeito sobre o rendimento escolar?” ou “As variáveis macroeconômicas trazem explicação adicional além das variáveis demográficas?”.  

Essas perguntas exigem uma formulação mais ampla: o **teste de hipóteses lineares gerais**, que engloba todos os testes conjuntos de coeficientes no MRLM Normal.

**(a) Formulação geral.**  
A hipótese nula estabelece $q$ restrições lineares sobre os $p$ parâmetros (incluindo o intercepto $\beta_0$):
$$
H_0:\ C\boldsymbol{\beta}=\mathbf{d}
\qquad \text{vs.}\qquad
H_1:\ C\boldsymbol{\beta}\neq \mathbf{d},
$$
onde $C$ é uma matriz $q\times p$ de posto $q$ (cada linha representando uma restrição independente) e $\mathbf{d}$ é um vetor fixo de dimensão $q$.

Alguns exemplos ajudam a fixar a ideia:

- **Bloco nulo:**  
Queremos testar se um grupo de coeficientes é nulo, por exemplo  
$$
H_0:\ \beta_1=\beta_3=\beta_4=0.
$$  
Aqui $q=3$ e a matriz $C$ seleciona exatamente esses coeficientes:  
$$
C=
\begin{bmatrix}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1
\end{bmatrix}, 
\qquad 
\mathbf{d}=
\begin{bmatrix}
0 \\ 0 \\ 0
\end{bmatrix}.
$$  

Se o intercepto também estiver entre os parâmetros testados, por exemplo $H_0:\ \beta_0=\beta_2=\beta_4=0$, a matriz $C$ incluiria a primeira coluna:
$$
C=
\begin{bmatrix}
1 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1
\end{bmatrix}, 
\qquad 
\mathbf{d}=
\begin{bmatrix}
0 \\ 0 \\ 0
\end{bmatrix}.
$$  

- **Igualdade entre coeficientes:**  
Para testar se $\beta_1=\beta_2$, escrevemos a restrição como $\beta_1-\beta_2=0$. Assim:  
$$
C=
\begin{bmatrix}
0 & 1 & -1 & 0 & \cdots & 0
\end{bmatrix}, 
\qquad 
\mathbf{d}=
\begin{bmatrix}
0
\end{bmatrix}.
$$  

Se fossem várias igualdades, como $\beta_1=\beta_2=\beta_3$, teríamos duas restrições lineares:  
$$
H_0:\ \beta_1-\beta_2=0 \quad\text{e}\quad \beta_2-\beta_3=0,
$$
com
$$
C=
\begin{bmatrix}
0 & 1 & -1 & 0 & 0 & \cdots & 0 \\
0 & 0 & 1 & -1 & 0 & \cdots & 0
\end{bmatrix}, 
\qquad 
\mathbf{d}=
\begin{bmatrix}
0 \\ 0
\end{bmatrix}.
$$  

- **Contraste geral:**  
Qualquer combinação linear pode ser escrita. Por exemplo,  
$$
H_0:\ 2\beta_1+3\beta_2=5.
$$  
Aqui, $q=1$ e  
$$
C=
\begin{bmatrix}
0 & 2 & 3 & 0 & \cdots & 0
\end{bmatrix},
\qquad 
\mathbf{d}=
\begin{bmatrix}
5
\end{bmatrix}.
$$  

Dessa forma, fica claro que **$C$ sempre “seleciona” ou “combina” os coeficientes de interesse**, enquanto $\mathbf{d}$ representa o valor de referência esperado sob $H_0$. Essa formulação é suficientemente geral para abarcar desde hipóteses simples ($q=1$, equivalente ao teste $t$) até combinações complexas de parâmetros.

**(b) Estatística de Wald.**  
A construção da estatística de teste é uma generalização natural da lógica do teste $t$. Sob o MRLM Normal, temos
$$
F=\frac{\big(C\hat{\boldsymbol{\beta}}-\mathbf{d}\big)^\top
\Big[C(\mathbf{X}^\top\mathbf{X})^{-1}C^\top\Big]^{-1}
\big(C\hat{\boldsymbol{\beta}}-\mathbf{d}\big)/q}{\hat{\sigma}^2},
$$
com
$$
\hat{\sigma}^2=\frac{SQ_{Res}}{n-p-1}.
$$

Sob $H_0$:
$$
F \sim F_{q,\,n-p-1}.
$$

O numerador mede a “distância” entre $C\hat{\boldsymbol{\beta}}$ e $\mathbf{d}$, em unidades do erro-padrão multivariado da estimativa. Essa distância quadrática é normalizada por $q$, resultando em uma média quadrática comparada à média quadrática residual no denominador.  

Quando $q=1$, o teste se reduz ao caso individual: $t_j^2=F_{1,\,n-p-1}$. Para $q>1$, temos a generalização que permite testar blocos de parâmetros.

**(c) Versão por somas de quadrados (modelos aninhados).**  
Outra forma, bastante intuitiva, é comparar dois modelos aninhados:

- **Modelo completo (irrestrito):** inclui todas as colunas relevantes de $\mathbf{X}$.  
- **Modelo reduzido (restrito):** impõe $H_0$, seja removendo variáveis, seja impondo restrições lineares.  

Denotando por $SQ_R$ a soma de quadrados da regressão do modelo completo e $SQ_R^\ast$ a do modelo reduzido, obtemos
$$
F=\frac{(SQ_R - SQ_R^\ast)/q}{SQ_{Res}/(n-p-1)}.
$$

Ou seja, o ganho de explicação ao passar do modelo reduzido para o completo (ajustado por $q$) é confrontado com a variabilidade residual média do modelo completo.  

Em notação matricial:
$$
SQ_R = \mathbf{Y}^\top H\mathbf{Y}, \quad
SQ_R^\ast=\mathbf{Y}^\top H^\ast \mathbf{Y}, \quad
SQ_{Res}=\mathbf{Y}^\top M\mathbf{Y},
$$
em que $H$ e $H^\ast$ são os projetores associados aos modelos completo e reduzido. Logo,
$$
SQ_R-SQ_R^\ast=\mathbf{Y}^\top(H-H^\ast)\mathbf{Y},
$$
com $H-H^\ast$ projetando exatamente no subespaço “liberado” pelas $q$ restrições.

**(d) Casos particulares importantes.**

- **Teste global da regressão:**  
$$
H_0:\ \beta_1=\cdots=\beta_{p}=0 \qquad (q=p).
$$  
A estatística é
$$
F=\frac{SQ_R/p}{SQ_{Res}/(n-p-1)}.
$$
Pergunta-chave: “Pelo menos um preditor (além do intercepto) explica $Y$?”.  

- **Bloco de dummies (ANCOVA):** verificar se indicadores de grupo, em conjunto, têm efeito significativo.  

- **Igualdade de parâmetros:** hipóteses como $\beta_2=\beta_3$ são formuladas diretamente via matriz $C$, sem precisar modificar os dados.

**(e) Como interpretar e decidir.**  
A decisão segue a regra padrão: rejeitamos $H_0$ se $F>F_{\alpha;\,q,n-p-1}$ ou se o valor-$p$ for menor que $\alpha$.  

Valores grandes de $F$ indicam que o bloco de variáveis contribui significativamente para explicar $Y$, **condicionalmente** às demais já presentes no modelo. Em relatórios, é importante detalhar:  
- o valor de $F$;  
- os graus de liberdade $(q,n-p-1)$;  
- o valor-$p$;  
- e, sobretudo, **quais restrições foram testadas** (traduzindo $C$ e $\mathbf{d}$ em termos aplicados).

**(f) $R^2$ parcial e medidas de efeito.**  
Para um bloco de $q$ variáveis, defina
$$
R^2_{\text{parcial}}=\frac{SQ_R - SQ_R^\ast}{SQ_T - SQ_R^\ast}.
$$
Ele mede a fração da variabilidade **remanescente** (depois do modelo reduzido) que o bloco adicional explica. Relação com $F$:
$$
F=\frac{R^2_{\text{parcial}}/q}{(1-R^2_{\text{parcial}})/(n-p-1)}.
$$
Em relatórios, $R^2_{\text{parcial}}$ comunica **magnitude** (tamanho de efeito conjunto), complementando a significância. (Para $q=1$, $R^2_{\text{parcial}}$ coincide com o quadrado da correlação parcial entre $Y$ e $x_j$ dado o restante.)

**(g) Pressupostos e robustez.**  

- **Exatidão em amostras finitas:** sob MRLM Normal, $F_{q,n-p-1}$ é exato.  
- **Assintótico:** sem normalidade, o teste é válido para $n$ grande.  
- **Heterocedasticidade:** a variância clássica pode falhar; nesse caso, usa-se a versão robusta (HC) da matriz-sanduíche (White, 1980).  
- **Dependência temporal:** em séries temporais, erros-padrão HAC (Newey–West, 1987) ou modelos GLS são alternativas adequadas.  

**(h) Ideias de demonstração.**  
1. Sob $H_0$, $C\hat{\boldsymbol{\beta}}-\mathbf{d}\mid \mathbf{X}\sim N_q(0,\ \sigma^2 C(\mathbf{X}^\top\mathbf{X})^{-1}C^\top)$.  
2. A forma quadrática correspondente segue $\chi^2_q$.  
3. Como é independente de $SQ_{Res}/\sigma^2\sim \chi^2_{n-p-1}$, o quociente leva a $F_{q,n-p-1}$.  
4. A versão em somas de quadrados decorre da decomposição de projeções: $SQ_R-SQ_R^\ast=\mathbf{Y}^\top(H-H^\ast)\mathbf{Y}$.  
 


## ANOVA em regressão múltipla e teste global

A análise de variância (ANOVA) em regressão múltipla organiza a variabilidade da resposta $Y$ em duas parcelas: a **parte explicada** pelo modelo (regressão) e a **parte não explicada** (resíduos). Essa decomposição, além de oferecer um retrato sintético do ajuste, fornece a base para o **teste global** do modelo. 

Em termos práticos, trata-se de avaliar se *pelo menos um* coeficiente (além do intercepto) é diferente de zero, justificando o uso do modelo de regressão em vez de simplesmente usar a média $\bar{y}$ como preditor.

**(a) Decomposição da soma de quadrados (forma escalar).**  
Seja $\bar{y}=\tfrac{1}{n}\sum_{i=1}^n y_i$. A variabilidade total da resposta em torno da média pode ser decomposta em:

$$
SQ_T=\sum_{i=1}^n (y_i-\bar{y})^2,\qquad
SQ_R=\sum_{i=1}^n (\hat{y}_i-\bar{y})^2,\qquad
SQ_{Res}=\sum_{i=1}^n (y_i-\hat{y}_i)^2.
$$

A identidade fundamental
$$
SQ_T = SQ_R + SQ_{Res}
$$
mostra que a variabilidade total se “quebra” em variabilidade **explicada pelo modelo** e variabilidade **residual**.

**(b) Forma matricial e papel das projeções.**  
Essa decomposição se expressa elegantemente em notação matricial. 

Definindo $J=\mathbf{1}\mathbf{1}^\top/n$ (projeção no subespaço gerado pelo vetor de uns), $H=\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top$ (matriz chapéu) e $M=I_n-H$, temos:

$$
SQ_T=\mathbf{Y}^\top(I_n-J)\mathbf{Y},\qquad
SQ_R=\mathbf{Y}^\top(H-J)\mathbf{Y},\qquad
SQ_{Res}=\mathbf{Y}^\top M\mathbf{Y}.
$$

Como $I_n-J=(H-J)+M$ e os subespaços de $(H-J)$ e $M$ são ortogonais, segue a decomposição $SQ_T=SQ_R+SQ_{Res}$.  

Geometricamente, $(H-J)\mathbf{Y}$ é a parte de $\mathbf{Y}$ explicada pelas colunas de $\mathbf{X}$ além da constante, enquanto $M\mathbf{Y}$ é o vetor de resíduos, ortogonal ao espaço coluna de $\mathbf{X}$.

**(c) Graus de liberdade e médias quadráticas.**  
Com intercepto no modelo, temos $\operatorname{rank}(\mathbf{X})=p+1$:

- $SQ_T$: $gl_T=n-1$  
- $SQ_R$: $gl_R=p$ (coeficientes além do intercepto)  
- $SQ_{Res}$: $gl_{Res}=n-p-1$  

As **médias quadráticas** são definidas como 
$$
MQ_R=\frac{SQ_R}{p} \qquad \text{e} \qquad MQ_{Res}=\frac{SQ_{Res}}{(n-p-1)},
$$
sendo a comparação entre ambas a essência do teste global.

**(d) Teste global da regressão (estatística $F$).**  
Hipóteses:
$$
H_0:\ \beta_1=\cdots=\beta_{p}=0
\qquad\text{vs.}\qquad
H_1:\ \text{pelo menos um }\beta_j\neq 0.
$$

A estatística de teste é:
$$
F=\frac{MQ_R}{MQ_{Res}}=\frac{SQ_R/p}{SQ_{Res}/(n-p-1)}.
$$

Sob o MRLM Normal:
$$
F \sim F_{p,\;n-p-1}.
$$

**Interpretação:** se $F$ é grande (ou valor-$p$ pequeno), concluímos que a variabilidade explicada pelo modelo é significativamente maior do que aquela explicada apenas pela média, justificando o uso do modelo de regressão múltipla.

**(e) Relação com $R^2$ e tamanho do modelo.**  
O coeficiente de determinação é:
$$
R^2=\frac{SQ_R}{SQ_T}.
$$

Em termos de $R^2$, a estatística $F$ global se escreve:
$$
F=\frac{R^2/p}{(1-R^2)/(n-p-1)}.
$$

Essa forma deixa claro que o $F$ global leva em conta não apenas o tamanho de $R^2$, mas também o **número de preditores** e o **tamanho da amostra**. A mesma elevação em $R^2$ terá peso menor se $p$ for grande ou se $n$ for pequeno, funcionando como uma penalização implícita. Para comparações entre modelos com tamanhos diferentes, medidas como $\bar{R}^2$, AIC ou BIC (ver Seção 3.6) são mais apropriadas.

**(f) Consistência com os testes conjuntos.**  
O teste global é um caso particular dos testes lineares gerais (Seção 3.3.2), em que $C$ é construído para anular **todos os coeficientes não constantes**.  

Quando $p=2$ (uma única variável explicativa além do intercepto), a equivalência $F_{1,n-2}=t_{n-2}^2$ mostra a coerência entre o teste global e o teste $t$ individual.

**(g) Questões práticas e variações.**  
- **Modelo sem intercepto:** se o intercepto é omitido, a decomposição padrão muda, e a interpretação de $SQ_R$ e $R^2$ deve ser ajustada.  

- **Multicolinearidade:** mesmo que testes $t$ individuais não detectem significância, o $F$ global pode ser significativo, pois avalia a relevância **conjunta** dos preditores.

**(h) O que o $F$ global testa — e o que não testa.**  
- O $F$ global responde: “Vale a pena usar o modelo como um todo?”.  
- Ele **não** diz quais variáveis individuais são relevantes; para isso, usamos os testes $t$ e os testes conjuntos direcionados.  
- É possível ter um $F$ global significativo com todos os testes $t$ não significativos (quando há multicolinearidade), ou o inverso em amostras pequenas.

**(i) Ideias de demonstração.**  
1. Sob MRLM Normal, $(H-J)\mathbf{Y}$ e $M\mathbf{Y}$ são projeções ortogonais, logo independentes.  
2. Portanto,
$$
\frac{SQ_R}{\sigma^2}=\frac{\mathbf{Y}^\top(H-J)\mathbf{Y}}{\sigma^2}\sim \chi^2_{p},\qquad
\frac{SQ_{Res}}{\sigma^2}=\frac{\mathbf{Y}^\top M\mathbf{Y}}{\sigma^2}\sim \chi^2_{n-p-1},
$$
independentes.  
3. O quociente das médias quadráticas leva a $F_{p,n-p-1}$.  
4. A identidade $SQ_T=SQ_R+SQ_{Res}$ segue de $I_n-J=(H-J)+M$ e da ortogonalidade dos projetores.

## Intervalos de confiança para coeficientes e região conjunta

Nas seções anteriores abordamos **testes de hipóteses** sobre os coeficientes do MRLM.  Outra forma de inferência, muitas vezes preferida em relatórios e publicações científicas, é a construção de **intervalos de confiança**, pois eles expressam não apenas *se* o efeito é estatisticamente distinto de zero, mas também **quanto** esse efeito pode variar de forma plausível.

Enquanto os testes se concentram em uma decisão pontual (“rejeitar” ou “não rejeitar” $H_0$), os intervalos apresentam uma **faixa de valores compatíveis com os dados**, permitindo interpretações mais ricas sobre magnitude, direção e incerteza.


### Intervalo individual para um coeficiente

Para a construção de intervalos de confianças individuais (marginais) para $\beta_j$, é necessário ter uma estimativa da variâcia do estimador de $\beta_j$, ou seja, $Var(\hat{\beta}_j)$. Para este fim, considere $c_{jj}$ o elemento $j$-ésimo da diagonal de $\mathbf{C} =(\mathbf{X}^\top\mathbf{X})^{-1}$.  Deste modo, a variância teórica de $\hat{\beta}_j$ é $\sigma^2 c_{jj}$, a qual é estimada empiricamente por $\hat{\sigma}^2 c_{jj}$, onde

$$
\hat{\sigma}^2 = \frac{SQ_{Res}}{n-p-1}
= \frac{\mathbf{Y}^\top \mathbf{M}\mathbf{Y}}{n-p-1}.
$$

Assim, pela distribuição amostral de $\hat{\beta}_j$, o intervalo de confiança $(1-\alpha)100\%$ para o parâmetro $\beta_j$ é dado por

$$
IC_{1-\alpha}(\beta_j) = \Big( \hat{\beta}_j \;-\; t_{\alpha/2,\,n-p-1}\;\hat{\sigma}\sqrt{c_{jj}} \ ; \ \hat{\beta}_j \;+\; t_{\alpha/2,\,n-p-1}\;\hat{\sigma}\sqrt{c_{jj}} \Big).
$$

Esse intervalo indica a **faixa de valores plausíveis** para $\beta_j$ considerando a incerteza da amostra.  Por exemplo, se o intervalo inclui zero, isso sugere que o efeito de $x_j$ pode não ser estatisticamente diferente de nulo. No entanto, essa conclusão só pode ser obtida com o teste de Hipóteses. 

Adicionalmente, cada intervalo individual pode ser visto como a projeção, sobre o eixo de $\beta_j$, da região conjunta de confiança em $\mathbb{R}^{p+1}$, apresentada a seguir.


### Intervalos simultâneos e região conjunta

Quando desejamos considerar simultaneamente todos os coeficientes do vetor $\boldsymbol{\beta}$, a generalização natural é a **região de confiança conjunta elipsoidal**, definida por:

$$
\frac{(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})^\top
(\mathbf{X}^\top\mathbf{X})
(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})}{\hat{\sigma}^2}
\ \le\ (p+1)\,F_{\alpha;\,p+1,\,n-p-1}.
$$

Essa desigualdade descreve um **elipsoide centrado em $\hat{\boldsymbol{\beta}}$**, cujos eixos e orientação são determinados pela estrutura de covariância $(\mathbf{X}^\top\mathbf{X})^{-1}$. Dentro desse elipsoide encontram-se, com probabilidade $(1-\alpha)$, todos os vetores de parâmetros $\boldsymbol{\beta}$ plausíveis à luz dos dados.

Enquanto os intervalos individuais assumem implicitamente independência entre parâmetros, a **região conjunta** leva em conta as **covariâncias** entre os estimadores. Essa dependência torna-se crucial em situações de **multicolinearidade**, nas quais o elipsoide se alonga em direções inclinadas, refletindo que combinações compensatórias de coeficientes podem produzir o mesmo ajuste.

Geometricamente, o elipsoide de confiança conjunta é a contrapartida, no espaço dos parâmetros, do princípio usado na ANOVA: comparar a variabilidade explicada (ajuste do modelo) com a variabilidade residual, agora em dimensão $(p+1)$ e sob uma métrica multivariada.


### Correções simultâneas e observações práticas  

Quando construímos **intervalos de confiança individuais** para cada coeficiente $\beta_j$, cada um deles tem nível de confiança nominal $(1 - \alpha)$.  
Entretanto, ao interpretarmos **vários intervalos ao mesmo tempo**, a probabilidade de que **pelo menos um** deles não contenha o verdadeiro valor do parâmetro aumenta — é o chamado **erro tipo I global**.  

Para controlar esse erro em análises com múltiplos parâmetros, utiliza-se a **correção de Bonferroni**.  
A ideia é simples: se queremos que a probabilidade conjunta de erro tipo I não ultrapasse $\alpha$, ajustamos o nível de significância individual para $\alpha' = \alpha / (p+1)$, de modo que cada intervalo satisfaça:  

$$
IC_{1-\alpha'}(\beta_j):\quad
\hat{\beta}_j \pm t_{\alpha'/2,\,n-p-1}\,\hat{\sigma}\sqrt{c_{jj}}, 
\qquad j=0,1,\dots,p.
$$

Com isso, a probabilidade de que **todos os intervalos** contenham simultaneamente os verdadeiros valores dos parâmetros é, no mínimo, $(1-\alpha)$.  

Essa correção é **conservadora** — os intervalos tornam-se mais largos e, portanto, menos precisos —, mas assegura o **controle do erro tipo I global**, evitando falsas descobertas quando muitos coeficientes são testados simultaneamente.  

Em síntese:  
- Use **intervalos individuais** quando o interesse principal for descritivo ou preditivo, e o número de parâmetros for pequeno.  
- Prefira **correções de Bonferroni** ou a **região conjunta elipsoidal** quando houver **múltiplas inferências simultâneas** ou quando for necessário **controle rigoroso do erro tipo I**.  
- Lembre que intervalos amplos não significam falta de precisão do modelo, mas apenas maior cautela na inferência.  

A escolha entre intervalos individuais, correções múltiplas ou região conjunta deve equilibrar **precisão estatística**, **clareza interpretativa** e **propósito da análise**, reconhecendo que a inferência no MRLM envolve inevitavelmente um trade-off entre **rigor e interpretabilidade**.



**Ideias de demonstração**

1. Sob o MRLM Normal, 
   $$
   \hat{\boldsymbol{\beta}}\mid \mathbf{X}\sim N_{p+1}\!\big(\boldsymbol{\beta},\,\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\big),
   $$
   o que implica que
   $$
   (\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})^\top
   [\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}]^{-1}
   (\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})\sim \chi^2_{p+1}.
   $$
2. Como $\hat{\sigma}^2$ é independente de $\hat{\boldsymbol{\beta}}$ sob normalidade, o quociente entre as duas médias quadráticas leva à distribuição
   $$
   F_{p+1,\,n-p-1},
   $$
   o que motiva a desigualdade que define a região elipsoidal conjunta.
3. Os intervalos marginais individuais são obtidos como projeções unidimensionais dessa região multidimensional.


## Intervalos para a média condicional e para predição

Em muitas aplicações, não basta “testar se o efeito existe”: é preciso **quantificar a incerteza** sobre o valor médio esperado de $Y$ em um ponto de interesse e, sobretudo, sobre **uma nova observação** nesse mesmo ponto. No MRLM Normal, essas duas tarefas dão origem a dois objetos distintos:

1. **Intervalo para a média condicional** $E(Y\mid x_0)$: incerteza apenas da **média** no ponto $x_0$.  
2. **Intervalo de predição** para $Y_0$: incerteza da média **+** a variabilidade **intrínseca** do novo erro $\varepsilon_0$.


**(a) Notação**

Seja $x_0=(1, x_{01},x_{02},\dots,x_{0p})^\top$ o vetor (com intercepto) que representa o ponto de interesse no espaço dos preditores. O valor ajustado é

$$
\hat{y}_0 = x_0^\top \hat{\boldsymbol{\beta}}.
$$


**(b) Intervalo para a média condicional**

Queremos estimar $E(Y\mid x_0)=x_0^\top\boldsymbol{\beta}$. Sob o MRLM Normal,

$$
\operatorname{Var}(\hat{y}_0\mid \mathbf{X}) = \sigma^2\, x_0^\top (\mathbf{X}^\top \mathbf{X})^{-1} x_0,
$$

que é estimada por $\hat{\sigma}^2 x_0^\top (\mathbf{X}^\top \mathbf{X})^{-1} x_0$, com

$$
\hat{\sigma}^2=\frac{SQ_{Res}}{\,n-(p+1)\,}=\frac{\mathbf{Y}^\top \mathbf{M}\,\mathbf{Y}}{\,n-(p+1)\,}.
$$

Logo, o intervalo $(1-\alpha)100\%$ para a **média** é

$$
IC_{1-\alpha}\!\big[E(Y\mid x_0)\big]:\quad
\hat{y}_0 \;\pm\; t_{\alpha/2,\,n-(p+1)}\;\hat{\sigma}\,\sqrt{x_0^\top (\mathbf{X}^\top \mathbf{X})^{-1} x_0}.
$$

Esse intervalo é tipicamente **mais estreito**, pois reflete apenas a incerteza da estimação da média em $x_0$.


**(c) Intervalo de predição para uma nova observação**

Para um novo $Y_0$ no ponto $x_0$,

$$
Y_0 = x_0^\top \boldsymbol{\beta} + \varepsilon_0, \qquad \varepsilon_0\sim N(0,\sigma^2)\ \text{e independente de }\hat{\boldsymbol{\beta}}.
$$

A variância do erro de predição é

$$
\operatorname{Var}(Y_0-\hat{y}_0\mid \mathbf{X}) = \sigma^2\Big(1 + x_0^\top (\mathbf{X}^\top \mathbf{X})^{-1} x_0\Big),
$$

o que leva ao intervalo de $(1-\alpha)100\%$ de confiança

$$
IP_{1-\alpha}(Y_0):\quad
\hat{y}_0 \;\pm\; t_{\alpha/2,\,n-(p+1)}\;\hat{\sigma}\,\sqrt{\,1 + x_0^\top (\mathbf{X}^\top \mathbf{X})^{-1} x_0\,}.
$$

O intervalo de predição é **sempre mais largo** que o da média condicional, pois inclui a variabilidade **idiossincrática** da nova observação (o termo “$1$” sob a raiz).



**(d) Papel do alavancamento $h_0$**

Defina

$$
h_0 \;=\; x_0^\top (\mathbf{X}^\top \mathbf{X})^{-1} x_0.
$$

- $h_0$ mede o **quanto $x_0$ está distante** do “miolo” dos dados no espaço das covariáveis; é a noção de **alavancamento** fora da amostra.  
- Quanto maior $h_0$, **maiores** os erros-padrão de $\hat{y}_0$ e, portanto, **mais largos** os intervalos (de média e de predição).  
- Pontos **próximos ao centro** da nuvem de $X$ têm $h_0$ pequeno; pontos de **extrapolação** (fora da região amostral) apresentam $h_0$ grande, tornando as bandas de incerteza muito amplas.


**(e) Leitura prática e boas práticas de relato**

- Use o **intervalo da média** quando o objetivo é descrever o valor médio esperado para unidades com características $x_0$ (efeitos médios, comparação de perfis, relatórios descritivos).  
- Use o **intervalo de predição** quando o foco é **prever um novo caso** (vendas futuras, exame clínico, valor de um imóvel específico).  
- Em aplicações de previsão, **não** reporte apenas o intervalo da média: ele subestima a incerteza relevante para novos casos.  
- Informe sempre $\hat{y}_0$, o erro-padrão correspondente e **qual tipo** de intervalo está sendo reportado.


**Ideias de demonstração**

1. **Média:** $\hat{y}_0=x_0^\top\hat{\boldsymbol{\beta}}$. Como $\hat{\boldsymbol{\beta}}\mid \mathbf{X}\sim N_{p+1}(\boldsymbol{\beta},\,\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1})$, segue $\hat{y}_0\mid \mathbf{X}\sim N(x_0^\top\boldsymbol{\beta},\,\sigma^2 h_0)$. Substituindo $\sigma$ por $\hat{\sigma}$ (independente sob normalidade), obtém-se o pivô com distribuição $t_{n-(p+1)}$.  

2. **Predição:** $Y_0-\hat{y}_0=\varepsilon_0 + x_0^\top(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})$, cuja variância é $\sigma^2(1+h_0)$ pela independência entre $\varepsilon_0$ e $\hat{\boldsymbol{\beta}}$. A padronização com $\hat{\sigma}$ novamente conduz à distribuição $t_{n-(p+1)}$.


## Efeitos parciais, $R^2$ parcial e tamanho de efeito  

Até aqui vimos como os testes $t$ e $F$ nos dizem **se há evidência estatística de efeito** — isto é, se uma variável ou um bloco de variáveis explicativas influencia $Y$ de forma diferente de zero.  Agora, queremos ir além: **quão grande** é esse efeito? Ele é **relevante** ou apenas **detectável** devido ao tamanho da amostra?  

Em outras palavras, **significância estatística** não é o mesmo que **importância prática**.  
Para entender o “tamanho” ou a “força” real do efeito de uma variável, usamos três ferramentas complementares:  

1. o **efeito parcial**,  
2. o **$R^2$ parcial**, e  
3. os **tamanhos de efeito padronizados**, como $f^2$ de Cohen.  

Essas medidas ajudam a responder não apenas *“se o efeito existe”*, mas *“quanto ele importa”*.


**(a) Efeito parcial de uma variável**  

O **efeito parcial** de $x_j$ representa o impacto que ela exerce sobre a média de $Y$, **mantendo as demais variáveis fixas**.  

No MRLM, cada coeficiente $\beta_j$ descreve esse efeito parcial: ele mostra o quanto $Y$ muda, em média, quando $x_j$ aumenta uma unidade e todas as outras variáveis permanecem constantes.  

Do ponto de vista geométrico, é como observar o **declive da hipersuperfície de regressão** apenas na direção de $x_j$, após remover a influência linear das demais colunas de $\mathbf{X}$.  

Nos testes, a estatística $t_j$ responde: *“o efeito parcial de $x_j$ é estatisticamente diferente de zero?”*  
Já o **$R^2$ parcial** responde: *“quanto o modelo melhora, em termos de explicação, quando $x_j$ é incluída?”*



**(b) $R^2$ parcial para uma variável**  

O **$R^2$ parcial** quantifica o **ganho de explicação** que uma variável traz ao modelo, considerando o que já estava explicado pelas outras.  Em termos intuitivos:
- Pegamos o modelo **sem $x_j$** (modelo reduzido) e medimos sua “falha” em explicar $Y$;
- Depois, adicionamos $x_j$ e verificamos **quanto dessa falha foi eliminada**.

Formalmente:

$$
R^2_{\text{parcial},\,j} = \frac{SQ_R - SQ_R^\ast}{SQ_T - SQ_R^\ast},
$$

onde:
- $SQ_R$: soma de quadrados da regressão do **modelo completo**,  
- $SQ_R^\ast$: soma de quadrados da regressão do **modelo reduzido** (sem $x_j$),  
- $SQ_T$: soma de quadrados total da resposta.

Há uma relação direta com a estatística $t_j$:

$$
R^2_{\text{parcial},\,j} = \frac{t_j^2}{t_j^2 + (n - p - 1)}.
$$

Note que enquanto o $R^2$ global mostra quanto o modelo completo explica de $Y$, o $R^2_{\text{parcial}}$ mostra **quanto cada variável adiciona de explicação exclusiva**, após controlar as demais.


**(c) $R^2$ parcial para um bloco de variáveis**  

Quando analisamos **um conjunto de variáveis** (por exemplo, todas as dummies de um fator), o raciocínio é idêntico.  
Com $q$ restrições simultâneas $H_0:\,C\boldsymbol{\beta}=0$, definimos:

$$
R^2_{\text{parcial}} = \frac{SQ_R - SQ_R^\ast}{SQ_T - SQ_R^\ast}.
$$

E sua relação com o teste $F$ é direta:

$$
F = \frac{R^2_{\text{parcial}}/q}{(1 - R^2_{\text{parcial}})/(n - p - 1)}.
$$

O numerador mede o **ganho de explicação** por restrição liberada, e o denominador compara esse ganho com a **variabilidade residual média**.



**(d) Tamanhos de efeito derivados**  

O $R^2_{\text{parcial}}$ é uma medida natural de **magnitude**, mas ainda depende da escala de $Y$.  Por isso, é comum usar medidas **padronizadas**, que permitem comparações diretas entre variáveis e estudos.

- **$f^2$ de Cohen**  
  $$
  f^2 = \frac{R^2_{\text{parcial}}}{1 - R^2_{\text{parcial}}}.
  $$
  Interpretação prática (Cohen, 1988):  
  - $f^2 \approx 0.02$: efeito **pequeno**,  
  - $f^2 \approx 0.15$: efeito **médio**,  
  - $f^2 \approx 0.35$: efeito **grande**.  

  Essa métrica é especialmente útil para discutir resultados de forma substantiva.  

- **$\eta^2_{\text{parcial}}$ (ANOVA)**  
  Em modelos lineares aninhados, $\eta^2_{\text{parcial}}$ coincide numericamente com $R^2_{\text{parcial}}$, sendo preferido em contextos experimentais ou de análise de variância.


**(e) Interpretação prática e recomendações**  

- Um **valor-$p$ pequeno**, mas $R^2_{\text{parcial}}$ muito baixo, indica que o efeito é **estatisticamente detectável**, mas **praticamente irrelevante**.  
- Uma variável **não significativa**, mas com $R^2_{\text{parcial}}$ razoável, pode indicar **falta de poder amostral** — talvez o efeito exista, mas os dados são insuficientes.  
- Sempre reporte **intervalos de confiança**, **valores-$p** e **tamanhos de efeito** juntos: cada um descreve uma dimensão diferente da evidência.  

> **Em resumo:**  
> - O teste $t$ diz se o efeito é *estatisticamente distinto de zero*;  
> - O $R^2_{\text{parcial}}$ diz *quanto ele acrescenta ao modelo*;  
> - O $f^2$ e o $\eta^2$ ajudam a *avaliar a relevância prática*.


**Ideias de demonstração**  

1. Para $q=1$, da relação $t_j^2 = F_{1,\,n-(p+1)}$, obtemos diretamente  
   $$
   R^2_{\text{parcial},j} = \frac{t_j^2}{t_j^2 + (n - p - 1)}.
   $$  

2. Para $q>1$, o teste $F$ pode ser expresso em termos das somas de quadrados dos modelos completo e reduzido.  

3. Pela álgebra das projeções $H$ e $M$, a diferença $SQ_R - SQ_R^\ast = \mathbf{Y}^\top (H - H^\ast)\mathbf{Y}$ é uma forma quadrática em variáveis normais, o que leva às distribuições $\chi^2$ e à razão $F_{q,\,n-(p+1)}$.  


> **Resumo conceitual:**  
> - **Efeito parcial:** variação de $Y$ associada a $x_j$, mantendo as demais variáveis fixas.  
> - **$R^2$ parcial:** proporção da variância residual que $x_j$ consegue explicar a mais.  
> - **$f^2$ e $\eta^2$:** medidas padronizadas de magnitude prática do efeito.  

Essas métricas complementam a inferência clássica e reforçam o entendimento de que, na modelagem estatística, **significância sem magnitude é apenas meio caminho para a interpretação completa**.


## Questões práticas e armadilhas comuns

O ferramental de inferência no MRLM como testes $t$, testes $F$, ANOVA, intervalos de confiança e predição, é extremamente poderoso. No entanto, o uso **automático** ou **acrítico** dessas ferramentas pode levar a conclusões enganosas. É essencial que o analista esteja atento a algumas **questões práticas** e **armadilhas recorrentes** que afetam a validade e a interpretação dos resultados.


**(a) Significância não é relevância.**  
Um valor-$p$ pequeno apenas indica que o efeito é **estatisticamente diferente de zero**, não que seja **importante**.  
É possível detectar efeitos triviais como “significativos” em amostras grandes, e perder efeitos relevantes em amostras pequenas.  Por isso, é importante reportar também **intervalos de confiança** e medidas de **tamanho de efeito**, como o $R^2_{\text{parcial}}$ e o $f^2$ de Cohen, que indicam **magnitude** e **relevância prática**.



**(b) Multicolinearidade.**  
Quando duas ou mais variáveis explicativas são altamente correlacionadas, a matriz $\mathbf{X}^\top\mathbf{X}$ torna-se quase singular.  Isso inflaciona os erros-padrão, reduz o poder dos testes e pode inverter sinais dos coeficientes.  
Sinais típicos incluem:

- coeficientes instáveis a pequenas mudanças na amostra;  
- erros-padrão muito altos;  
- perda de significância de variáveis teoricamente importantes.  

*Ferramentas úteis:*  
- **Fator de Inflação da Variância (VIF)**;  
- análise de correlação entre preditores;  
- decomposição espectral de $\mathbf{X}^\top\mathbf{X}$ (valores singulares pequenos).


**(c) Suposições violadas.**  
Os testes clássicos do MRLM assumem homocedasticidade, independência e normalidade dos erros. Na prática, essas condições nem sempre se verificam.


**(d) Uso excessivo de testes múltiplos **
- **Risco de falsos positivos:** com muitos testes, mesmo em $\alpha=0{,}05$, espera-se rejeitar $H_0$ por acaso em ~5% dos casos. Isso **infla o erro tipo I**.
- **Bonferroni:** divide $\alpha$ pelo nº de testes. É **simples** e **conservador** (reduz falsos positivos, mas pode aumentar falsos negativos).
- **FDR (False Discovery Rate):** controla a **proporção esperada de falsos entre os achados** (ex.: Benjamini–Hochberg). **Menos conservador** que Bonferroni; bom quando há muitos testes simultâneos.
- **p-hacking:** selecionar modelos/testes “até dar significativo” (ex.: tentar várias especificações sem plano prévio). Produz **achados ao acaso** e invalida a inferência.
- **Boas práticas:** pré-registrar hipóteses, limitar testes exploratórios, reportar o nº total de testes e aplicar correções (Bonferroni ou FDR) quando apropriado.


**(e) Intervalos de predição subestimados.**  
Um erro comum é confundir **intervalo de confiança da média** com **intervalo de predição**.  O primeiro descreve a incerteza sobre o valor médio esperado de $Y$, enquanto o segundo incorpora a **variabilidade individual** de uma nova observação.  Por definição, o intervalo de predição é **sempre mais largo**, e deve ser o reportado em análises voltadas à previsão de novos dados.


**(f) Tamanho amostral e poder estatístico.**  
- Em **amostras pequenas**, mesmo efeitos relevantes podem não aparecer significativos, devido à grande variância dos estimadores.  
- Em **amostras muito grandes**, qualquer efeito minúsculo pode se tornar estatisticamente “significativo”.  

A interpretação deve equilibrar **evidência estatística** e **plausibilidade substantiva**, evitando confundir precisão com relevância.


**(g) O perigo da extrapolação.**  
Inferências e previsões são válidas **apenas dentro do espaço das covariáveis observadas**.  Usar o modelo para prever ou estimar efeitos em regiões distantes dos dados (extrapolação) é arriscado, pois a linearidade pode não se manter fora da faixa amostral.  O alavancamento $h_0$ é um bom indicador de quão “longe” um ponto está da nuvem de dados.


**(h) Diagnósticos ignorados.**  
A confiabilidade de qualquer inferência depende da verificação das condições do modelo. Ignorar a análise de resíduos, medidas de alavancagem e influência é uma das **principais causas de interpretações equivocadas**. Esses aspectos serão detalhados na **Seção 3.8 (Diagnóstico de modelos)**.

A inferência estatística no MRLM é uma ferramenta robusta e versátil, capaz de responder perguntas complexas com base empírica.  Mas sua aplicação correta requer **consciência dos pressupostos**, **verificação dos dados** e **interpretação contextualizada**.


