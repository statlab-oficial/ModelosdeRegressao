% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  brazilian,
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}


\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage{graphicx}
\usepackage{eso-pic}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Índice}
\else
  \newcommand\contentsname{Índice}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{Lista de Figuras}
\else
  \newcommand\listfigurename{Lista de Figuras}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{Lista de Tabelas}
\else
  \newcommand\listtablename{Lista de Tabelas}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figura}
\else
  \newcommand\figurename{Figura}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Tabela}
\else
  \newcommand\tablename{Tabela}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listagem}
\newcommand*\listoflistings{\listof{codelisting}{Lista de Listagens}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdflang={pt-BR},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\author{}
\date{}
\begin{document}

% Capa sem número
\pagenumbering{gobble}

\begin{titlepage}
\thispagestyle{empty}

% Imagem como fundo, ancorada no topo-esquerdo, sem distorção
\AddToShipoutPictureBG*{%
  \AtPageUpperLeft{%
    % "desce" a origem para dentro da página
    \raisebox{-\paperheight}{%
      \includegraphics[width=\paperwidth,keepaspectratio]{images/capa.png}%
    }%
  }%
}

\null
\vfill
\end{titlepage}

\ClearShipoutPictureBG

% Reinicia numeração normal no conteúdo
\clearpage
\pagenumbering{arabic}
\setcounter{page}{1}

\renewcommand*\contentsname{Índice}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Informações Legais e Declaração de Uso de Inteligência
Artificial}\label{informauxe7uxf5es-legais-e-declarauxe7uxe3o-de-uso-de-inteliguxeancia-artificial}
\addcontentsline{toc}{chapter}{Informações Legais e Declaração de Uso de
Inteligência Artificial}

\markboth{Informações Legais e Declaração de Uso de Inteligência
Artificial}{Informações Legais e Declaração de Uso de Inteligência
Artificial}

\section*{Direitos Autorais e Uso da
Obra}\label{direitos-autorais-e-uso-da-obra}
\addcontentsline{toc}{section}{Direitos Autorais e Uso da Obra}

\markright{Direitos Autorais e Uso da Obra}

© 2026 --- Os autores.

Todos os direitos reservados. Esta obra é protegida pela legislação
brasileira de direitos autorais (Lei nº 9.610/1998). É vedada a
reprodução, distribuição, armazenamento ou transmissão total ou parcial
deste livro, por qualquer meio ou processo, eletrônico ou mecânico, sem
autorização expressa e por escrito dos autores, salvo nos casos
permitidos pela legislação vigente para fins exclusivamente acadêmicos e
não comerciais, com a devida citação da fonte.

A utilização do material em ambientes de ensino é permitida desde que
preservada a integridade do conteúdo, mencionada a autoria e respeitados
os princípios de uso responsável da informação científica. A reprodução
para fins comerciais, bem como a modificação substancial do conteúdo sem
autorização, constitui violação de direitos autorais.

\section*{Declaração sobre o Uso de Ferramentas de Inteligência
Artificial}\label{declarauxe7uxe3o-sobre-o-uso-de-ferramentas-de-inteliguxeancia-artificial}
\addcontentsline{toc}{section}{Declaração sobre o Uso de Ferramentas de
Inteligência Artificial}

\markright{Declaração sobre o Uso de Ferramentas de Inteligência
Artificial}

Durante a elaboração deste livro foram utilizadas ferramentas de
Inteligência Artificial, em especial sistemas de apoio à escrita e
organização textual, como recurso complementar no processo de produção
acadêmica.

O uso de IA teve caráter \textbf{estritamente assistivo}, não
substituindo o desenvolvimento conceitual, a formulação matemática, a
interpretação estatística nem as decisões pedagógicas da obra. Todo
conteúdo foi cuidadosamente revisado, validado e adaptado pelos autores,
que assumem integral responsabilidade científica e intelectual pelo
texto final.

Não foram delegadas à IA decisões metodológicas, inferenciais ou
conclusões analíticas. A elaboração teórica, a seleção de exemplos, a
validação matemática e a coerência didática resultam da experiência
acadêmica e da atuação dos autores na área de Modelos de Regressão.

Reafirmamos que o uso responsável de tecnologias emergentes deve sempre
preservar o rigor científico, a integridade acadêmica e o protagonismo
intelectual humano.

\bookmarksetup{startatroot}

\chapter*{Prefácio}\label{prefuxe1cio}
\addcontentsline{toc}{chapter}{Prefácio}

\markboth{Prefácio}{Prefácio}

Este livro resulta da experiência acumulada no ensino, na orientação de
estudantes e no desenvolvimento de pesquisas em modelos de regressão.
Somos docentes pesquisadores do Departamento de Estatística e Matemática
Aplicada da Universidade Federal do Ceará (DEMA/UFC), com atuação
contínua em regressão linear, modelagem estatística e inferência.
Integramos o Laboratório de Inovação em Estatística --- StatLab/UFC
(\url{https://statlab.quarto.pub/}) e o Grupo de Pesquisa \emph{Modelos
de Regressão e Aplicações} do CNPq
(\url{http://dgp.cnpq.br/dgp/espelhogrupo/6344659534806942}), espaços
nos quais articulamos ensino, pesquisa e aplicação a problemas reais.

O material tem como base as disciplinas Modelos de Regressão I (CC0290)
e Modelagem Estatística (CC0452), momentos centrais na formação do
estudante, em que probabilidade, inferência e álgebra matricial se
integram em uma estrutura unificada de modelagem. Ao longo dos anos,
consolidamos a convicção de que ensinar regressão exige equilíbrio entre
rigor teórico e clareza interpretativa.

O livro desenvolve o Modelo de Regressão Linear Simples e o Modelo de
Regressão Linear Múltipla de forma progressiva, enfatizando
interpretação, hipóteses, propriedades dos estimadores, inferência
clássica e diagnóstico. A implementação é realizada integralmente no
\textbf{R}, utilizado como ambiente de experimentação conceitual e não
apenas como ferramenta computacional.

Os fundamentos matemáticos mais densos são apresentados em apêndices
específicos, preservando o fluxo didático do texto principal e
permitindo diferentes níveis de aprofundamento. Acreditamos que a
regressão deve ser ensinada como modelo estatístico, com atenção às suas
suposições, limitações e implicações práticas.

Esperamos que este livro contribua para a formação de profissionais
capazes de utilizar modelos de regressão com rigor técnico, pensamento
estatístico crítico e responsabilidade analítica.

\part{Parte I --- Modelagem}

\chapter{Introdução e Panorama dos Modelos de
Regressão}\label{introduuxe7uxe3o-e-panorama-dos-modelos-de-regressuxe3o}

Este livro é dedicado aos estudos de \textbf{Modelos de Regressão},
desde a formulação clássica até extensões modernas. O material apoia as
disciplinas Modelos de Regressão I (CC0290) e Modelagem Estatística
(CC0452), ambas com implementação no software R. Ao longo do semestre, o
estudante terá contato tanto com a fundamentação matemática e
estatística quanto com aplicações práticas em diferentes áreas,
utilizando ferramentas computacionais para explorar dados reais.

O curso está estruturado de forma progressiva: parte-se da regressão
linear simples, como porta de entrada ao raciocínio de modelagem,
avança-se para a regressão linear múltipla, inclusão de variáveis
categóricas e técnicas de seleção de variáveis e análise de diagnóstico.
Tópicos adicionais como modelos lineares generalizados (MLGs), extensões
não lineares e métodos de regularização também serão apresentados.

Mais do que aprender procedimentos técnicos, o objetivo é desenvolver a
capacidade de interpretar resultados, avaliar a adequação dos modelos e
comunicar conclusões de forma clara. A ênfase está tanto na teoria
quanto na prática, de modo que o estudante seja capaz de aplicar a
modelagem estatística em contextos multidisciplinares.

\section{A centralidade da
regressão}\label{a-centralidade-da-regressuxe3o}

A análise de regressão ocupa posição central na Estatística e, em
especial, na Econometria. Como afirma Hoffmann (2016):

\begin{quote}
``A análise de regressão é o método mais importante da econometria.''
\end{quote}

Essa afirmação reflete o fato de que praticamente toda modelagem
econométrica, seja para estimar elasticidades, avaliar políticas
públicas, medir impactos ou testar teorias, passa, de alguma forma, por
um modelo de regressão.

Mas essa centralidade não é exclusiva da economia. Na saúde, a regressão
mede risco e associações; na engenharia, modela desempenho; nas ciências
ambientais, estima impactos; nas ciências sociais, investiga relações
estruturais; na ciência de dados, permanece como ferramenta
interpretável diante de modelos mais complexos.

A regressão tornou-se, portanto, uma linguagem universal para responder
a uma pergunta fundamental:

\begin{quote}
Como varia uma quantidade quando outra varia?
\end{quote}

Essa pergunta é simples. A resposta exige matemática, probabilidade,
inferência e interpretação.

\section{Objetivos do livro}\label{objetivos-do-livro}

\begin{itemize}
\tightlist
\item
  Contextualizar historicamente os modelos de regressão;\\
\item
  Compreender a lógica da modelagem estatística: componente sistemático,
  componente aleatório e relação entre estes;\\
\item
  Apresentar e discutir os principais modelos abordados na disciplina;\\
\item
  Conectar a teoria com aplicações em economia, saúde, engenharia,
  ciências sociais e ambientais;\\
\item
  Discutir potenciais e limitações de cada abordagem, reconhecendo as
  hipóteses subjacentes;\\
\item
  Desenvolver a capacidade de usar ferramentas computacionais para
  ajuste, diagnóstico e interpretação de modelos.
\end{itemize}

Este material é concebidao como uma jornada pela família dos modelos de
regressão. Partimos de um problema simples, como relacionar uma variável
resposta a uma variável explicativa, e avançamos gradualmente até
modelos capazes de lidar com múltiplos fatores, variáveis categóricas,
dados de contagem, proporções e situações em que as hipóteses clássicas
do modelo deixam de ser válidas.

A ideia central é que, ao final do semestre, o estudante seja capaz de
compreender não apenas \textbf{como ajustar} um modelo, mas também
\textbf{quando e por que} usá-lo, avaliando sua adequação e reconhecendo
seus limites.

\section{Breve História da
Regressão}\label{breve-histuxf3ria-da-regressuxe3o}

A regressão, como hoje conhecemos, é fruto de mais de um século de
evolução teórica e prática. Seu ponto de partida remonta ao século XIX,
quando estudos empíricos sobre hereditariedade começaram a revelar
regularidades que poderiam ser descritas matematicamente. Desde então, a
regressão deixou de ser apenas uma curiosidade biológica e tornou-se um
dos pilares da estatística moderna, sustentando análises nas mais
diversas áreas.

\subsection{Francis Galton (1822--1911)}\label{francis-galton-18221911}

\begin{figure}[H]

{\centering \includegraphics[width=0.35\linewidth,height=\textheight,keepaspectratio]{images/galton.jpg}

}

\caption{Francis Galton}

\end{figure}%

A história da regressão começa com uma inquietação simples, quase
doméstica. No final do século XIX, Francis Galton observava famílias
inglesas e fazia uma pergunta que parecia trivial, mas que mudaria a
estatística para sempre: \textbf{filhos de pais muito altos serão
igualmente altos?}

Ao analisar dados de estaturas familiares, Galton percebeu algo
intrigante: filhos de pais muito altos tendiam a ser altos, mas não
tanto quanto seus pais; filhos de pais muito baixos tendiam a ser
baixos, mas não tão baixos quanto seus progenitores. Havia uma força
invisível puxando as medidas extremas de volta ao centro. Em 1886
(Galton (1886b); Galton (1886a)), ele chamou esse fenômeno de
\emph{regression toward mediocrity}.

Sem perceber completamente, Galton havia introduzido quatro pilares da
regressão moderna:\\
uma variável resposta, uma variável explicativa, uma média condicional e
uma variabilidade em torno dessa média.

A matemática ainda era rudimentar. Mas a ideia estava lançada.

\subsection{Karl Pearson (1857--1936)}\label{karl-pearson-18571936}

\begin{figure}[H]

{\centering \includegraphics[width=0.35\linewidth,height=\textheight,keepaspectratio]{images/pearson.jpg}

}

\caption{Karl Pearson}

\end{figure}%

Se Galton teve a intuição, Karl Pearson deu forma matemática ao
fenômeno. Discípulo e colaborador de Galton, Pearson transformou
observações empíricas em estrutura formal. Desenvolveu o coeficiente de
correlação linear, sistematizou métodos de ajuste e ajudou a fundar o
primeiro departamento de estatística do mundo, no University College
London. Criou também a revista \emph{Biometrika}, marco na consolidação
da estatística como disciplina científica.

A regressão deixava de ser apenas uma regularidade observada em dados
biológicos. Tornava-se parte da teoria da probabilidade e da estatística
matemática (Pearson e Lee (1903)).

O que antes era descrição começava a se tornar método.

\subsection{Ronald A. Fisher
(1890--1962)}\label{ronald-a.-fisher-18901962}

\begin{figure}[H]

{\centering \includegraphics[width=0.35\linewidth,height=\textheight,keepaspectratio]{images/fisher.jpg}

}

\caption{Ronald Fisher}

\end{figure}%

Nas décadas seguintes, a estatística enfrentava um novo desafio: não
bastava ajustar curvas; era preciso decidir. Ronald A. Fisher foi o
arquiteto dessa virada. Na década de 1920, incorporou fundamentos de
inferência à regressão e redefiniu o papel da estatística na ciência.

Com Fisher surgem a análise de variância (ANOVA), o método da máxima
verossimilhança e os princípios modernos de planejamento experimental. A
regressão passa a permitir testar hipóteses, construir intervalos de
confiança e quantificar incertezas.

A técnica deixa de ser apenas descritiva. Torna-se ferramenta de decisão
científica.

\subsection{Jerzy Neyman (1894--1981) \& Egon Pearson
(1895--1980)}\label{jerzy-neyman-18941981-egon-pearson-18951980}

\begin{figure}[H]

{\centering \includegraphics[width=0.55\linewidth,height=\textheight,keepaspectratio]{images/neyman_pearson.jpg}

}

\caption{Jerzy Neyman \& Egon Pearson}

\end{figure}%

A década de 1930 marca outro salto conceitual. Jerzy Neyman e Egon
Pearson, filho de Karl Pearson, estruturam formalmente os testes de
hipóteses. Definem os erros do tipo I e tipo II, introduzem critérios de
decisão e consolidam o conceito de intervalo de confiança.

A regressão, agora, não apenas estima relações: ela fornece regras
claras para aceitar ou rejeitar hipóteses. A incerteza passa a ser
quantificada de maneira sistemática.

O método ganha rigor lógico.

\subsection{John Tukey (1915--2000)}\label{john-tukey-19152000}

\begin{figure}[H]

{\centering \includegraphics[width=0.4\linewidth,height=\textheight,keepaspectratio]{images/tukey.jpg}

}

\caption{John Tukey}

\end{figure}%

Mas a estatística não evolui apenas por formalização. Na década de 1950,
John Tukey propõe algo radical: antes de testar, é preciso explorar.
Defende que os dados devem ser interrogados visualmente. Populariza
gráficos de resíduos, diagnósticos e técnicas exploratórias.

A regressão passa a ser acompanhada por perguntas práticas:\\
os pressupostos fazem sentido? há pontos influentes? o modelo realmente
representa os dados?

A estatística recupera o diálogo com a realidade empírica.

\subsection{Peter McCullagh (1952--) \& John Nelder
(1934--2010)}\label{peter-mccullagh-1952-john-nelder-19342010}

\begin{figure}[H]

{\centering \includegraphics[width=0.55\linewidth,height=\textheight,keepaspectratio]{images/mccullagh_nelder.jpg}

}

\caption{Peter McCullagh \& John Nelder}

\end{figure}%

Com o avanço da computação nas décadas de 1960 e 1970, a regressão
amplia suas fronteiras. A álgebra matricial viabiliza modelos com
múltiplos preditores, e a necessidade de analisar dados não normais
torna-se evidente.

Em 1983, McCullagh e Nelder publicam \emph{Generalized Linear Models},
obra que sistematiza os Modelos Lineares Generalizados. A regressão
deixa de ser restrita a variáveis contínuas normalmente distribuídas.
Passa a modelar contagens, proporções, tempos de ocorrência.

A ideia que nasceu da altura de pais e filhos expande-se para
praticamente todos os campos científicos.

A regressão não surgiu pronta. Foi construída por inquietações,
formalizações, rupturas e expansões. Cada geração acrescentou uma
camada: intuição, estrutura, inferência, decisão, diagnóstico e
generalização.

O que começou como uma pergunta sobre estatura tornou-se uma das
ferramentas centrais da ciência moderna.

\section{O contexto histórico:
eugenia}\label{o-contexto-histuxf3rico-eugenia}

É necessário reconhecer que parte do desenvolvimento inicial da
estatística ocorreu em um contexto marcado por ideias eugenistas.
Francis Galton cunhou o termo ``eugenia'' em 1883. A eugenia defendia o
``melhoramento'' da raça humana por meio de seleção artificial.

Karl Pearson foi defensor ativo dessas ideias e dirigiu o laboratório
Francis Galton para a Eugenia Nacional na University College London.
Ronald Fisher também expressou posições eugenistas em seus escritos.

As ideias eugenistas foram posteriormente utilizadas para justificar
políticas discriminatórias e violações graves de direitos humanos no
século XX. Após a Segunda Guerra Mundial, consolidou-se um consenso
internacional de condenação à eugenia como ideologia racista e
incompatível com princípios éticos fundamentais.

Essa contextualização histórica não diminui a importância dos avanços
metodológicos produzidos por esses autores. Pelo contrário, reforça a
necessidade de compreender que métodos estatísticos são ferramentas cujo
uso exige responsabilidade ética.

A regressão é instrumento científico poderoso. O modo como é utilizada
depende do pesquisador.

\section{Consolidação da regressão como pilar da Estatística
moderna}\label{consolidauxe7uxe3o-da-regressuxe3o-como-pilar-da-estatuxedstica-moderna}

A trajetória da regressão pode ser compreendida como uma sequência
evolutiva:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Observação empírica da regressão à média (Galton);\\
\item
  Formalização matemática e correlação (Pearson);\\
\item
  Inferência e planejamento experimental (Fisher);\\
\item
  Estrutura formal de testes de hipóteses (Neyman-Pearson);\\
\item
  Diagnóstico e análise exploratória (Tukey);\\
\item
  Generalização para modelos lineares generalizados (McCullagh e
  Nelder);\\
\item
  Regularização moderna (Ridge e LASSO);\\
\item
  Integração com ciência de dados e aprendizagem de máquina.
\end{enumerate}

Apesar das transformações metodológicas, a pergunta central permanece:

\begin{quote}
Qual é a relação média entre uma variável resposta e seus preditores?
\end{quote}

\section{Panorama dos Modelos de
Regressão}\label{panorama-dos-modelos-de-regressuxe3o}

Tendo em mente os objetivos, estruturas e suposições discutidos
anteriormente, é possível visualizar o \textbf{panorama dos modelos de
regressão} que compõem esta disciplina. A ideia é seguir uma trajetória
progressiva: partir de modelos simples e intuitivos, que permitem
enxergar diretamente a relação entre duas variáveis, e avançar
gradualmente para estruturas mais sofisticadas, capazes de lidar com
múltiplos fatores, respostas não normais e bases de dados complexas.

O percurso do curso está organizado em quatro blocos principais:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Fundamentos} --- regressão linear simples (MRLS), que introduz
  a lógica de média condicional, estimação por mínimos quadrados e
  análise de resíduos.\\
\item
  \textbf{Modelos com múltiplos fatores} --- regressão linear múltipla
  (MRLM), inclusão de variáveis categóricas e interpretação de efeitos
  parciais.\\
\item
  \textbf{Validação e escolha de modelos} --- métodos de diagnóstico,
  análise de resíduos, critérios de seleção de variáveis e
  regularização.\\
\item
  \textbf{Extensões} --- modelos lineares generalizados (GLMs),
  regressão polinomial, modelos não lineares e métodos modernos de
  regularização (Ridge e LASSO), fundamentais na era dos grandes volumes
  de dados.
\end{enumerate}

Em cada etapa, o modelo será apresentado de forma integrada: sua
\textbf{fundamentação teórica}, as \textbf{suposições} que o tornam
válido, exemplos de \textbf{interpretação prática} e \textbf{aplicações
reais} em áreas como economia, saúde, engenharia, ciências ambientais e
ciência de dados.

Assim, o panorama da disciplina funciona como um \textbf{mapa de
navegação}: consolidamos primeiro os alicerces da regressão, depois
exploramos aplicações mais complexas e, por fim, avançamos até modelos
modernos que dialogam diretamente com os desafios atuais da análise de
dados.

\chapter{Modelagem Estatística e
Regressão}\label{modelagem-estatuxedstica-e-regressuxe3o}

\section{Introdução à Modelagem}\label{introduuxe7uxe3o-uxe0-modelagem}

Antes de estudarmos a regressão em si, é importante entender que ela faz
parte de um campo mais amplo chamado \textbf{modelagem
matemática/estatística}. Modelar é o ato de construir representações
formais de fenômenos reais com o objetivo de descrevê-los, explicá-los,
prever seu comportamento ou orientar decisões.

Em termos conceituais, modelar significa responder à seguinte pergunta:

\begin{quote}
Como posso representar, de maneira estruturada, um fenômeno complexo do
mundo real por meio de variáveis e relações formais?
\end{quote}

De forma geral, a modelagem matemática/estatística consiste na tentativa
de traduzir um problema do mundo real em termos matemáticos ou
estatísticos, estruturando hipóteses, equações e relações que permitam
analisar e responder à pergunta proposta. Ela está presente em diversas
áreas, como física, química, biologia, economia, engenharia e ciências
sociais.

É fundamental compreender que \textbf{um modelo não é a realidade}. Ele
é uma aproximação útil. Todo modelo envolve simplificações, suposições e
escolhas: quais variáveis considerar, quais ignorar, que tipo de relação
assumir, qual grau de precisão é necessário. Assim, modelar é sempre um
exercício de equilíbrio entre realismo e simplicidade.

Alguns exemplos de fenômenos que podem ser descritos por modelos
matemáticos incluem:\\
- Crescimento populacional;\\
- Reações químicas;\\
- Sistemas mecânicos e eletrônicos;\\
- Previsão do clima;\\
- Dinâmica do tráfego e da logística;\\
- Estratégias econômicas e financeiras;\\
- Mudanças ambientais e climáticas.

\subsection{Modelo matemático versus modelo
estatístico}\label{modelo-matemuxe1tico-versus-modelo-estatuxedstico}

Uma distinção conceitual importante é a diferença entre:

\begin{itemize}
\tightlist
\item
  \textbf{Modelo matemático (determinístico)}: descreve uma relação
  funcional exata entre variáveis. Para cada valor de entrada, existe um
  único valor de saída.\\
\item
  \textbf{Modelo estatístico (estocástico)}: descreve uma relação média
  ou probabilística, reconhecendo a presença de variabilidade não
  explicada pelas variáveis observadas.
\end{itemize}

No modelo determinístico, escreve-se algo como:

\[
Y = 2X
\]

Aqui, se \(X=5\), então necessariamente \(Y=10\). Não há variação
possível.

Já em um modelo estatístico, reconhecemos que fenômenos reais sofrem
influência de fatores não observados, erros de mensuração, flutuações
naturais ou variáveis omitidas. Assim, escreve-se:

\[
Y = 2X + \varepsilon,
\]

em que \(\varepsilon\) representa a componente aleatória (ruído).

A diferença entre essas duas estruturas pode ser visualizada na
simulação abaixo.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{modelagem_files/figure-pdf/fig-modelo-matematico-vs-estatistico-1.pdf}}

}

\caption{\label{fig-modelo-matematico-vs-estatistico}Modelo matemático
(determinístico) versus modelo estatístico (com ruído): no primeiro, a
relação é funcional; no segundo, observa-se uma tendência média com
variabilidade aleatória.}

\end{figure}%

Observe que, no modelo estatístico, não buscamos uma igualdade exata,
mas uma \textbf{tendência média} em torno da qual os dados se
distribuem.

Essa diferença é central para compreender a regressão.

\subsection{Classificações didáticas de
modelos}\label{classificauxe7uxf5es-diduxe1ticas-de-modelos}

De forma didática, modelos matemáticos/estatísticos podem ser
classificados em:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Determinísticos ou estocásticos (estatísticos)}\\
  Dependendo se o acaso está explicitamente presente na formulação.
\item
  \textbf{Discretos ou contínuos}\\
  Dependendo da natureza das variáveis envolvidas. Contagens são
  discretas; altura, peso e temperatura são contínuos.
\item
  \textbf{Dinâmicos ou estáticos}\\
  Dependendo se o tempo é incorporado explicitamente na estrutura do
  modelo.
\end{enumerate}

Essas classificações são discutidas em livros clássicos de modelagem
matemática, como Meerschaert (2013) e Giordano, Fox, e Horton (2013).
Elas ajudam a organizar o tipo de pergunta que estamos fazendo e a
estrutura matemática adequada para respondê-la.

\subsection{O ciclo da modelagem}\label{o-ciclo-da-modelagem}

O processo de modelagem geralmente envolve as seguintes etapas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Formular a pergunta em termos matemáticos.\\
\item
  Selecionar uma abordagem de modelagem.\\
\item
  Construir o modelo com base nas variáveis e hipóteses do problema.\\
\item
  Resolver o modelo matemático (ou ajustá-lo aos dados, no caso
  estatístico).\\
\item
  Interpretar a solução em termos do fenômeno real.
\end{enumerate}

Esse ciclo é \textbf{iterativo}: muitas vezes o modelo precisa ser
ajustado ou refinado conforme novas informações surgem. Ao confrontar o
modelo com dados, podem surgir questões como:

\begin{itemize}
\tightlist
\item
  A forma funcional escolhida faz sentido?
\item
  Variáveis importantes foram omitidas?
\item
  O comportamento dos resíduos está de acordo com as hipóteses?
\item
  O modelo mantém desempenho em novos dados?
\end{itemize}

Modelagem não é um procedimento linear; é um processo de aproximações
sucessivas.

Além disso, três elementos costumam acompanhar o ciclo de modelagem:

\begin{itemize}
\tightlist
\item
  \textbf{Estimação ou calibração}: ajuste de parâmetros com base em
  dados observados.
\item
  \textbf{Validação}: verificação da qualidade do ajuste.
\item
  \textbf{Análise de sensibilidade}: avaliação do impacto de pequenas
  mudanças nas hipóteses ou nos dados.
\end{itemize}

\subsection{Exemplos de modelagem}\label{exemplos-de-modelagem}

\begin{itemize}
\tightlist
\item
  \textbf{Problema de otimização}: determinar o momento ideal de venda
  de um animal de criação, considerando ganho de peso, custo de
  manutenção e preço de mercado.\\
\item
  \textbf{Problema de crescimento populacional}: prever a evolução da
  população de um país ou espécie a partir de dados censitários.
\end{itemize}

Dentro desse panorama mais amplo, a regressão ocupa um papel específico:
ela é uma técnica estatística voltada para modelar a \textbf{média
condicional} de uma variável resposta em função de variáveis
explicativas. Em outras palavras, ela formaliza matematicamente a ideia
de que parte da variação observada pode ser explicada sistematicamente,
enquanto outra parte permanece como ruído.

Modelagem é, portanto, a linguagem que conecta teoria, dados e
inferência. A regressão é uma de suas expressões mais importantes.

\subsubsection{Exemplos de modelagem em diferentes
áreas}\label{exemplos-de-modelagem-em-diferentes-uxe1reas}

A modelagem matemática e estatística é utilizada para descrever e prever
o comportamento de fenômenos em uma grande variedade de contextos.
Alguns exemplos ilustrativos incluem:

\begin{itemize}
\tightlist
\item
  \textbf{Epidemiologia}: modelos SIR
  (Susceptíveis--Infectados--Recuperados) para descrever a propagação de
  doenças infecciosas ao longo do tempo.
\item
  \textbf{Engenharia de Pesca}: relação entre esforço de pesca (dias de
  mar, número de embarcações) e o estoque pesqueiro disponível.
\item
  \textbf{Ciência de Dados}: previsão da demanda de energia elétrica a
  partir de temperatura, hora do dia e perfil de consumo.
\item
  \textbf{Economia}: modelos de oferta e demanda que relacionam preços e
  quantidades em equilíbrio de mercado.
\item
  \textbf{Climatologia}: simulações que conectam emissão de gases de
  efeito estufa, temperatura média global e regimes de precipitação.
\item
  \textbf{Educação}: análise da relação entre investimento em ensino e
  desempenho em exames padronizados.
\item
  \textbf{Oceanografia}: modelos que descrevem correntes marinhas em
  função de gradientes de temperatura e salinidade.
\end{itemize}

Esses exemplos mostram que \textbf{modelagem é uma linguagem comum em
ciência e tecnologia}. A regressão estatística é uma forma particular de
modelagem que foca em quantificar relações entre variáveis observáveis,
buscando separar o \textbf{sinal} (estrutura determinística) do
\textbf{ruído} (componente aleatória).

\section{Introduçao à Regressão}\label{introduuxe7ao-uxe0-regressuxe3o}

Regressão é um \textbf{modelo matemático-estatístico} que busca
relacionar uma \textbf{variável resposta} (\(Y\)) com uma ou mais
\textbf{variáveis explicativas}. De forma conceitual, a regressão parte
da ideia de que o comportamento médio de \(Y\) pode ser descrito
condicionalmente às variáveis explicativas \(X\).

Em termos mais precisos, o que a regressão modela não é simplesmente
\(Y\), mas a \textbf{média condicional}:

\[
E(Y \mid X)
\]

Essa perspectiva é central nos textos clássicos de regressão e
econometria (Charnet et al. (2008); Hoffmann (2006); Gujarati (2006)). O
objetivo não é afirmar que \(X\) determina exatamente \(Y\), mas que
existe uma \textbf{estrutura sistemática média} associada às variáveis
explicativas.

A ideia fundamental é que a variação observada em um fenômeno pode ser
decomposta em duas partes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Uma \textbf{estrutura determinística} (ou componente sistemática), que
  representa o sinal da relação entre as variáveis;
\item
  Uma \textbf{componente aleatória}, representada pelo ruído
  \(\varepsilon\), que captura variações não explicadas pelo modelo.
\end{enumerate}

Essa decomposição aparece de forma recorrente na literatura de regressão
aplicada (Draper e Smith (1998); Kutner et al. (2005); Montgomery, Peck,
e Vining (2021)) como a base conceitual do modelo linear.

De forma geral, existem duas representações usuais para essa ideia. -
\textbf{Modelo Aditivo} (o mais utilizado):

\[
Y = \mu(X) + \varepsilon
\]

\begin{itemize}
\tightlist
\item
  \textbf{Modelo Multiplicativo} (útil em contextos onde a variabilidade
  é proporcional ao nível médio):
\end{itemize}

\[
Y = \mu(X) \cdot \varepsilon
\]

com:

\begin{itemize}
\tightlist
\item
  \(mu(X)\) → parte determinística (estrutura média);
\item
  \(\varepsilon\) → componente aleatória (ruído), que pode ser aditivo
  ou multiplicativo.
\end{itemize}

O sinal \(\mu(X)\) também é denominado de função de regressão.

No modelo aditivo clássico, assume-se que

\[
E(\varepsilon \mid X) = 0,
\]

isto é, o erro não carrega informação sistemática adicional sobre \(Y\)
além da já contida em \(X\). Essa condição garante que
\(E(Y \mid X) = \mu(X)\), permitindo interpretar \(\mu(X)\) como a
\textbf{média condicional de \(Y\) dado \(X\)}. Essa hipótese é central
para a interpretação dos coeficientes como efeitos médios condicionais
(Hoffmann (2006); Gujarati (2006)).

No caso do \textbf{modelo multiplicativo}, a condição análoga é \[
E(\varepsilon \mid X) = 1.
\]

Nesse caso, temos
\(E(Y \mid X) = \mu(X)\,E(\varepsilon \mid X) = \mu(X)\), de modo que
\(\mu(X)\) continua representando a média condicional de \(Y\) dado
\(X\).

A diferença fundamental entre as duas formulações é estrutural. No
modelo aditivo, o erro representa um \textbf{deslocamento absoluto} em
torno da média condicional: a variação aleatória soma-se ao valor
esperado, produzindo oscilações cuja magnitude, em princípio, não
depende do nível médio. Já no modelo multiplicativo, o erro atua como um
\textbf{fator proporcional}, ampliando ou reduzindo o valor médio de
acordo com sua própria intensidade. Nesse caso, a variabilidade está
intrinsecamente ligada ao nível esperado da variável resposta.

Essa distinção tem implicações relevantes. Em estruturas aditivas,
concebe-se a variabilidade como relativamente independente da escala do
fenômeno; em estruturas multiplicativas, a dispersão tende a crescer ou
decrescer proporcionalmente ao valor médio. Fenômenos econômicos,
biológicos e ambientais frequentemente apresentam esse comportamento
proporcional, por exemplo, renda, produção, crescimento populacional ou
biomassa, o que torna a formulação multiplicativa conceitualmente mais
adequada em muitos contextos.

A escolha entre uma estrutura aditiva ou multiplicativa envolve
considerações teóricas e empíricas. Modelos econômicos frequentemente
sugerem relações proporcionais, enquanto a inspeção gráfica dos dados
pode revelar padrões de variância crescente. Assim, a definição da forma
funcional e da natureza da variação depende da teoria utilizada, da
escala de mensuração das variáveis e do comportamento empírico observado
nos dados (Gujarati (2006); Hoffmann (2006)).

No caso do modelo aditivo clássico, a hipótese de que o erro não carrega
informação sistemática adicional além da contida nas variáveis
explicativas é central para a interpretação dos coeficientes como
efeitos médios condicionais. Essa perspectiva, segundo a qual a
regressão modela a média condicional e não valores individuais,
constitui um dos pilares da regressão aplicada e da econometria
tradicional (Hoffmann (2006); Gujarati (2006)).

\subsection{Interpretação conceitual: sinal e
ruído}\label{interpretauxe7uxe3o-conceitual-sinal-e-ruuxeddo}

A regressão parte do reconhecimento de que fenômenos reais são
influenciados por múltiplos fatores, muitos dos quais não são
observáveis ou mensuráveis. Assim, mesmo que exista uma relação
estrutural entre \(X\) e \(Y\), os dados observados não estarão
perfeitamente alinhados sobre uma curva.

O modelo assume que:

\[
Y = \text{sinal} + \text{ruído}
\]

O sinal representa a parte explicável pelas variáveis incluídas no
modelo; o ruído representa:

\begin{itemize}
\tightlist
\item
  variáveis omitidas,
\item
  erros de mensuração,
\item
  flutuações naturais,
\item
  fatores aleatórios não controlados.
\end{itemize}

Essa separação entre componente sistemática e erro é um dos pilares da
regressão linear clássica (Draper e Smith (1998)).

\subsubsection{Exemplo ilustrativo: Sinal +
Ruído}\label{exemplo-ilustrativo-sinal-ruuxeddo}

A seguir, simulamos um conjunto de dados artificiais para ilustrar a
lógica central da regressão: \textbf{um sinal determinístico mais um
componente de ruído}.

Considere a relação:

\[
Y = 3 + 2X + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, 5^2),
\]

com \(X \in \{ 1, 2, \dots, 30\}\). A reta \(Y = 3 + 2X\) representa o
\textbf{sinal verdadeiro}, enquanto os pontos observados \((X, Y)\)
incluem a variabilidade aleatória do ruído \(\varepsilon\).

O gráfico abaixo mostra a dispersão dos dados simulados juntamente com a
curva verdadeira. Essa visualização reforça a ideia de que a regressão
busca recuperar a estrutura determinística do fenômeno em meio à
aleatoriedade introduzida pelos ruídos (erros ou fonte de variação).

\pandocbounded{\includegraphics[keepaspectratio]{modelagem_files/figure-pdf/unnamed-chunk-1-1.pdf}}

\subsubsection{Exemplo ilustrativo: Sinal + Ruído (homoscedástico vs
heteroscedástico)}\label{exemplo-ilustrativo-sinal-ruuxeddo-homosceduxe1stico-vs-heterosceduxe1stico}

Neste exemplo, comparamos dois cenários de regressão linear que diferem
apenas na \textbf{dispersão dos ruídos}:

\begin{itemize}
\tightlist
\item
  \textbf{Homoscedástico}: a variância dos erros é constante em todos os
  valores de \(X\). Nesse caso, a nuvem de pontos se distribui de forma
  aproximadamente uniforme em torno da reta verdadeira,
  independentemente da posição no eixo \(X\).\\
\item
  \textbf{Heteroscedástico}: a variância cresce com \(X\). Assim, para
  valores pequenos de \(X\) os pontos estão mais concentrados, enquanto
  para valores grandes de \(X\) a dispersão aumenta.
\end{itemize}

Esse contraste é fundamental: se a homoscedasticidade não for
respeitada, os estimadores de mínimos quadrados continuam \textbf{não
viesados}, mas deixam de ser eficientes (ou seja, não são os de menor
variância). Isso motiva o uso de métodos alternativos, como os mínimos
quadrados ponderados (WLS) ou transformações nos dados.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{modelagem_files/figure-pdf/unnamed-chunk-2-1.png}}

}

\caption{Sinal + ruído: (esq.) variância constante; (dir.) variância
crescente (heteroscedasticidade).}

\end{figure}%

\subsubsection{Exemplo ilustrativo: Possíbildiades de relações ente X e
Y}\label{exemplo-ilustrativo-possuxedbildiades-de-relauxe7uxf5es-ente-x-e-y}

Observe que nem toda relação entre duas variáveis é \textbf{linear e
forte}. Considere algums possibilidades comuns:

\begin{itemize}
\tightlist
\item
  \textbf{(a) Sem correlação}: não existe padrão claro entre \(X\) e
  \(Y\); conhecer \(X\) não ajuda a prever \(Y\).\\
\item
  \textbf{(b) Correlação linear fraca}: existe uma tendência positiva,
  mas com grande dispersão ao redor da reta.\\
\item
  \textbf{(c) Correlação linear forte}: os pontos seguem de perto uma
  tendência linear; \(X\) \emph{explica} grande parte da variabilidade
  em \(Y\).\\
\item
  \textbf{(d) Relação não linear}: \(X\) e \(Y\) se relacionam, mas a
  forma não é bem descrita por uma reta (por exemplo, uma parábola).
\end{itemize}

Esse exemplo mostra que a \textbf{correlação linear} é apenas um caso
particular dentro de uma variedade de possíveis dependências entre
variáveis. Por isso, ao analisar dados, é sempre importante, quando
possível, \textbf{visualizar os diagramas de dispersão} antes de ajustar
um modelo, evitando conclusões equivocadas sobre linearidade.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{modelagem_files/figure-pdf/unnamed-chunk-3-1.png}}

}

\caption{Correlação entre X e Y: (a) nenhuma, (b) linear fraca, (c)
linear forte, (d) não linear.}

\end{figure}%

\subsubsection{\texorpdfstring{Exemplo ilustrativo: Relação não linear
entre \(X\) e
\(Y\)}{Exemplo ilustrativo: Relação não linear entre X e Y}}\label{exemplo-ilustrativo-relauxe7uxe3o-nuxe3o-linear-entre-x-e-y}

Até agora, discutimos a regressão sob a perspectiva de relações
lineares. No entanto, nem todo fenômeno real apresenta comportamento
aproximadamente linear. Em muitos contextos, como crescimento biológico,
processos de absorção, liberação acumulada de substâncias ou resposta a
doses, a relação entre as variáveis pode apresentar \textbf{curvatura},
\textbf{saturação} ou \textbf{pontos de inflexão}.

Neste exemplo, construiremos artificialmente uma base de dados cujo
\textbf{sinal verdadeiro não é linear}. O comportamento adotado será do
tipo \textbf{crescimento com saturação}: inicialmente, pequenos aumentos
em \(X\) produzem grandes aumentos em \(Y\); à medida que \(X\) cresce,
o efeito marginal diminui e a curva tende a estabilizar.

Essa estrutura pode ser representada genericamente por uma função do
tipo:

\[
Y = f(X) + \varepsilon
\]

em que \(f(X)\) é não linear e \(\varepsilon\) representa o componente
aleatório.

O objetivo aqui é didático: comparar dois cenários sobre os mesmos dados
observados:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ajustar um \textbf{modelo linear}, mesmo sabendo que o sinal não é
  linear;
\item
  Comparar esse ajuste com a \textbf{curva verdadeira} que gerou os
  dados.
\end{enumerate}

Essa comparação permite visualizar um ponto conceitual central da
modelagem:

\begin{quote}
Um modelo pode estar corretamente estimado sob suas hipóteses e, ainda
assim, estar incorretamente especificado.
\end{quote}

Ou seja, o problema pode não estar na estimação, mas na \textbf{forma
funcional escolhida}.

Ao observar os gráficos, procure refletir:

\begin{itemize}
\tightlist
\item
  O modelo linear captura adequadamente o padrão médio?
\item
  Há evidências visuais de curvatura?
\item
  O erro parece sistemático ao longo de \(X\)?
\item
  O que aconteceria com os resíduos nesse caso?
\end{itemize}

Esse tipo de análise visual é uma etapa importante antes da formalização
do modelo. A regressão linear é uma ferramenta poderosa, mas sua
adequação depende da coerência entre a estrutura assumida e o
comportamento real dos dados.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{modelagem_files/figure-pdf/fig-linear-vs-nao-linear-1.pdf}}

}

\caption{\label{fig-linear-vs-nao-linear}Exemplo com sinal não linear:
(esq.) ajuste linear; (dir.) sinal verdadeiro que gerou os dados.}

\end{figure}%

\subsection{Objetivos e Estruturas do Modelo da
Regressão}\label{objetivos-e-estruturas-do-modelo-da-regressuxe3o}

Depois de explorar exemplos visuais de associação entre variáveis, é
importante consolidar uma visão mais conceitual sobre a regressão. A
regressão não é simplesmente uma técnica de ajuste de curvas; ela é uma
estrutura formal para modelar a \textbf{relação média entre variáveis
observáveis}.

Em termos conceituais, a regressão procura compreender como o
comportamento médio de \(Y\) varia em função de \(X\). Isto é, ela
organiza a seguinte pergunta:

\begin{quote}
Como a variável resposta se comporta, em média, quando as variáveis
explicativas variam?
\end{quote}

\subsubsection{Objetivos centrais}\label{objetivos-centrais}

A regressão pode ser compreendida a partir de três grandes finalidades:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Explicar}\\
  Identificar quais fatores estão associados à variável resposta e
  quantificar a magnitude dessas associações.
\item
  \textbf{Predizer}\\
  Utilizar o modelo ajustado para estimar valores futuros ou não
  observados de \(Y\).
\item
  \textbf{Controlar}\\
  Avaliar o efeito de uma variável mantendo as demais constantes,
  permitindo interpretações condicionais.
\end{enumerate}

Esses objetivos aparecem de maneira combinada na prática. Em
econometria, frequentemente busca-se compreender relações estruturais
entre variáveis macroeconômicas (Gujarati (2006); Hoffmann (2006)). Em
engenharia e estatística aplicada, pode haver maior ênfase na capacidade
preditiva do modelo (Montgomery, Peck, e Vining (2021)). Em estudos
científicos em geral, explicar e prever caminham juntos.

\subsubsection{Observações conceituais
importantes}\label{observauxe7uxf5es-conceituais-importantes}

Antes de avançarmos para a formalização do modelo linear simples, duas
observações merecem destaque.

\textbf{1. Correlação significativa e ajuste linear}

Em geral, quando existe correlação linear significativa entre variável
explicativa e variável resposta, é razoável esperar que o Modelo de
Regressão Linear Simples produza um ajuste satisfatório. Isso ocorre
porque a correlação mede o grau de associação linear entre duas
variáveis. Se essa associação é forte, a reta ajustada tende a capturar
uma parcela substancial da variabilidade observada.

Entretanto, correlação elevada não garante que o modelo esteja
corretamente especificado. Pode haver curvatura, variância não constante
ou outros padrões estruturais não capturados por uma reta. A inspeção
gráfica continua sendo essencial.

\textbf{2. Associação estatística não implica causa e efeito}

Uma relação estatística entre variáveis não implica automaticamente
causalidade.Mesmo que um coeficiente estimado seja estatisticamente
significativo, isso não significa que uma variável cause a outra. A
interpretação causal:

\begin{itemize}
\tightlist
\item
  não pode se basear apenas na amostra considerada;
\item
  deve estar apoiada em teoria ou conhecimento empírico;
\item
  pode exigir desenho experimental ou hipóteses estruturais adicionais.
\end{itemize}

Exemplos clássicos ilustram essa distinção:

\begin{itemize}
\tightlist
\item
  \textbf{Despesas de consumo pessoal e renda pessoal disponível} ---
  aqui, a teoria econômica sustenta a direção causal.
\item
  \textbf{Ganho de peso e consumo de calorias} --- a evidência empírica
  e experimental apoia a interpretação causal.
\end{itemize}

Sem essa base teórica ou experimental, a regressão revela associação,
não mecanismo causal.

\subsubsection{Duas características fundamentais do modelo de
regressão}\label{duas-caracteruxedsticas-fundamentais-do-modelo-de-regressuxe3o}

Do ponto de vista probabilístico, um modelo de regressão possui duas
características essenciais:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Para cada nível fixado de \(X\), existe uma \textbf{distribuição de
  probabilidade de} \(Y\).
\item
  As médias dessas distribuições variam de forma sistemática com \(X\).
\end{enumerate}

Essa segunda característica é o coração da regressão: modelar como a
média de \(Y\) se altera quando \(X\) varia.

Visualmente, isso significa que, para cada valor de \(X\), não há um
único valor possível de \(Y\), mas sim uma distribuição de valores
possíveis. O modelo descreve o comportamento médio dessas distribuições.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{modelagem_files/figure-pdf/unnamed-chunk-4-1.pdf}}

}

\caption{Como as observações são geradas em regressão linear.}

\end{figure}%

Essa perspectiva probabilística será formalizada no próximo capítulo,
quando introduzirmos o Modelo de Regressão Linear Simples (MRLS).

\subsubsection{Estrutura conceitual da
regressão}\label{estrutura-conceitual-da-regressuxe3o}

Independentemente da área, a regressão parte de uma ideia fundamental:

\[
Y = \text{parte sistemática} + \text{parte não explicada}
\]

A parte sistemática representa o padrão médio associado às variáveis
explicativas. A parte não explicada representa variações adicionais que
não são capturadas pelo modelo, seja por fatores não observados,
limitações de mensuração ou variabilidade inerente ao fenômeno.

A regressão organiza essa decomposição de maneira formal e mensurável.
Ela não elimina a variabilidade; ela a estrutura.

\subsubsection{Alguns exemplos que mostram a importância
prática}\label{alguns-exemplos-que-mostram-a-importuxe2ncia-pruxe1tica}

A utilidade da regressão torna-se mais clara quando observamos
aplicações concretas.

\begin{itemize}
\item
  \textbf{Economia}\\
  Modelos de regressão são usados para analisar como variáveis como taxa
  de juros, câmbio e nível de consumo se associam à inflação. O objetivo
  pode ser compreender o mecanismo econômico (explicação) ou projetar
  cenários futuros (previsão).
\item
  \textbf{Saúde}\\
  Em estudos clínicos, a regressão permite avaliar a relação entre
  tratamento e resposta terapêutica, controlando por idade, sexo ou
  comorbidades. Aqui, a regressão organiza a comparação entre grupos e
  ajuda a quantificar diferenças médias.
\item
  \textbf{Engenharia}\\
  Na modelagem da resistência de materiais, regressões relacionam tensão
  aplicada e deformação observada, permitindo prever limites
  operacionais.
\item
  \textbf{Esportes}\\
  Pode-se modelar o desempenho de uma equipe em função de variáveis como
  investimento, tempo de posse de bola ou eficiência ofensiva,
  identificando padrões associados ao resultado final.
\item
  \textbf{Pesca e aquicultura}\\
  Relações entre esforço de pesca e biomassa capturada, ou entre tempo
  de cultivo e ganho de peso, podem ser analisadas por regressão para
  apoiar decisões produtivas.
\item
  \textbf{Políticas públicas}\\
  Avaliações de impacto utilizam regressão para investigar como
  programas sociais se associam a indicadores como renda, escolaridade
  ou emprego.
\end{itemize}

Esses exemplos mostram que regressão não é apenas um instrumento
matemático; é uma ferramenta de organização do raciocínio quantitativo
em contextos reais.

\subsubsection{Potenciais e limites}\label{potenciais-e-limites}

A regressão possui algumas virtudes que explicam sua ampla utilização:

\begin{itemize}
\tightlist
\item
  Permite quantificar efeitos médios;
\item
  Oferece interpretação relativamente direta dos coeficientes;
\item
  Estrutura a análise de dados de forma sistemática;
\item
  Serve como base para extensões mais sofisticadas.
\end{itemize}

Ao mesmo tempo, é importante reconhecer que:

\begin{itemize}
\tightlist
\item
  A qualidade da conclusão depende da qualidade dos dados;
\item
  A forma funcional escolhida influencia os resultados;
\item
  Modelos podem ser mal especificados;
\item
  Associação estatística não é automaticamente causalidade.
\end{itemize}

Reconhecer essas dimensões faz parte da maturidade estatística.

A regressão é, portanto, uma ferramenta que conecta \textbf{teoria,
dados e decisão}. Ela organiza a variabilidade observada em uma
estrutura interpretável e mensurável.

No próximo capítulo, iniciaremos o estudo formal do \textbf{Modelo de
Regressão Linear Simples (MRLS)}, que constitui o ponto de partida para
compreender, com rigor matemático, como essa estrutura é estimada e
quais propriedades possui.

\chapter{Exercícios e atividades}\label{exercuxedcios-e-atividades}

\section{Exercícios conceituais}\label{exercuxedcios-conceituais}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Ao transformar um problema do mundo real em um modelo matemático ou
  estatístico, é necessário definir variáveis e relações formais.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    O que significa ``traduzir um problema real'' em linguagem
    matemática?\\
  \item
    Como a escolha das variáveis influencia o tipo de resposta que o
    modelo pode oferecer?\\
  \item
    Dê um exemplo em que a escolha inadequada das variáveis leve a
    conclusões limitadas ou equivocadas.
  \end{enumerate}
\item
  Considere um problema de crescimento populacional ao longo do tempo.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Explique a diferença conceitual entre um modelo estático e um modelo
    dinâmico nesse contexto.\\
  \item
    Por que a inclusão explícita do tempo altera a estrutura matemática
    do modelo?
  \end{enumerate}
\item
  Em muitos fenômenos reais, diferentes modelos podem ser propostos para
  descrever o mesmo problema.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Quais critérios podem ser utilizados para escolher entre dois
    modelos concorrentes?\\
  \item
    Explique como o critério da parcimônia atua nesse processo de
    escolha.\\
  \item
    Por que modelos excessivamente complexos podem ser problemáticos,
    mesmo quando ajustam melhor os dados?
  \end{enumerate}
\item
  Explique a afirmação: ``um modelo não é a realidade; é uma aproximação
  útil''.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    O que significa equilíbrio entre realismo e simplicidade?\\
  \item
    Por que todo modelo envolve escolhas e simplificações?\\
  \item
    Dê um exemplo em que um modelo necessariamente ignora parte da
    complexidade do fenômeno.
  \end{enumerate}
\item
  Considere as duas estruturas apresentadas no capítulo:

  \[Y = 2X\]

  e

  \[Y = 2X + \varepsilon.\]

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Diferencie conceitualmente modelo determinístico e modelo
    estatístico.\\
  \item
    Explique o papel de \(\varepsilon\) na segunda equação.\\
  \item
    Liste três possíveis fontes para \(\varepsilon\).\\
  \item
    Por que, no modelo estatístico, buscamos uma tendência média e não
    uma igualdade exata?
  \end{enumerate}
\item
  O capítulo afirma que a regressão modela a média condicional:

  \[E(Y \mid X).\]

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Explique o que significa média condicional em linguagem intuitiva.\\
  \item
    Mostre como a ideia de \(E(Y \mid X)\) está relacionada à
    decomposição
  \end{enumerate}

  \[Y = \text{sinal} + \text{ruído}.\]

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \setcounter{enumii}{2}
  \tightlist
  \item
    É possível que um valor observado de \(Y\) esteja distante de
    \(E(Y \mid X)\) e, ainda assim, o modelo esteja adequado?
    Justifique.
  \end{enumerate}
\item
  O texto apresenta duas formas gerais de modelagem:

  \[Y = f(X) + \varepsilon\]

  e

  \[Y = f(X)\cdot \varepsilon.\]

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Diferencie erro aditivo e erro multiplicativo.\\
  \item
    Em que tipo de fenômeno o erro tende a ser proporcional ao nível
    médio?
  \end{enumerate}
\item
  Sobre o ciclo da modelagem:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Explique por que o processo de modelagem é iterativo.\\
  \item
    Diferencie estimação, validação e análise de sensibilidade.\\
  \item
    Dê um exemplo de situação em que, após ajustar o modelo, seria
    necessário voltar e modificar hipóteses ou forma funcional.
  \end{enumerate}
\item
  O capítulo mostra que nem toda relação entre \(X\) e \(Y\) é linear.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Por que correlação linear elevada não garante especificação correta
    do modelo?\\
  \item
    Dê um exemplo conceitual de relação não linear em que o coeficiente
    de correlação de Pearson possa ser próximo de zero.\\
  \item
    O que significa dizer que há ``erro sistemático ao longo de \(X\)''?
  \end{enumerate}
\item
  Sobre associação e causalidade:
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Explique a diferença entre associação estatística e causalidade.\\
\item
  Liste duas razões pelas quais um coeficiente estimado pode ser
  estatisticamente significativo e ainda não representar um efeito
  causal.\\
\item
  Que tipo de informação adicional (teórica ou experimental) seria
  necessária para sustentar uma interpretação causal?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Sobre previsão e extrapolação:
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Diferencie interpolação e extrapolação.\\
\item
  Por que extrapolar pode ser arriscado mesmo quando o ajuste parece bom
  no intervalo observado?\\
\item
  Dê um exemplo aplicado em que extrapolação seria particularmente
  problemática.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\tightlist
\item
  Escolha um dos contextos aplicados citados no capítulo.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Defina uma variável resposta \(Y\) e pelo menos três variáveis
  explicativas \(X_1, X_2, X_3\).\\
\item
  Indique se o modelo conceitual seria mais plausivelmente aditivo ou
  multiplicativo e justifique.\\
\item
  Liste duas suposições que você consideraria críticas para interpretar
  os resultados.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{12}
\tightlist
\item
  Reflita sobre a seguinte estrutura geral:
\end{enumerate}

\[Y = f(X) + \varepsilon.\]

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  O que significa assumir que \(E(\varepsilon \mid X) = 0\)?\\
\item
  Por que essa hipótese é central para interpretar os coeficientes como
  efeitos médios condicionais?\\
\item
  O que pode acontecer se essa hipótese não for satisfeita?
\end{enumerate}

As respostas devem ser redigidas de forma argumentativa, conectando
explicitamente os conceitos apresentados no capítulo.

\section{Atividade de Simulação e
Regressão}\label{atividade-de-simulauxe7uxe3o-e-regressuxe3o}

Esta atividade tem como objetivo consolidar os conceitos estudados ao
longo do capítulo 2 por meio de \textbf{simulações controladas}. A
proposta é investigar, de forma sistemática, como diferentes estruturas
funcionais (linear, quadrática, exponencial e potência) se comportam sob
ruído e como o ajuste por mínimos quadrados responde a essas situações.

Em todos os exercícios:

\begin{itemize}
\tightlist
\item
  Gere os dados conforme indicado no código.
\item
  Produza os gráficos solicitados.
\item
  Ajuste os modelos especificados.
\item
  Responda às questões de forma \textbf{argumentativa}, conectando os
  resultados aos conceitos de sinal, ruído, forma funcional e
  especificação do modelo.
\end{itemize}

\textbf{Objetivo:} simular dados sob diferentes modelos, visualizar
dispersões, calcular correlações e ajustar modelos via \textbf{MQO}
(OLS), comparando a \textbf{curva verdadeira} que gerou os dados com o
\textbf{ajuste estimado}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Preparação do Ambiente
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preparação do ambiente (simples e reprodutível)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Simular dados lineares simples
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 2) Simular dados lineares simples}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{30}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n)}
\NormalTok{beta0 }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{beta1 }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \DecValTok{5}

\NormalTok{sinal }\OtherTok{\textless{}{-}}\NormalTok{ beta0 }\SpecialCharTok{+}\NormalTok{ beta1}\SpecialCharTok{*}\NormalTok{x}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ sinal }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =}\NormalTok{ sigma)}

\CommentTok{\# Visualização: pontos + reta verdadeira}
\FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"X"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Y"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Linear: sinal + ruído"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, sinal, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"dados"}\NormalTok{, }\StringTok{"reta verdadeira"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{), }\AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{), }\AttributeTok{bty =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{part1_ex_files/figure-pdf/2.CHUNK_02-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Correlação}
\NormalTok{cor\_xy }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(x, y)}
\NormalTok{cor\_xy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.8794497
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Ajustar reta por MQO
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 3) Ajustar reta por MQO (OLS)}
\NormalTok{mod\_lin }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}

\CommentTok{\# Resumo do ajuste}
\FunctionTok{summary}\NormalTok{(mod\_lin)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = y ~ x)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.6607 -2.8240 -0.1325  3.0743 11.4207 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   3.4090     1.6321   2.089   0.0459 *  
x             2.7402     0.2803   9.777 1.58e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.582 on 28 degrees of freedom
Multiple R-squared:  0.7734,    Adjusted R-squared:  0.7653 
F-statistic: 95.58 on 1 and 28 DF,  p-value: 1.581e-10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualização: dados + reta verdadeira + reta ajustada}
\FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"X"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Y"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Linear: reta verdadeira vs MQO"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, sinal, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, }\FunctionTok{fitted}\NormalTok{(mod\_lin), }\AttributeTok{col =} \StringTok{"darkgreen"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}

\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"dados"}\NormalTok{, }\StringTok{"reta verdadeira"}\NormalTok{, }\StringTok{"ajuste MQO"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
       \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
       \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"darkgreen"}\NormalTok{),}
       \AttributeTok{bty =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{part1_ex_files/figure-pdf/2.CHUNK_03-1.pdf}}

Perguntas -- linear a) A correlação está próxima de 1? Por quê?\\
b) O coeficiente estimado da inclinação \(\beta_1\) ficou próximo do
valor verdadeiro?\\
c) Experimente:\\
- Aumente o ruído para \texttt{rnorm(30,\ mean\ =\ 0,\ sd\ =\ 10)} e
veja o que acontece com a correlação e o ajuste.\\
- Reduza o ruído para \texttt{rnorm(30,\ mean\ =\ 0,\ sd\ =\ 2)} e
observe a diferença.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Modelo polinomial quadrático
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 4) Modelo polinomial quadrático (sinal não linear)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{30}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n)}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \DecValTok{2}

\NormalTok{sinal }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{{-}} \FloatTok{0.05}\SpecialCharTok{*}\NormalTok{x}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ sinal }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =}\NormalTok{ sigma)}

\CommentTok{\# Visualização: pontos + curva verdadeira}
\FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"X"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Y"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Quadrático: sinal + ruído"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, sinal, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"dados"}\NormalTok{, }\StringTok{"curva verdadeira"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{), }\AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{), }\AttributeTok{bty =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{part1_ex_files/figure-pdf/2.CHUNK_04-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Correlação (linear) pode enganar em relação não linear}
\FunctionTok{cor}\NormalTok{(x, y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.8480377
\end{verbatim}

Perguntas -- Quadrático a) A correlação de Pearson reflete bem a relação
entre \texttt{x} e \texttt{y} neste caso? é linear?\\
b) Aumente o coeficiente do termo quadrático: troque \texttt{-0.05} por
\texttt{-0.5}. E agora?\\
c) Altere o sinal do termo quadrático: use \texttt{+0.5}. Como fica a
concavidade da curva?\\
d) Aumente o ruído (ex.: \texttt{sigma\ \textless{}-\ 4}). O que
acontece com a correlação e a visualização da curva?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Modelo Exponencial
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 5) Modelo exponencial com ruído multiplicativo}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{30}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n)}

\NormalTok{a }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{b }\OtherTok{\textless{}{-}} \FloatTok{0.2}
\NormalTok{sigma\_log }\OtherTok{\textless{}{-}} \FloatTok{0.2}  \CommentTok{\# ruído no log (multiplicativo em Y)}

\NormalTok{sinal }\OtherTok{\textless{}{-}}\NormalTok{ a }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(b}\SpecialCharTok{*}\NormalTok{x)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ sinal }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =}\NormalTok{ sigma\_log))}

\CommentTok{\# Visualização}
\FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"X"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Y"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Exponencial: sinal + ruído multiplicativo"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, sinal, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"dados"}\NormalTok{, }\StringTok{"curva verdadeira"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{), }\AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{), }\AttributeTok{bty =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{part1_ex_files/figure-pdf/2.CHUNK_05-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Correlação}
\FunctionTok{cor}\NormalTok{(x, y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.8255609
\end{verbatim}

Perguntas -- Exponencial a) A curva gerada com \(b = 0.2\) parece
linear? A correlação confirma isso?\\
b) Aumente o parâmetro de crescimento para \(b = 0.8\). A curva agora se
afasta de uma reta?\\
c) Aumente ainda mais para \(b = 1.5\). Como fica a forma da curva e a
correlação de Pearson?\\
d) Reduza o ruído (\texttt{0.2\ →\ 0.05}). O que muda na dispersão e na
clareza da forma exponencial?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Modelo Potência
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 6) Modelo potência com ruído multiplicativo}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{30}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n)}

\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{1.5}
\NormalTok{expoente }\OtherTok{\textless{}{-}} \FloatTok{1.1}
\NormalTok{sigma\_log }\OtherTok{\textless{}{-}} \FloatTok{0.1}

\NormalTok{sinal }\OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ (x}\SpecialCharTok{\^{}}\NormalTok{expoente)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ sinal }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =}\NormalTok{ sigma\_log))}

\CommentTok{\# Visualização}
\FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"X"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Y"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Potência: sinal + ruído multiplicativo"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, sinal, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"dados"}\NormalTok{, }\StringTok{"curva verdadeira"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{), }\AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{), }\AttributeTok{bty =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{part1_ex_files/figure-pdf/2.CHUNK_06-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Correlação}
\FunctionTok{cor}\NormalTok{(x, y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9769569
\end{verbatim}

Perguntas -- Potência a) Com expoente \(1.1\), a curva parece linear? A
correlação confirma isso?\\
b) Aumente o expoente para \(2.5\). A curva agora se distancia de uma
reta?\\
c) Teste com um expoente ainda maior, por exemplo \(3.5\). O que muda na
curvatura e na dispersão dos pontos?\\
d) Dobre o ruído (\texttt{0.1\ →\ 0.2}). O que acontece com a clareza da
curva e com a correlação de Pearson?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Exponencial --- linear vs.~polinomial
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 7) Exponencial — comparar ajuste linear vs polinomial (grau 2)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{30}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n)}

\NormalTok{a }\OtherTok{\textless{}{-}} \FloatTok{2.0}
\NormalTok{b }\OtherTok{\textless{}{-}} \FloatTok{0.2}        \CommentTok{\# depois teste 0.8 e 1.5}
\NormalTok{sigma\_log }\OtherTok{\textless{}{-}} \FloatTok{0.1}

\NormalTok{sinal }\OtherTok{\textless{}{-}}\NormalTok{ a }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(b}\SpecialCharTok{*}\NormalTok{x)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ sinal }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =}\NormalTok{ sigma\_log))}

\CommentTok{\# Ajustes}
\NormalTok{mod\_lin }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\NormalTok{mod\_poly }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}

\CommentTok{\# Comparação visual}
\FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"X"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Y"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Exponencial: linear vs polinomial (2)"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, sinal, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, }\FunctionTok{fitted}\NormalTok{(mod\_lin), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, }\FunctionTok{fitted}\NormalTok{(mod\_poly), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{3}\NormalTok{)}

\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"dados"}\NormalTok{, }\StringTok{"verdade"}\NormalTok{, }\StringTok{"linear"}\NormalTok{, }\StringTok{"polinomial (2)"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
       \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{),}
       \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"black"}\NormalTok{, }\StringTok{"black"}\NormalTok{),}
       \AttributeTok{bty =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{part1_ex_files/figure-pdf/2.CHUNK_07-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# R² (comparação rápida)}
\FunctionTok{c}\NormalTok{(}\AttributeTok{R2\_linear =} \FunctionTok{summary}\NormalTok{(mod\_lin)}\SpecialCharTok{$}\NormalTok{r.squared,}
  \AttributeTok{R2\_polinomial2 =} \FunctionTok{summary}\NormalTok{(mod\_poly)}\SpecialCharTok{$}\NormalTok{r.squared)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     R2_linear R2_polinomial2 
     0.7751012      0.7760694 
\end{verbatim}

Perguntas --- Exponencial (começando quase reta) a) Com \textbf{b =
0.2}, os ajustes \textbf{linear} e \textbf{polinomial (2)} parecem
semelhantes? O R² confirma? b) Aumente \textbf{b} para \textbf{0.8} e
depois \textbf{1.5}. Como mudam o gráfico e os R²? Qual ajuste passa a
representar melhor a curva? c) Reduza o ruído para
\texttt{sigma\_log\ \textless{}-\ 0.05}. Fica mais fácil perceber a
diferença entre o linear e o polinomial quando \textbf{b} é maior?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Potência --- linear vs.~polinomial
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 8) Potência — comparar ajuste linear vs polinomial (grau 2)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{30}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n)}

\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{1.5}
\NormalTok{expoente }\OtherTok{\textless{}{-}} \FloatTok{1.1}   \CommentTok{\# depois teste 2.5 e 3.5}
\NormalTok{sigma\_log }\OtherTok{\textless{}{-}} \FloatTok{0.1}

\NormalTok{sinal }\OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ (x}\SpecialCharTok{\^{}}\NormalTok{expoente)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ sinal }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =}\NormalTok{ sigma\_log))}

\CommentTok{\# Ajustes}
\NormalTok{mod\_lin }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\NormalTok{mod\_poly }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}

\CommentTok{\# Comparação visual}
\FunctionTok{plot}\NormalTok{(x, y, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"X"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Y"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Potência: linear vs polinomial (2)"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, sinal, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, }\FunctionTok{fitted}\NormalTok{(mod\_lin), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, }\FunctionTok{fitted}\NormalTok{(mod\_poly), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{3}\NormalTok{)}

\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{,}
       \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"dados"}\NormalTok{, }\StringTok{"verdade"}\NormalTok{, }\StringTok{"linear"}\NormalTok{, }\StringTok{"polinomial (2)"}\NormalTok{),}
       \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
       \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{),}
       \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"black"}\NormalTok{, }\StringTok{"black"}\NormalTok{),}
       \AttributeTok{bty =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{part1_ex_files/figure-pdf/2.CHUNK_08-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# R² (comparação rápida)}
\FunctionTok{c}\NormalTok{(}\AttributeTok{R2\_linear =} \FunctionTok{summary}\NormalTok{(mod\_lin)}\SpecialCharTok{$}\NormalTok{r.squared,}
  \AttributeTok{R2\_polinomial2 =} \FunctionTok{summary}\NormalTok{(mod\_poly)}\SpecialCharTok{$}\NormalTok{r.squared)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     R2_linear R2_polinomial2 
     0.9422860      0.9487419 
\end{verbatim}

Perguntas --- Potência (começando quase reta)

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  Com \textbf{expoente = 1.1}, os ajustes \textbf{linear} e
  \textbf{polinomial (2)} parecem semelhantes? O R² confirma?
\item
  Aumente o \textbf{expoente} para \textbf{2.5} e depois \textbf{3.5}.
  Como mudam o gráfico e os R²? Qual ajuste passa a representar melhor a
  curva?
\item
  Dobre o ruído para \texttt{sigma\_log\ \textless{}-\ 0.2}. Fica mais
  difícil perceber a diferença entre o linear e o polinomial quando o
  expoente é maior?
\end{enumerate}

\part{Parte II --- Modelo de Regressão Linear Simples (MRLS)}

\chapter{O MRLS como Modelo para a Média
Condicional}\label{o-mrls-como-modelo-para-a-muxe9dia-condicional}

A compreensão do \textbf{Modelo de Regressão Linear Simples (MRLS)} é
essencial para o estudo dos modelos de regressão. Sua importância não se
limita à simplicidade algébrica, mas repousa no fato de que ele
estabelece as bases conceituais para toda a teoria de modelagem
estatística. Ao assumir que a variação média de uma variável resposta
\(Y\) pode ser explicada por uma única variável explicativa \(X\), o
MRLS introduz a noção central de \textbf{média condicional}, isto é, a
ideia de que existe uma estrutura determinística que organiza o
comportamento médio dos dados, à qual se sobrepõe uma componente
aleatória que representa o ruído \emph{inevitável} das observações
empíricas.

Essa leitura ``pela média condicional'' é a forma mais precisa de
entender o que a regressão linear simples afirma: para cada valor fixado
de \(X\), existe uma distribuição de \(Y\), e o modelo especifica como a
média dessa distribuição varia com \(X\). (ver Montgomery, Peck, e
Vining (2021); Hoffmann (2016))

O intercepto e a inclinação da reta de regressão apresentadas
anteriormente traduzem a parte sistemática do fenômeno, enquanto o erro
agrega fatores não observados, variações aleatórias ou imprecisões de
medição. É nessa combinação entre regularidade e aleatoriedade que se
encontra a força do modelo: a regressão linear simples oferece uma
linguagem matemática capaz de quantificar associações e, ao mesmo tempo,
de reconhecer que o mundo real não se comporta de maneira perfeitamente
determinística.

Em particular, o termo ``erro'' não deve ser lido como ``falha'': ele
representa a parcela de variabilidade de \(Y\) que permanece mesmo
quando \(X\) é conhecido e o componente médio \(E(Y\mid X)\) foi
especificado. (ver Kutner et al. (2005))

O MRLS pode ser usado em diferentes perspectivas. Em um primeiro plano,
ele ajuda a compreender como uma variável se relaciona com outra,
permitindo isolar a contribuição média de \(X\) sobre \(Y\). Em seguida,
oferece meios de previsão, já que a reta ajustada pode ser utilizada
para estimar valores futuros ou não observados de \(Y\). Finalmente, ele
fornece um instrumento de controle, pois ao quantificar a variação
esperada em \(Y\) para uma mudança em \(X\), torna-se possível avaliar
de forma objetiva a influência de um fator específico mantendo os demais
aspectos fixos ou controlados no desenho do estudo. Essa tríade,
nomeadamente; explicação, predição e controle, sustenta a relevância
prática do modelo e justifica sua centralidade tanto no ensino quanto na
aplicação da estatística.

Um cuidado conceitual importante é distinguir ``prever o valor médio''
de ``prever uma observação individual'': mesmo que a média condicional
seja bem descrita, observações individuais ainda variam ao redor dessa
média por causa do erro aleatório. (ver Montgomery, Peck, e Vining
(2021))

A intuição do MRLS pode ser visualizada em gráficos de dispersão: os
pontos \((X,Y)\) representam as observações empíricas e, sobre esse
conjunto, a reta de regressão traduz a tendência média. As distâncias
verticais entre cada ponto e a reta correspondem aos resíduos, isto é,
às variações não capturadas pelo modelo. Adicionalmente, sabendo que a
relação entre \(X\) e \(Y\) pode assumir diferentes intensidades e
direções, mesmo dentro de um modelo linear simples, considere os quatro
cenários a seguir, todos baseados em uma reta verdadeira perturbada por
erros aleatórios:

\begin{itemize}
\tightlist
\item
  \textbf{(a) Correlação positiva forte}: a inclinação da reta é
  positiva e os pontos se distribuem próximos a ela. O sinal
  determinístico domina, e o ruído é pequeno em relação à estrutura
  média.\\
\item
  \textbf{(b) Correlação positiva fraca}: a reta mantém inclinação
  positiva, mas a dispersão em torno dela é elevada. O sinal ainda
  existe, mas é encoberto por grande variabilidade aleatória.\\
\item
  \textbf{(c) Correlação negativa forte}: a inclinação é negativa e os
  pontos se alinham de forma clara em torno da reta decrescente. A média
  condicional é bem definida e o erro exerce papel secundário.\\
\item
  \textbf{(d) Correlação negativa fraca}: a inclinação é negativa, porém
  os pontos apresentam grande dispersão em torno da reta. A
  variabilidade do erro é tão relevante quanto a estrutura
  determinística, tornando a associação menos evidente.
\end{itemize}

Essas quatro situações destacam a essência do \textbf{MRLS}:
independentemente da direção ou da força da associação, o modelo parte
da ideia de que a média condicional de \(Y\) pode ser descrita por uma
função linear em \(X\), à qual se soma um ruído \(\varepsilon_i\) com
\(E[\varepsilon_i \mid X_i]=0\).

Vale enfatizar por que essa condição é conceitualmente importante: ela
expressa que, uma vez fixado \(X_i\), o termo de erro não tem
\emph{tendência sistemática} (em média, não empurra \(Y\) para cima nem
para baixo), de modo que toda a variação média de \(Y\) com \(X\) fica
concentrada no termo \(E(Y\mid X)\). Quando essa condição falha, o que
se interpreta como ``efeito de \(X\)'' pode estar contaminado por
fatores omitidos que variam com \(X\) (situação que, em econometria,
está relacionada à ideia de endogeneidade). O gráfico, portanto,
antecipa de forma intuitiva a formulação matemática que será detalhada
na próxima seção.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_files/figure-pdf/unnamed-chunk-1-1.png}}

}

\caption{Quatro cenários de associação linear: (a) positiva forte, (b)
positiva fraca, (c) negativa forte, (d) negativa fraca.}

\end{figure}%

Essa representação gráfica simples, mas poderosa, revela a lógica do
modelo: a regressão não busca explicar cada observação em particular,
mas descrever o comportamento médio de \(Y\) em função de \(X\). É nesse
sentido que ela constitui uma primeira aproximação, um alicerce sobre o
qual se constroem modelos mais complexos. (ver Montgomery, Peck, e
Vining (2021))

\section{Formulação Matemática do
MRLS}\label{formulauxe7uxe3o-matemuxe1tica-do-mrls}

A formulação do modelo de regressão linear simples parte da ideia de que
cada observação \(Y_i\) pode ser decomposta em duas partes: uma
componente sistemática, que expressa o valor médio de \(Y\) condicionado
a um dado valor de \(X_i\), e uma componente aleatória, que traduz as
variações não explicadas. Em notação formal, escrevemos

\[
Y_i = \mu_i + \varepsilon_i, \quad \text{com} \quad \mu_i = E[Y_i \mid X_i],
\] em que \(\mu_i\) é a média condicional de \(Y\) dado \(X_i\), e
\(\varepsilon_i\) é o erro aleatório associado à observação \(i\).

Para que essa decomposição tenha interpretação estatística clara, é
conveniente explicitar as \textbf{suposições} (ou hipóteses) que
conectam o termo aleatório à componente sistemática. A hipótese central
é que o erro, em média, não carrega informação adicional além de
\(X_i\), de modo que

\[
E[\varepsilon_i \mid X_i] = 0.
\] Essa condição de exogeneidade fraca garante que a parte sistemática
do modelo seja, de fato, uma descrição da média condicional, e não uma
mistura entre efeito sistemático e ruído (ver Gujarati (2006)). Um modo
equivalente (e útil) de ler essa hipótese é: ao fixar \(X_i\), a média
de \(Y_i\) é exatamente \(\mu_i\), pois

\[
E(Y_i\mid X_i)=E(\mu_i+\varepsilon_i\mid X_i)=\mu_i+E(\varepsilon_i\mid X_i)=\mu_i.
\] Além da condição de média nula, frequentemente se acrescenta uma
hipótese sobre a dispersão do erro, que relaciona o modelo à variância
condicional de \(Y\):

\[
Var(\varepsilon_i\mid X_i)=\sigma^2.
\] Por fim, para que as observações tragam informação ``nova'' umas em
relação às outras e para que os resultados usuais de estimação e
inferência sejam válidos, costuma-se assumir ausência de dependência
linear entre erros de unidades distintas. Uma forma padrão de expressar
isso é impor \textbf{covariância nula} entre erros diferentes:

\[
Cov(\varepsilon_i,\varepsilon_j\mid X_i,X_j)=0,\quad \forall i\neq j,
\]

isto é, condicionando ao conjunto de regressores, os termos de erro não
apresentam associação linear entre observações distintas. Em muitos
textos, essa hipótese aparece na forma mais forte de independência entre
os erros; a condição de covariância nula é a expressão mínima necessária
para várias propriedades algébricas clássicas do modelo linear. (ver
Kutner et al. (2005); Montgomery, Peck, e Vining (2021))

Sob essas suposições, podemos derivar \textbf{propriedades imediatas} do
modelo:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \textbf{Média condicional}
\end{enumerate}

\[
E(Y_i\mid X_i)=E(\beta_0+\beta_1X_i+\varepsilon_i \mid X_i)=\beta_0+\beta_1X_i.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Variância condicional}
\end{enumerate}

\[
Var(Y_i\mid X_i)=E\left\{\left[Y_i-E(Y_i\mid X_i)\right]^2\mid X_i\right\}=E(\varepsilon_i^2\mid X_i)=Var(\varepsilon_i\mid X_i)=\sigma^2.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Covariância condicional entre observações distintas}
\end{enumerate}

Para \(i\neq j\), e condicionando ao conjunto de regressores), temos

\[
Cov(Y_i,Y_j\mid X_i,X_j)=Cov(\beta_0+\beta_1X_i+\varepsilon_i,\ \beta_0+\beta_1X_j+\varepsilon_j\mid X_i,X_j)=Cov(\varepsilon_i,\varepsilon_j\mid X_i,X_j).
\]

Assim, sob a hipótese \(Cov(\varepsilon_i,\varepsilon_j\mid X_i,X_j)=0\)
para \(i\neq j\), segue que

\[
Cov(Y_i,Y_j\mid X_i,X_j)=0,\quad i\neq j.
\]

A função de regressão do modelo é, portanto, a própria média condicional
(ou média do componente sistemático):

\[
\mu(X_i;\beta_0,\beta_1)=E(Y_i\mid X_i)=\beta_0+\beta_1X_i.
\]

No caso linear, supõe-se que a média condicional é uma função linear nos
parâmetros, de forma que

\[
E[Y_i \mid X_i] = \mu(X_i;\beta_0,\beta_1) = \beta_0 + \beta_1 X_i,
\]

o que leva à formulação completa do modelo:

\[
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i, \quad i=1,2,\dots,n.
\] Uma vez especificada a função \(\mu(X_i;\beta_0,\beta_1)\) e as
hipóteses sobre \(\varepsilon_i\), a tarefa estatística passa a ser
\textbf{estimar} \(E(Y_i\mid X_i)\) (isto é, a função de regressão) a
partir dos dados, tipicamente via métodos como mínimos quadrados (MMQ)
e, quando apropriado, máxima verossimilhança (MMV). (ver Kutner et al.
(2005); Montgomery, Peck, e Vining (2021))

\section{Interpretação dos Parâmetros e Hipóteses do
Modelo}\label{interpretauxe7uxe3o-dos-paruxe2metros-e-hipuxf3teses-do-modelo}

Nessa estrutura,

\[
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i,
\]

\(\beta_0\) representa o valor esperado de \(Y\) quando \(X=0\), isto é,

\[
E(Y\mid X=0)=\beta_0,
\]

enquanto \(\beta_1\) indica a taxa média de variação de \(Y\) a cada
incremento unitário em \(X\):

\[
E(Y\mid X=x+1)-E(Y\mid X=x)=\beta_1.
\]

Assim, \(\beta_1\) mede o efeito médio de uma variação unitária em \(X\)
sobre a média condicional de \(Y\). O erro \(\varepsilon_i\) traduz
tanto a variabilidade natural dos fenômenos quanto fatores não
observados, assumindo sempre que sua esperança condicional a \(X_i\)
seja nula.

É essencial notar que ``\(X=0\)'' pode não ter significado em alguns
contextos; ainda assim, \(\beta_0\) permanece necessário como parâmetro
de localização da reta. Quando \(0\) não pertence ao intervalo observado
de \(X\), ou quando não possui interpretação prática, pode-se redefinir
a variável explicativa por centralização (por exemplo,
\(X_i^\ast = X_i - \bar X\)), de modo que o novo intercepto represente o
valor esperado de \(Y\) em um ponto de referência mais informativo. (ver
Montgomery, Peck, e Vining (2021))

\section{Hipóteses do MRLS}\label{hipuxf3teses-do-mrls}

Para que o modelo tenha propriedades estatísticas bem definidas, é
conveniente explicitar as \textbf{hipóteses clássicas do MRLS},
usualmente apresentadas na literatura de modelos lineares (ver Kutner et
al. (2005)). Sob a formulação clássica, assumimos:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \textbf{Linearidade nos parâmetros}
\end{enumerate}

A função de regressão é linear nos parâmetros:

\[
E(Y_i\mid X_i)=\beta_0+\beta_1X_i.
\]

Observe que a linearidade refere-se aos parâmetros \(\beta_0,\beta_1\),
e não necessariamente à variável \(X\) em si.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Valores de \(X\) fixos (ou condicionais)}
\end{enumerate}

Os valores \(X_i\) são considerados fixos pelo planejamento do estudo,
ou, alternativamente, a análise é conduzida condicionalmente aos valores
observados de \(X\). Essa hipótese garante que toda a aleatoriedade do
modelo esteja concentrada no termo de erro.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Erro com média zero}
\end{enumerate}

\[
E(\varepsilon_i\mid X_i)=0.
\]

Essa condição assegura que o componente sistemático do modelo coincide
com a média condicional de \(Y\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Homoscedasticidade}
\end{enumerate}

A variância do erro é constante para todos os valores de \(X\):

\[
Var(\varepsilon_i\mid X_i)=\sigma^2.
\]

Consequentemente,

\[
Var(Y_i\mid X_i)=\sigma^2.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Ausência de correlação entre erros}
\end{enumerate}

Para \(i\neq j\),

\[
Cov(\varepsilon_i,\varepsilon_j\mid X)=0.
\]

Sob essa hipótese, segue que

\[
Cov(Y_i,Y_j\mid X)=0,\quad \forall i\neq j.
\]

Essas cinco condições compõem o conjunto clássico de hipóteses do modelo
linear simples conforme apresentado em textos de modelos lineares
aplicados (ver Kutner et al. (2005)).

\subsection{Hipóteses adicionais para
inferência}\label{hipuxf3teses-adicionais-para-inferuxeancia}

Até este ponto, não foi necessário supor nenhuma distribuição específica
para os erros. As hipóteses acima são suficientes para garantir
propriedades como não-viesamento dos estimadores de mínimos quadrados e
expressões fechadas para suas variâncias.

Para a construção de intervalos de confiança exatos e testes de
hipóteses com distribuição conhecida em amostras finitas, acrescenta-se
frequentemente a suposição de \textbf{normalidade dos erros}:

\[
\varepsilon_i \sim N(0,\sigma^2).
\]

Sob essa condição, o vetor de respostas possui distribuição normal
multivariada condicional a \(X\), o que permite derivar resultados
exatos para estatísticas \(t\) e \(F\).

É conceitualmente importante distinguir:

\begin{itemize}
\tightlist
\item
  \textbf{Hipóteses do modelo básico}: linearidade em
  \(\beta_0,\beta_1\) e \(E(\varepsilon\mid X)=0\);
\item
  \textbf{Hipóteses para eficiência e inferência exata}:
  homoscedasticidade, ausência de correlação e normalidade.
\end{itemize}

Essa distinção é enfatizada na literatura de regressão aplicada, que
separa claramente a estrutura do modelo da estrutura probabilística
necessária para inferência (ver Weisberg (2005)).

Por fim, essa formulação pode ser interpretada sob duas perspectivas
equivalentes. Na abordagem clássica, \(X\) é tratado como fixo. Em
contextos amostrais, pode-se admitir \(X\) aleatório, desde que se
mantenha a condição

\[
E(\varepsilon_i\mid X_i)=0,
\]

que garante a validade das propriedades do modelo condicionalmente a
\(X\). Em ambas as leituras, permanece a essência: a regressão linear
simples é um modelo para a média condicional de \(Y\) dado \(X\), e não
para cada observação individual. (ver Gujarati (2006))

\section{Representação Gráfica e Intuição
Geométrica}\label{representauxe7uxe3o-gruxe1fica-e-intuiuxe7uxe3o-geomuxe9trica}

A Figura a seguir ilustra o que significa afirmar que o \textbf{MRLS é
um modelo para a média condicional}. Os pontos azuis representam as
observações empíricas \((X_i,Y_i)\), que se espalham devido ao ruído
aleatório \(\varepsilon_i\). A reta vermelha mostra a estrutura
determinística \(\beta_0 + \beta_1 X\), isto é, o valor esperado de
\(Y\) para cada valor de \(X\). Já os círculos pretos conectados indicam
médias locais de \(Y\) em diferentes intervalos de \(X\), funcionando
como uma aproximação empírica de \(E[Y \mid X]\). O alinhamento dessas
médias com a reta reforça a ideia de que o modelo busca descrever a
\textbf{tendência média} e não cada observação individual.

Este gráfico torna visível a condição fundamental
\(E[\varepsilon_i \mid X_i]=0\). Embora cada ponto esteja sujeito a
variações não explicadas, quando olhamos para a média em cada faixa de
\(X\), os erros se compensam e a estrutura linear emerge. Assim, é
possível notar intuitivamente por que se fala em ``média condicional'' e
compreendemos que a regressão não elimina o ruído, mas organiza o
comportamento médio das observações em torno de uma reta. (ver Charnet
et al. (2008))

Mais adiante, quando abordarmos o método dos mínimos quadrados
ordinários, introduziremos outra visualização complementar, na qual os
resíduos aparecem como segmentos verticais entre os pontos observados e
a reta ajustada. Essas representações reforçam a interpretação
fundamental: o MRLS não pretende capturar cada realização individual,
mas descrever a tendência média de \(Y\) em função de \(X\), admitindo
explicitamente a presença de ruído.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_files/figure-pdf/unnamed-chunk-2-1.png}}

}

\caption{MRLS como média condicional: pontos observados (X,Y), reta
verdadeira (média condicional) e médias locais de Y por faixas de X.}

\end{figure}%

\chapter{Estimação por Mínimos Quadrados no
MRLS}\label{estimauxe7uxe3o-por-muxednimos-quadrados-no-mrls}

\section{Paradigmas de Estimação no
MRLS}\label{paradigmas-de-estimauxe7uxe3o-no-mrls}

A formulação do \textbf{Modelo de Regressão Linear Simples (MRLS)},
discutida anteriormente, descreve a estrutura da média condicional de
\(Y\) em função de \(X\), isto é,

\[
E(Y_i \mid X_i) = \beta_0 + \beta_1 X_i.
\]

O desafio agora é \textbf{estimar os parâmetros desconhecidos}
\(\beta_0\) e \(\beta_1\) a partir de dados observados
\(\{(X_i,Y_i)\}_{i=1}^n\). Esse processo de estimação pode ser realizado
via diferentes métodos, cada um apoiado em princípios e hipóteses
próprias.

A estimação pode ser conduzida sob diferentes \textbf{paradigmas}, isto
é, diferentes princípios fundamentais que definem o que significa
``estimar bem'' um parâmetro. Esses paradigmas não diferem apenas em
técnica, mas em filosofia estatística e nas hipóteses assumidas sobre o
modelo.

O \textbf{Método dos Mínimos Quadrados Ordinários (MQO)} é a abordagem
clássica no contexto da regressão linear. Seu princípio é puramente
geométrico e algébrico: escolher \(\hat\beta_0\) e \(\hat\beta_1\) de
modo a minimizar a soma dos quadrados dos resíduos,

\[
S(\beta_0,\beta_1) = \sum_{i=1}^n \left[Y_i - (\beta_0 + \beta_1 X_i)\right]^2.
\]

Esse critério não exige, para a obtenção dos estimadores, a
especificação de uma distribuição para os erros. A minimização conduz a
um sistema de equações conhecido como \textbf{equações normais}, que
caracteriza a solução de mínimos quadrados (ver Draper e Smith (1998);
Montgomery, Peck, e Vining (2021)). A ausência de suposição distributiva
mostra que o MQO é, antes de tudo, um procedimento de ajuste
determinístico baseado na estrutura linear do modelo.

Do ponto de vista estatístico, sob as hipóteses já apresentadas, a
saber, linearidade nos parâmetros, \(E(\varepsilon_i\mid X_i)=0\),
homoscedasticidade e ausência de correlação entre erros os estimadores
de MQO possuem propriedades fundamentais como não viés e variâncias com
forma explícita. Essas propriedades não dependem da normalidade dos
erros; a normalidade é necessária apenas quando se desejam distribuições
exatas em amostras finitas para testes e intervalos de confiança (ver
Kutner et al. (2005)). Assim, o MQO é um método de estimação que se
apoia primariamente na estrutura do modelo médio e nas condições de
regularidade, e não em hipóteses distributivas fortes.

Outro caminho é o \textbf{Método da Máxima Verossimilhança (MV)}. Nesse
paradigma, parte-se da especificação completa da distribuição
condicional de \(Y_i\mid X_i\), frequentemente assumindo

\[
\varepsilon_i \sim \mathcal{N}(0,\sigma^2),
\]

o que implica que \(Y_i\mid X_i\) também segue distribuição normal com
média \(\mu_i=\beta_0+\beta_1X_i\) e variância \(\sigma^2\). Os
estimadores são então definidos como aqueles que maximizam a função de
verossimilhança, isto é, a probabilidade conjunta dos dados observados
vista como função dos parâmetros. Quando o modelo probabilístico está
corretamente especificado, a MV produz estimadores consistentes,
assintoticamente normais e eficientes sob condições regulares (ver
Casella e Berger (2002)).

No caso particular do modelo linear com erros normais homoscedásticos e
não correlacionados, os estimadores de máxima verossimilhança coincidem
com os estimadores de mínimos quadrados. Essa coincidência não é
acidental: a minimização da soma de quadrados é equivalente à
maximização da verossimilhança normal. Contudo, conceitualmente, os dois
métodos partem de princípios distintos, um geométrico/algebraico e outro
probabilístico.

Uma terceira alternativa são os \textbf{métodos bayesianos}, nos quais
os parâmetros \(\beta_0\) e \(\beta_1\) são tratados como variáveis
aleatórias. Nesse caso, especifica-se uma distribuição a priori conjunta
para \(\beta_0\) e \(\beta_1\) e combina-se essa informação com a
verossimilhança dos dados por meio do Teorema de Bayes, obtendo-se a
distribuição a posteriori

\[
p(\beta_1,\beta_2 \mid y,X) \propto p(y,X \mid \beta_1,\beta_2)\, p(\beta_1,\beta_2).
\]

A estimação passa então a ser baseada em características dessa
distribuição a posteriori (como média, mediana ou moda). Esse paradigma
explicita a incerteza sobre os parâmetros e permite incorporar
informação prévia de forma formal (ver Casella e Berger (2002); Gelman
et al. (2014)).

Portanto, a estimação no MRLS pode ser conduzida sob diferentes
paradigmas: minimização de resíduos (MQO), maximização da
verossimilhança (MV) ou atualização bayesiana de crenças. Cada abordagem
parte de fundamentos conceituais distintos, tais quais,
geométrico-algébrico, probabilístico ou epistemológico, e conduz a
interpretações próprias dos parâmetros e da incerteza associada.

Neste livro, a \textbf{estimação por mínimos quadrados ordinários (MQO)}
receberá tratamento mais detalhado e sistemático. A razão é dupla: em
primeiro lugar, o MQO não exige a especificação de uma distribuição para
os erros para a obtenção dos estimadores, apoiando-se apenas na
estrutura do modelo médio e nas hipóteses clássicas de exogeneidade e
regularidade; em segundo lugar, ele constitui a base do Teorema de
Gauss--Markov e de grande parte da teoria dos modelos lineares, servindo
como alicerce conceitual para extensões posteriores.

A \textbf{máxima verossimilhança (MV)} também será contemplada,
sobretudo quando discutirmos aspectos inferenciais e conexões entre
estrutura probabilística e eficiência assintótica. No caso do modelo
linear com erros normais, veremos inclusive a coincidência formal entre
MQO e MV, o que reforça a unidade conceitual entre os métodos sob
hipóteses adicionais.

Por outro lado, embora o paradigma \textbf{bayesiano} seja
conceitualmente relevante e metodologicamente poderoso, sua abordagem
completa exigiria o desenvolvimento de ferramentas próprias, como
escolha de distribuições a priori, análise da posteriori e métodos
computacionais, que extrapolam os objetivos centrais deste texto. Assim,
ele será mencionado para fins de contextualização, mas não será
desenvolvido formalmente neste livro.

\section{O Critério dos Mínimos Quadrados
Ordinários}\label{o-crituxe9rio-dos-muxednimos-quadrados-ordinuxe1rios}

O Método dos Mínimos Quadrados Ordinários (MQO) é a abordagem clássica
para a estimação em regressão linear. Seu objetivo é encontrar a reta
que melhor descreve a relação média entre a variável resposta \(Y\) e a
variável explicativa \(X\). Essa ``melhor'' reta é definida como aquela
que minimiza a soma dos quadrados dos \textbf{resíduos}, isto é, das
diferenças entre os valores observados e os valores ajustados pelo
modelo (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).

Se denotarmos por \(\hat{Y}_i\) o valor ajustado para a observação
\(i\), temos:

\[
\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i,
\]

e o resíduo correspondente é

\[
e_i = Y_i - \hat{Y}_i.
\] O critério de mínimos quadrados escolhe os parâmetros que minimizam a
função de perda quadrática

\[
S(\beta_0, \beta_1) = \sum_{i=1}^n \left[ Y_i - (\beta_0 + \beta_1 X_i) \right]^2.
\]

Do ponto de vista matemático, trata-se de um problema clássico de
otimização: encontrar \((\hat\beta_0,\hat\beta_1)\) que minimizam
\(S(\beta_0,\beta_1)\) sobre \(\mathbb{R}^2\). A condição de primeira
ordem leva a um sistema de duas equações lineares nas incógnitas
\(\beta_0\) e \(\beta_1\), conhecido como \textbf{equações normais}.
Essas equações caracterizam completamente a solução de mínimos quadrados
no modelo linear simples (ver Draper e Smith (1998)).

Esse procedimento garante que, entre todas as retas possíveis, a
escolhida é aquela que deixa os resíduos, em conjunto, ``o mais curtos
possível'' no sentido quadrático. A escolha da penalização quadrática
não é arbitrária: a função objetivo é uma função polinomial de segundo
grau nos parâmetros, contínua e diferenciável, e admite solução única
sempre que os valores de \(X\) não forem todos iguais, isto é, sempre
que houver variabilidade na variável explicativa. Essa condição assegura
a existência e a unicidade da reta de mínimos quadrados.

Além disso, a penalização pelo quadrado dos desvios atribui maior peso a
observações mais afastadas, o que explica tanto a eficiência do método
sob hipóteses clássicas quanto sua sensibilidade a valores discrepantes
(ver Weisberg (2005)).

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_emq_files/figure-pdf/unnamed-chunk-1-1.png}}

}

\caption{MQO: reta ajustada e resíduos destacados.}

\end{figure}%

A figura acima ilustra essa lógica. Os pontos azuis representam as
observações \((X_i,Y_i)\), a reta vermelha mostra a reta ajustada pelo
MQO, e as linhas tracejadas cinzas indicam os resíduos associados a cada
ponto. Visualmente, o MQO busca a reta que minimiza a soma dos quadrados
dessas distâncias verticais. Essa interpretação geométrica ajuda a
compreender que a regressão não elimina o erro, mas organiza o ruído de
forma a recuperar a estrutura média do fenômeno.

Essa formulação admite duas interpretações complementares.

\begin{itemize}
\tightlist
\item
  \textbf{Geométrica}: o MQO pode ser visto como a projeção ortogonal do
  vetor de respostas \(\mathbf{Y}=(Y_1,\ldots,Y_n)'\) no subespaço
  gerado pelos vetores \(\mathbf{1}=(1,\ldots,1)'\) e
  \(\mathbf{X}=(X_1,\ldots,X_n)'\). A condição de minimização implica
  que o vetor de resíduos \(\hat\varepsilon = Y-\hat Y\) é ortogonal ao
  espaço gerado pelos regressores, isto é,
\end{itemize}

\[
\sum_{i=1}^n \hat\varepsilon_i = 0
\quad \text{e} \quad
\sum_{i=1}^n X_i \hat\varepsilon_i = 0.
\]

Essas duas condições são precisamente as equações normais no caso
simples.

\begin{itemize}
\tightlist
\item
  \textbf{Estatística}: a ortogonalidade amostral dos resíduos aos
  regressores é o análogo empírico da hipótese populacional
\end{itemize}

\[
E(\varepsilon_i \mid X_i)=0.
\] Em outras palavras, após o ajuste, não resta componente linear em
\(X\) capaz de explicar sistematicamente os resíduos. A condição
populacional de exogeneidade é refletida, no nível amostral, pela
ortogonalidade dos resíduos estimados (ver Kutner et al. (2005)).

Um aspecto central é que o MQO não exige, para a obtenção dos
estimadores, a especificação de uma distribuição para os erros. Sob as
hipóteses de média condicional corretamente especificada,
homoscedasticidade e ausência de correlação entre erros, os estimadores
resultantes são não viesados e apresentam variâncias com forma
explícita, propriedades que independem da normalidade (ver Montgomery,
Peck, e Vining (2021)).

A normalidade é introduzida apenas quando se desejam distribuições
exatas em amostras finitas para estatísticas de teste e construção de
intervalos de confiança. Em contextos práticos com caudas pesadas ou
observações discrepantes, podem ser considerados métodos robustos ou
funções de perda alternativas. Essa generalidade explica por que o MQO
constitui o ponto de partida natural e o método mais amplamente ensinado
e utilizado na análise de regressão linear.

\newpage

\section{Solução Analítica: A Reta de Regressão por
MQO}\label{soluuxe7uxe3o-analuxedtica-a-reta-de-regressuxe3o-por-mqo}

\begin{tcolorbox}[enhanced jigsaw, titlerule=0mm, bottomrule=.15mm, opacityback=0, coltitle=black, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Teorema --- Reta de Regressão por MQO}, breakable, toptitle=1mm, left=2mm, colframe=quarto-callout-important-color-frame, opacitybacktitle=0.6, arc=.35mm, leftrule=.75mm, colback=white, bottomtitle=1mm, rightrule=.15mm, toprule=.15mm]

No modelo de regressão linear simples

\[
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i, \quad i = 1,2,\dots,n,
\]

os estimadores de mínimos quadrados ordinários (MQO) de \(\beta_0\) e
\(\beta_1\) são obtidos como aqueles que \textbf{minimizam a soma dos
quadrados dos resíduos}. A solução do problema de minimização leva às
formas fechadas:

\[
\hat{\beta}_1 = \frac{S_{xy}}{S_{xx}}, 
\qquad 
\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X},
\]

onde

\[
S_{xx}=\sum_{i=1}^n (X_i-\bar X)^2, 
\qquad 
S_{xy}=\sum_{i=1}^n (X_i-\bar X)(Y_i-\bar Y).
\]

\end{tcolorbox}

A demonstração resulta da minimização da função \[
S(\beta_0,\beta_1)=\sum_{i=1}^n \left[Y_i-(\beta_0+\beta_1X_i)\right]^2,
\]

por meio do cálculo das derivadas parciais em relação a \(\beta_0\) e
\(\beta_1\) e da resolução do sistema de equações normais
correspondente. A dedução algébrica completa pode ser consultada no
Apêndice de Demonstrações \{\#demo\}.

Esse resultado estabelece a \textbf{reta de regressão por MQO} como a
linha que, ao mesmo tempo, minimiza a soma dos quadrados dos resíduos e
traduz o padrão médio de associação entre \(X\) e \(Y\). A estrutura
explícita das soluções mostra que a existência e a unicidade dependem
apenas de \(S_{xx}>0\), isto é, da presença de variabilidade em \(X\),
condição necessária para que a informação sobre a inclinação seja
identificável (ver Kutner et al. (2005); Montgomery, Peck, e Vining
(2021)).

Do ponto de vista matemático, a minimização de \(S(\beta_0,\beta_1)\)
conduz a uma função quadrática estritamente convexa nos parâmetros
quando \(S_{xx}>0\), assegurando que a solução encontrada pelas equações
normais seja única. A demonstração detalhada dessa propriedade pode ser
consultada no Apêndice de Demonstrações \{\#demo\}.

No entanto, conhecer a forma explícita da reta ajustada é apenas o
primeiro passo. A expressão fechada dos estimadores revela como eles
dependem das quantidades amostrais, mas não informa, por si só, se tais
estimadores são centrados nos verdadeiros parâmetros, quão precisos são
ou como se comportam sob repetição amostral. Para que possamos confiar
nesses estimadores e utilizá-los em inferência estatística, precisamos
examinar suas \textbf{propriedades probabilísticas}: não viés,
variâncias, covariância entre \(\hat\beta_0\) e \(\hat\beta_1\) e
qualidade das predições produzidas. É justamente esse o foco da próxima
seção (ver Kutner et al. (2005)).

\subsection{Interpretação dos Estimadores obtidos via
MQO}\label{interpretauxe7uxe3o-dos-estimadores-obtidos-via-mqo}

\begin{itemize}
\tightlist
\item
  O estimador da inclinação pode ser reescrito como
\end{itemize}

\[
\hat\beta_1 = \frac{\sum_{i=1}^n (X_i-\bar X)(Y_i-\bar Y)}
{\sum_{i=1}^n (X_i-\bar X)^2},
\]

o que evidencia que ele corresponde à \textbf{covariância amostral entre
\(X\) e \(Y\) dividida pela variância amostral de \(X\)}. Essa forma
deixa claro que \(\hat\beta_1\) mede a variação média de \(Y\) associada
a um aumento unitário em \(X\), sendo proporcional ao grau de associação
linear entre as duas variáveis (ver Montgomery, Peck, e Vining (2021)).

\begin{itemize}
\tightlist
\item
  O estimador do intercepto,
\end{itemize}

\[
\hat\beta_0 = \bar Y - \hat\beta_1 \bar X,
\]

implica que a reta ajustada satisfaz

\[
\hat Y(\bar X)=\bar Y,
\]

ou seja, a reta de regressão \textbf{passa necessariamente pelo ponto
médio amostral} \((\bar X,\bar Y)\). Essa propriedade decorre
diretamente das equações normais e da ortogonalidade dos resíduos aos
regressores (ver Kutner et al. (2005)).

\section{Propriedades Probabilísticas dos Estimadores de
MQO}\label{propriedades-probabiluxedsticas-dos-estimadores-de-mqo}

Nesta seção reunimos as propriedades essenciais dos estimadores de
mínimos quadrados no modelo

\[
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i, \quad i = 1,2,\dots,n,
\] sob as hipóteses usuais de exogeneidade fraca e regularidade: \[
E[\varepsilon_i \mid X_i]=0, 
\qquad 
Var(\varepsilon_i\mid X_i)=\sigma^2,
\qquad 
Cov(\varepsilon_i,\varepsilon_j\mid X_i,X_j)=0 \ \forall(i \neq j),
\]

com

\[
S_{xx}=\sum_{i=1}^n (X_i-\bar X)^2>0.
\]

Essas condições são suficientes para estabelecer as principais
propriedades dos estimadores de MQO, sem necessidade de assumir
normalidade dos erros. Trata-se exatamente do conjunto de hipóteses sob
o qual se desenvolve a teoria clássica do modelo linear (ver Kutner et
al. (2005); Montgomery, Peck, e Vining (2021)).

Os estimadores de mínimos quadrados ordinários (MQO)

\[
\hat{\beta}_1 = \frac{S_{xy}}{S_{xx}}, 
\qquad 
\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X},
\]

em que

\[
S_{xy}=\sum_{i=1}^n (X_i-\bar X)(Y_i-\bar Y), 
\qquad 
\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i,
\qquad 
\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i,
\] possuem propriedades importantes, que garantem sua validade para
inferência estatística.

\subsection{Não viés}\label{nuxe3o-viuxe9s}

Ambos os estimadores são não viesados:

\[
E[\hat{\beta}_0] = \beta_0 
\qquad \text{e} \qquad 
E[\hat{\beta}_1] = \beta_1.
\] Um estimador é dito \textbf{não viesado} quando sua esperança
coincide com o parâmetro verdadeiro. No presente caso, os estimadores
\(\hat\beta_0\) e \(\hat\beta_1\) são \textbf{centrados} em \(\beta_0\)
e \(\beta_1\), respectivamente. Em termos frequentistas, isso significa
que, sob repetição hipotética do processo amostral nas mesmas condições,
a média das estimativas convergiria para os valores verdadeiros.

A demonstração formal desse resultado baseia-se na linearidade do
operador esperança e na hipótese de exogeneidade fraca
\(E[\varepsilon_i\mid X_i]=0\), e pode ser consultada no Apêndice de
Demonstrações \{\#demo\}. Conceitualmente, o ponto central é que, ao
condicionar em \(X\), o erro não contém componente sistemática capaz de
deslocar, em média, os estimadores.

\subsection{Variâncias e covariância dos
estimadores}\label{variuxe2ncias-e-covariuxe2ncia-dos-estimadores}

As variâncias dos estimadores são dadas por

\[
Var(\hat{\beta}_0) =  \left(\frac{1}{n} + \frac{\bar{X}^2}{S_{xx}}\right)\sigma^2,
\qquad 
Var(\hat{\beta}_1) = \frac{1}{S_{xx}}\sigma^2,
\]

e a covariância entre eles é

\[
Cov(\hat{\beta}_0,\hat{\beta}_1) = - \frac{\bar{X}}{S_{xx}}\sigma^2.
\]

Essas expressões decorrem diretamente da representação linear dos
estimadores em função dos \(Y_i\) e das hipóteses sobre a estrutura de
variância-covariância dos erros. A demonstração detalhada também pode
ser vista no Apêndice de Demonstrações \{\#demo\} (ver Kutner et al.
(2005)).

Algumas interpretações conceituais importantes emergem dessas fórmulas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \textbf{Influência da dispersão de \(X\)}: quanto maior \(S_{xx}\),
  menor \(Var(\hat{\beta}_1)\). Portanto, amostras com maior
  variabilidade em \(X\) contêm mais informação sobre a inclinação da
  reta. Se os valores de \(X\) estiverem muito concentrados, a
  estimativa da inclinação torna-se imprecisa.
\item
  \textbf{Dependência do intercepto em relação à origem}: a variância de
  \(\hat{\beta}_0\) depende de \(\bar X\). Quanto mais distante a média
  de \(X\) estiver da origem, maior será a variância do intercepto,
  refletindo o fato de que \(\hat\beta_0\) é obtido por extrapolação da
  reta até \(X=0\).
\item
  \textbf{Covariância negativa}: quando \(\bar X>0\), a covariância
  entre \(\hat\beta_0\) e \(\hat\beta_1\) é negativa. Isso indica que
  uma estimativa maior da inclinação tende a ser compensada por uma
  estimativa menor do intercepto, preservando a propriedade geométrica
  de que a reta ajustada passa por \((\bar X,\bar Y)\).
\end{enumerate}

Do ponto de vista geométrico, essas propriedades decorrem da
ortogonalidade dos resíduos aos regressores, isto é,

\[
\sum_{i=1}^n \hat\varepsilon_i = 0 
\qquad \text{e} \qquad 
\sum_{i=1}^n X_i \hat\varepsilon_i = 0.
\]

Essas condições são equivalentes às equações normais e garantem que a
projeção de \(Y\) sobre o subespaço gerado por \(1\) e \(X\) seja
ortogonal ao vetor de resíduos. A conexão entre ortogonalidade e
estrutura de variâncias é discutida em textos clássicos de regressão
linear (ver Montgomery, Peck, e Vining (2021)).

\subsection{\texorpdfstring{Estimativa de \(\sigma^2\) (graus de
liberdade e não
viés)}{Estimativa de \textbackslash sigma\^{}2 (graus de liberdade e não viés)}}\label{estimativa-de-sigma2-graus-de-liberdade-e-nuxe3o-viuxe9s}

Definindo a \textbf{soma dos quadrados dos resíduos} como

\[
SQRes = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2,
\]

temos que

\[
E[SQRes] = (n-2)\sigma^2.
\] A demonstração desse resultado utiliza a decomposição ortogonal do
vetor \(Y\) em componente ajustada e componente residual, podendo ser
consultada no Apêndice de Demonstrações \{\#demo\}.

Assim, o estimador

\[
s^2 = \frac{SQRes}{n-2}
\]

é \textbf{não viesado} para a variância dos erros:

\[
E[s^2] = \sigma^2.
\] Os dois graus de liberdade subtraídos refletem a estimação dos dois
parâmetros do modelo \((\beta_0, \beta_1)\). Essa correção garante que a
variabilidade residual não seja subestimada pelo fato de termos ajustado
uma reta aos dados.

Na prática, substitui-se \(\sigma^2\) por \(s^2\) nas expressões de
\(Var(\hat\beta_0)\) e \(Var(\hat\beta_1)\), obtendo-se estimativas dos
erros-padrão. Observe que até aqui \textbf{não foi necessária a
suposição de normalidade}: as propriedades de não viés e as fórmulas de
variância decorrem apenas das hipóteses de média zero,
homoscedasticidade e ausência de correlação entre erros (ver Kutner et
al. (2005); Montgomery, Peck, e Vining (2021)).

Portanto, os estimadores de MQO no MRLS apresentam um conjunto de
propriedades fundamentais: são \textbf{não viesados}, possuem
\textbf{variâncias explicitamente caracterizadas}, exibem
\textbf{covariância estrutural negativa} entre intercepto e inclinação e
permitem a construção de um \textbf{estimador não viesado de
\(\sigma^2\)} a partir dos resíduos.

Essas características asseguram a solidez probabilística do método sob
hipóteses relativamente gerais e preparam o terreno para a próxima
questão natural: dentro da classe dos estimadores lineares não viesados,
seria possível obter variâncias menores? O \textbf{Teorema de
Gauss--Markov} responde negativamente a essa pergunta, estabelecendo a
eficiência relativa do MQO.

\section{Teorema de Gauss--Markov}\label{teorema-de-gaussmarkov}

\begin{tcolorbox}[enhanced jigsaw, titlerule=0mm, bottomrule=.15mm, opacityback=0, coltitle=black, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Teorema (Gauss--Markov)}, breakable, toptitle=1mm, left=2mm, colframe=quarto-callout-important-color-frame, opacitybacktitle=0.6, arc=.35mm, leftrule=.75mm, colback=white, bottomtitle=1mm, rightrule=.15mm, toprule=.15mm]

No modelo de regressão linear simples

\[
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i, \quad i=1,2,\dots,n,
\] sob as hipóteses

\[
E[\varepsilon_i\mid X_i]=0, 
\qquad 
Var(\varepsilon_i\mid X_i)=\sigma^2,
\qquad 
Cov(\varepsilon_i,\varepsilon_j\mid X_i,X_j)=0 \ (\forall i\neq j),
\]

os estimadores de mínimos quadrados ordinários são \textbf{lineares em
\(Y\)}, \textbf{não viesados} e possuem \textbf{variância mínima} dentro
da classe de todos os estimadores lineares não viesados dos parâmetros
\(\beta_0\) e \(\beta_1\).

\end{tcolorbox}

Em outras palavras, se restringirmos nossa atenção a estimadores que
sejam combinações lineares das observações \(Y_i\) e que sejam não
viesados para os parâmetros verdadeiros, então nenhum outro estimador
dessa classe terá variância menor que a dos estimadores de MQO. Essa é a
essência do qualificativo \emph{best}: não significa ``melhor entre
todos os estimadores possíveis'', mas ``melhor dentro da classe dos
estimadores lineares não viesados''.

Este teorema organiza três ideias fundamentais:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \textbf{Linearidade do estimador}: o estimador pode ser escrito como
  combinação linear das respostas observadas.\\
\item
  \textbf{Não viés}: sua esperança coincide com o parâmetro
  verdadeiro.\\
\item
  \textbf{Eficiência relativa}: entre todos os estimadores que
  satisfazem (1) e (2), o MQO apresenta a menor variância.
\end{enumerate}

O resultado não depende da normalidade dos erros. Essa é uma distinção
crucial: a normalidade é necessária apenas quando se deseja obter
distribuições exatas finitas para estatísticas. A propriedade BLUE
decorre exclusivamente da estrutura de média e variância do modelo
linear clássico (ver Kutner et al. (2005); Montgomery, Peck, e Vining
(2021)).

Geometricamente, o teorema está intimamente ligado à interpretação do
MQO como projeção ortogonal do vetor \(Y\) no subespaço gerado pelos
regressores. A projeção ortogonal é, por construção, o vetor ajustado
que minimiza a distância quadrática a \(Y\). A minimização da distância
quadrática no espaço amostral se traduz, no plano probabilístico, em
minimização da variância entre estimadores lineares não viesados. Essa
ponte entre geometria e probabilidade é um dos aspectos mais profundos
do modelo linear.

É importante enfatizar também o alcance do resultado. O teorema não
afirma que o MQO é o estimador de menor variância entre todos os
estimadores imagináveis. Métodos não lineares ou estimadores viesados
podem, em certos contextos, apresentar menor erro quadrático médio. O
que o Teorema de Gauss--Markov garante é a \textbf{otimalidade dentro da
classe linear não viesada}, uma classe ampla e natural no contexto da
regressão.

A demonstração formal do teorema, baseada em argumentos de decomposição
de variância e ortogonalidade, pode ser consultada no Apêndice de
Demonstrações \{\#demo\}.

Em termos práticos, o teorema fornece a base teórica que sustenta o uso
do MQO como método padrão de estimação em regressão linear. Ele mostra
que, sob hipóteses relativamente fracas e sem necessidade de
normalidade, o procedimento adotado é eficiente dentro de uma classe
ampla de estimadores. Essa combinação de simplicidade algébrica,
interpretação geométrica clara e fundamentação probabilística sólida
explica por que o MQO ocupa posição central na estatística aplicada e na
econometria.

\chapter{Inferência no MRLS com erros
normais}\label{inferuxeancia-no-mrls-com-erros-normais}

\section{Por que assumir
normalidade?}\label{por-que-assumir-normalidade}

Até aqui, estudamos as propriedades dos estimadores de mínimos quadrados
ordinários (MQO) no Modelo de Regressão Linear Simples (MRLS). Mostramos
que \(\hat\beta_0\) e \(\hat\beta_1\) são \textbf{não viesados}, possuem
\textbf{variâncias explícitas} e, pelo \textbf{Teorema de
Gauss--Markov}, são os \textbf{melhores estimadores lineares não
viesados (BLUE)} sob as hipóteses clássicas de exogeneidade,
homoscedasticidade e independência dos erros (ver Kutner et al. (2005);
Montgomery, Peck, e Vining (2021)).

No entanto, até este ponto conhecemos apenas \textbf{momentos de
primeira e segunda ordem} das distribuições amostrais dos estimadores,
ou seja, suas esperanças e variâncias. Não conhecemos suas
\textbf{distribuições exatas}. De inferência, já sabemos que um
estimador ser não viesado e eficiente dentro de uma classe não é
suficiente para construir intervalos de confiança exatos ou realizar
testes de hipóteses com nível de significância controlado em amostras
finitas.

Para superar essa limitação, acrescentamos uma hipótese mais forte e
específica: a \textbf{normalidade dos erros},

\[
\varepsilon_i \sim N(0,\sigma^2), 
\quad i=1,2,\dots,n, 
\quad \text{independentes}.
\]

Ou seja, cada erro segue uma \textbf{distribuição normal} com média zero
e variância constante \(\sigma^2>0\), sendo ainda independentes entre
si.

\subsection{Estrutura probabilística do MRLS com erros
normais}\label{estrutura-probabiluxedstica-do-mrls-com-erros-normais}

Com essa suposição adicional, a formulação probabilística do modelo
passa a ser

\[
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i, 
\quad \varepsilon_i \sim N(0,\sigma^2).
\]

Logo, condicionalmente a \(X_i\), temos

\[
Y_i \mid X_i \sim N(\beta_0 + \beta_1 X_i, \, \sigma^2).
\]

Isso significa que o modelo deixa de ser apenas um modelo para a média
condicional e passa a especificar completamente a \textbf{distribuição
condicional de \(Y\) dado \(X\)}. Em outras palavras, a normalidade
fornece não apenas a forma do valor esperado, mas também a forma
funcional da incerteza em torno dessa média.

\subsection{Consequências conceituais da
normalidade}\label{consequuxeancias-conceituais-da-normalidade}

A introdução da hipótese de normalidade tem implicações precisas:

\begin{itemize}
\item
  O \textbf{MRLS continua sendo um modelo para a média condicional}, mas
  agora a variabilidade em torno dessa média é descrita por uma
  estrutura probabilística completamente especificada.
\item
  As hipóteses de Gauss--Markov já asseguravam que os estimadores de MQO
  eram BLUE, mas \textbf{não determinavam suas distribuições exatas}. A
  normalidade preenche exatamente essa lacuna.
\item
  Como combinações lineares de variáveis normais são normais, os
  estimadores \(\hat\beta_0\) e \(\hat\beta_1\), que são combinações
  lineares dos \(Y_i\), passam a ter distribuições normais exatas em
  amostras finitas (ver Montgomery, Peck, e Vining (2021)).
\item
  A estatística baseada na soma dos quadrados dos resíduos passa a ter
  distribuição qui-quadrado, o que permite derivar distribuições \(t\) e
  \(F\) de forma exata (ver Kutner et al. (2005)).
\end{itemize}

Do ponto de vista metodológico, a normalidade não é necessária para a
obtenção das propriedades de não viés ou eficiência relativa, mas é uma
boa alternativa para a construção de \textbf{procedimentos inferenciais
exatos em amostras finitas}. Essa hipótese não altera os estimadores de
MQO, mas altera o que podemos afirmar sobre sua variabilidade e sobre a
incerteza associada às estimativas.

Portanto, a introdução da normalidade transforma o MRLS de um modelo com
propriedades ótimas em termos de média e variância em um modelo com
\textbf{estrutura probabilística completa}, apto a sustentar intervalos
de confiança e testes de hipóteses com boas propriedades.

\section{Distribuições amostrais no MRLS com erros
normais}\label{distribuiuxe7uxf5es-amostrais-no-mrls-com-erros-normais}

Sob a hipótese adicional de normalidade dos erros no MRLS, podemos
derivar as \textbf{distribuições amostrais exatas} dos principais
estimadores do modelo. Esse é o ponto de transição entre propriedades
puramente algébricas (não viés, variância mínima dentro de uma classe) e
\textbf{inferência estatística formal}.

Recordemos que, sob normalidade,

\[
\varepsilon_i \sim N(0,\sigma^2), \quad \text{independentes}.
\]

Como os estimadores de MQO podem ser escritos como \textbf{combinações
lineares dos \(Y_i\)}, e cada \(Y_i\) é normal condicionalmente a
\(X_i\), segue que \(\hat\beta_0\) e \(\hat\beta_1\) são também
normalmente distribuídos. Essa conclusão decorre do fato fundamental de
que combinações lineares de variáveis normais independentes permanecem
normais (ver Montgomery, Peck, e Vining (2021)).

\subsection{\texorpdfstring{Distribuição de \(\hat\beta_0\),
\(\hat\beta_1\)}{Distribuição de \textbackslash hat\textbackslash beta\_0, \textbackslash hat\textbackslash beta\_1}}\label{distribuiuxe7uxe3o-de-hatbeta_0-hatbeta_1}

Para a inclinação, obtemos:

\[
\hat\beta_1 \sim N\!\left(\beta_1, \, \frac{\sigma^2}{S_{xx}}\right),
\qquad 
S_{xx} = \sum_{i=1}^n (X_i - \bar X)^2.
\]

Para o intercepto:

\[
\hat\beta_0 \sim N\!\left(\beta_0, \, \sigma^2\left(\frac{1}{n}+\frac{\bar X^2}{S_{xx}}\right)\right).
\]

A demonstração dessas distribuições pode ser vista no Apêndice de
Demonstrações \{\#demo\}, onde se explora explicitamente a representação
linear dos estimadores em função dos \(Y_i\) e a estrutura de
variância-covariância do vetor de respostas.

Além disso, para o estimador da variância residual,

\[
s^2 = \frac{SQRes}{n-2}, 
\qquad 
SQRes = \sum_{i=1}^n (Y_i - \hat Y_i)^2,
\]

vale o resultado fundamental:

\[
\frac{(n-2)s^2}{\sigma^2} \sim \chi^2_{n-2}.
\]

Esse resultado decorre da decomposição ortogonal do vetor \(Y\) em
componente ajustada e componente residual, cuja demonstração também pode
ser consultada no Apêndice \{\#demo\} (ver Kutner et al. (2005)). A
perda de dois graus de liberdade reflete a estimação dos dois parâmetros
\(\beta_0\) e \(\beta_1\).

É importante destacar que a normalidade \textbf{não altera os
estimadores de MQO}: as expressões de \(\hat\beta_0\), \(\hat\beta_1\) e
\(s^2\) permanecem as mesmas. O ganho está em outro ponto: ela fornece
uma descrição probabilística completa da variabilidade desses
estimadores, algo que as hipóteses de Gauss--Markov não entregam por si
só.

Em particular, o resultado

\[
\frac{(n-2)s^2}{\sigma^2} \sim \chi^2_{n-2}
\]

é a peça-chave que permite obter, de forma exata, as distribuições \(t\)
e \(F\) usadas em intervalos de confiança e testes de hipóteses. Os
\(n-2\) graus de liberdade refletem a estimação de \((\beta_0,\beta_1)\)
e garantem que \(s^2\) seja não viesado para \(\sigma^2\).

\section{Predição pontual da média
condicional}\label{prediuxe7uxe3o-pontual-da-muxe9dia-condicional}

A predição pontual é o primeiro passo para inferir sobre a relação média
entre \(Y\) e \(X\) no MRLS. Antes de introduzir intervalos, é útil
explicitar que os valores ajustados são quantidades aleatórias (pois
dependem da amostra) e, sob normalidade, possuem distribuição conhecida.

\subsection{Distribuição dos valores
ajustados}\label{distribuiuxe7uxe3o-dos-valores-ajustados}

Para um valor genérico \(X_0\), definimos o valor ajustado (ou média
condicional estimada) como

\[
\hat{\mu}(X_0) \;=\; \hat\beta_0 + \hat\beta_1 X_0.
\]

Sob erros normais, \(\hat\beta_0\) e \(\hat\beta_1\) são combinações
lineares dos \(Y_i\) e, portanto, \(\hat{\mu}(X_0)\) também é uma
combinação linear de variáveis normais. Assim, \(\hat{\mu}(X_0)\) é
normalmente distribuído (ver Montgomery, Peck, e Vining (2021)).

Além disso, sua esperança é

\[
E[\hat{\mu}(X_0)] = \beta_0 + \beta_1 X_0 = \mu(X_0),
\]

isto é, \(\hat{\mu}(X_0)\) é \textbf{não viesado} para a média
condicional.

Sua variância é

\[
Var(\hat{\mu}(X_0)) 
= 
\sigma^2\left[
\frac{1}{n} + \frac{(X_0-\bar X)^2}{S_{xx}}
\right],
\qquad 
S_{xx}=\sum_{i=1}^n (X_i-\bar X)^2,
\]

de modo que

\[
\hat{\mu}(X_0)\sim 
N\!\left(\mu(X_0),\;
\sigma^2\left[\frac{1}{n} + \frac{(X_0-\bar X)^2}{S_{xx}}\right]\right).
\]

Essa expressão evidencia um aspecto estrutural da regressão: a incerteza
sobre a média ajustada é \textbf{menor perto de \(\bar X\)} e aumenta à
medida que \(X_0\) se afasta do centro dos dados, refletindo a geometria
do ajuste por mínimos quadrados (ver Kutner et al. (2005)).

\subsection{Predição pontual}\label{prediuxe7uxe3o-pontual}

A predição pontual da resposta média em \(X_0\) é, portanto,

\[
\hat{Y}_0 = \hat{\mu}(X_0)=\hat\beta_0+\hat\beta_1X_0,
\]

que corresponde à função de regressão estimada avaliada em \(X_0\). É
importante enfatizar que \(\hat{Y}_0\) se refere à \textbf{média
condicional} \(E[Y\mid X_0]\), e não ao valor de uma nova observação
individual.

\subsection{Limitação da predição
pontual}\label{limitauxe7uxe3o-da-prediuxe7uxe3o-pontual}

Embora \(\hat{Y}_0\) forneça uma estimativa central, ela não quantifica
a incerteza associada ao ajuste. Por isso, em aplicações, a predição
pontual deve ser acompanhada de:

\begin{itemize}
\tightlist
\item
  um \textbf{intervalo de confiança} para \(\mu(X_0)\), quando o
  interesse é a tendência média; ou
\item
  um \textbf{intervalo de predição}, quando o objetivo é prever uma nova
  observação individual.
\end{itemize}

Essas duas construções serão desenvolvidas nas próximas subseções (ver
Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).

Considere o seguinte gráfico, onde o ponto destacado corresponde a
\(\hat Y_0\), isto é, à estimativa da média condicional em \(X_0\).
Observe que o valor pontual não informa, por si só, o grau de incerteza
associado à estimativa, questão que será tratada na próxima subseção.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_inferencia_files/figure-pdf/unnamed-chunk-2-1.pdf}}

}

\caption{Predição pontual: reta ajustada e valor previsto para X0=5
horas.}

\end{figure}%

\section{Intervalos de confiança no
MRLS}\label{intervalos-de-confianuxe7a-no-mrls}

Na seção anterior vimos que, sob a hipótese de erros normais, os
estimadores de MQO possuem distribuições normais quando a variância
\(\sigma^2\) é conhecida. No entanto, na prática, \(\sigma^2\) é
desconhecida e deve ser substituída por seu estimador não viesado

\[
s^2 = \frac{SQRes}{n-2},
\qquad 
SQRes = \sum_{i=1}^n (Y_i-\hat Y_i)^2.
\]

Essa substituição tem consequência direta na forma das distribuições
amostrais: ao padronizarmos os estimadores utilizando \(s\) em vez de
\(\sigma\), as estatísticas resultantes deixam de seguir a normal padrão
e passam a seguir distribuições \(t\) de Student com \(n-2\) graus de
liberdade (ver Kutner et al. (2005); Montgomery, Peck, e Vining (2021)).

Essa mudança decorre do fato de que

\[
\frac{(n-2)s^2}{\sigma^2} \sim \chi^2_{n-2},
\]

e de que \(\hat\beta_0\) e \(\hat\beta_1\) são independentes de \(s^2\)
sob normalidade dos erros, resultado cuja demonstração pode ser
consultada no Apêndice de Demonstrações \{\#demo\}.

\subsection{\texorpdfstring{Intervalos para \(\beta_0\),
\(\beta_1\)}{Intervalos para \textbackslash beta\_0, \textbackslash beta\_1}}\label{intervalos-para-beta_0-beta_1}

Para a inclinação, a estatística

\[
T_{\beta_1} = 
\frac{\hat\beta_1 - \beta_1}
{s/\sqrt{S_{xx}}}, \qquad 
S_{xx}=\sum_{i=1}^n (X_i-\bar X)^2.
\]

segue distribuição t de Student com \(n-2\) graus de liberdade

\[
T_{\beta_1} \sim t_{n-2}.
\]

Analogamente, para o intercepto, temos a mesma distribuição t de Student
com \(n-2\) graus de liberdade

\[
T_{\beta_0} =
\frac{\hat\beta_0 - \beta_0}
{s\sqrt{\tfrac{1}{n}+\tfrac{\bar X^2}{S_{xx}}}}
\sim t_{n-2}.
\]

A demonstração formal dessas distribuições padronizadas pode ser vista
no Apêndice \{\#demo\}, onde se utiliza a independência entre
estimadores lineares normais e a soma de quadrados residual.

Com base nessas estatísticas, os intervalos de confiança de nível
\((1-\alpha)\times 100\%\) são dados por

\[
IC_{1-\alpha}(\beta_0) =
\hat\beta_0 
\pm 
t_{n-2;1-\alpha/2}\;
s\sqrt{\frac{1}{n}+\frac{\bar X^2}{S_{xx}}}
\]

e

\[
IC_{1-\alpha}(\beta_1) =
\hat\beta_1 
\pm 
t_{n-2;1-\alpha/2}\;
\frac{s}{\sqrt{S_{xx}}}.
\]

Aqui, \(t_{n-2;1-\alpha/2}\) denota o quantil superior da distribuição
\(t\) com \(n-2\) graus de liberdade.

\subsection{Intervalo de confiança para a média
condicional}\label{intervalo-de-confianuxe7a-para-a-muxe9dia-condicional}

Anteriormente vimos que a predição pontual da média condicional em
\(X_0\) é

\[
\hat{\mu}(X_0) = \hat{\beta}_0 + \hat{\beta}_1 X_0,
\]

estimativa natural de

\[
\mu(X_0) = E[Y \mid X_0].
\]

Sob a hipótese de erros normais, a combinação linear \(\hat{\mu}(X_0)\)
possui distribuição normal quando \(\sigma^2\) é conhecido. Como, na
prática, \(\sigma^2\) é substituído por seu estimador não viesado
\(s^2 = SQ_{Res}/(n-2)\), a estatística padronizada

\[
T =
\frac{\hat{\mu}(X_0) - \mu(X_0)}
{s \sqrt{\frac{1}{n} + \frac{(X_0-\bar X)^2}{S_{xx}}}}
\]

segue distribuição \(t\) de Student com \(n-2\) graus de liberdade (ver
Kutner et al. (2005); Montgomery, Peck, e Vining (2021)). A demonstração
formal pode ser consultada no Apêndice de Demonstrações \{\#demo\}.

Consequentemente, um intervalo de confiança de nível
\((1-\alpha)\times 100\%\) para a média condicional é

\[
IC_{1-\alpha}\left[\mu(X_0)\right] 
=
\hat{\mu}(X_0) 
\pm 
t_{n-2;\,1-\alpha/2}\; 
s \sqrt{\frac{1}{n} + \frac{(X_0-\bar X)^2}{S_{xx}}},
\]

em que:

\begin{itemize}
\tightlist
\item
  \(s^2 = SQ_{Res}/(n-2)\) é a variância residual estimada;
\item
  \(S_{xx} = \sum_{i=1}^n (X_i-\bar X)^2\);
\item
  o termo dentro da raiz representa a \textbf{variabilidade da
  estimativa da média condicional}.
\end{itemize}

O fator

\[
\frac{1}{n} + \frac{(X_0-\bar X)^2}{S_{xx}}
\]

possui interpretação estrutural clara. Ele combina:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \(\frac{1}{n}\): componente associado à incerteza na estimação do
  intercepto;
\item
  \(\frac{(X_0-\bar X)^2}{S_{xx}}\): componente associado à incerteza da
  inclinação e ao afastamento de \(X_0\) em relação ao centro da
  amostra.
\end{enumerate}

Essa decomposição revela que a precisão da estimativa depende da posição
de \(X_0\) no domínio observado. O intervalo é:

\begin{itemize}
\tightlist
\item
  \textbf{mais estreito} quando \(X_0=\bar X\);
\item
  \textbf{mais largo} à medida que \(X_0\) se afasta da média amostral.
\end{itemize}

Esse comportamento decorre diretamente da geometria da regressão linear
e da estrutura de projeção ortogonal subjacente aos mínimos quadrados.

É fundamental enfatizar que esse intervalo refere-se à \textbf{média
condicional}

\[
\mu(X_0) = E[Y \mid X_0],
\]

e não a uma nova observação individual. Ele quantifica a incerteza sobre
a \textbf{tendência média da resposta} para a condição \(X=X_0\).

\textbf{Dicas de uso}

\begin{itemize}
\tightlist
\item
  Utilize este intervalo quando o objetivo for inferir sobre a
  \textbf{tendência média} da resposta para um valor específico do
  regressor.
\item
  Não o confunda com o intervalo de predição para um novo indivíduo, que
  incorpora variabilidade adicional do erro aleatório.
\end{itemize}

A figura a seguir ilustra um intervalo de confiança de 95\% para a média
condicional ao longo do domínio observado.

Observe qye banda em torno da reta representa a incerteza sobre
\(\mu(X)\) ao longo dos valores observados de \(X\). Note que ela é mais
estreita nas proximidades de \(\bar X\) e se alarga progressivamente nos
extremos do domínio amostral, refletindo o aumento da variância de
\(\hat{\mu}(X_0)\).

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_inferencia_files/figure-pdf/unnamed-chunk-3-1.pdf}}

}

\caption{Intervalo de confiança (95\%) para a média condicional.}

\end{figure}%

\subsection{Interpretação frequentista e significado dos
intervalos}\label{interpretauxe7uxe3o-frequentista-e-significado-dos-intervalos}

A interpretação frequentista de um intervalo de confiança de nível
\(1-\alpha\) é a seguinte: se o experimento fosse repetido um número
grande de vezes e sob as mesmas condições, aproximadamente
\((1-\alpha)\times 100\%\) dos intervalos construídos conteriam o
verdadeiro parâmetro. O parâmetro é fixo; o que varia é o intervalo,
pois ele depende da amostra observada.

A introdução da distribuição \(t\) de Student desempenha papel essencial
nesse contexto. Ao substituir \(\sigma\) por seu estimador \(s\),
incorporamos a incerteza adicional decorrente da estimação da variância.
Essa correção é particularmente relevante em amostras pequenas: quanto
menor \(n\), mais pesada é a cauda da distribuição \(t_{n-2}\) e,
consequentemente, mais largos são os intervalos. À medida que \(n\)
cresce, a distribuição \(t_{n-2}\) converge para a normal padrão, e os
intervalos passam a se aproximar daqueles que seriam obtidos se
\(\sigma^2\) fosse conhecido.

Os intervalos são centrados em \(\hat\beta_0\) e \(\hat\beta_1\) porque
esses estimadores são não viesados. Em média, as retas ajustadas
coincidem com a reta verdadeira; os intervalos quantificam precisamente
a incerteza em torno dessa centralidade.

É fundamental distinguir os diferentes objetos inferenciais:

\begin{itemize}
\tightlist
\item
  O intervalo para \(\beta_0\) e \(\beta_1\) refere-se a
  \textbf{parâmetros estruturais do modelo}.
\item
  O intervalo para \(\mu(X_0)=E[Y\mid X_0]\) refere-se à \textbf{média
  condicional}.
\item
  Nenhum desses intervalos corresponde à previsão de uma nova observação
  individual.
\end{itemize}

A distinção entre inferência sobre a média condicional e previsão
individual é conceitualmente importante, pois a segunda incorpora não
apenas a incerteza na estimação dos parâmetros, mas também a
variabilidade intrínseca do processo aleatório. Essa diferença será
aprofundada na subseção seguinte.

\section{Intervalo de predição para nova
observação}\label{intervalo-de-prediuxe7uxe3o-para-nova-observauxe7uxe3o}

Até aqui construímos intervalos de confiança para a \textbf{média
condicional} \(\mu(X_0)=E[Y\mid X_0]\). No entanto, muitas aplicações
exigem algo diferente: prever o valor de uma \textbf{nova observação
individual} associada a \(X_0\).

Se uma nova unidade experimental for observada no mesmo valor \(X_0\),
seu modelo é

\[
Y_{\text{novo}}(X_0) 
=
\beta_0 + \beta_1 X_0 + \varepsilon_{\text{novo}},
\]

em que \(\varepsilon_{\text{novo}} \sim N(0,\sigma^2)\) e é independente
dos erros da amostra original.

A diferença conceitual é que agora não estamos estimando apenas a média
condicional, mas prevendo uma realização específica que contém, além da
incerteza na estimação dos parâmetros, a \textbf{variabilidade
intrínseca do erro aleatório}.

Portanto, a quantidade relevante é

\[
Y_{\text{novo}}(X_0) - \hat{\mu}(X_0),
\]

cuja variância é

\[
Var\!\left[Y_{\text{novo}}(X_0) - \hat{\mu}(X_0)\right]
=
\sigma^2
\left[
1 + \frac{1}{n} + \frac{(X_0-\bar X)^2}{S_{xx}}
\right].
\]

O termo adicional ``\(+1\)'' aparece porque a nova observação contém um
erro próprio, independente daquele utilizado na estimação dos
parâmetros. A demonstração formal dessa variância pode ser consultada no
Apêndice de Demonstrações \{\#demo\} (ver Kutner et al. (2005);
Montgomery, Peck, e Vining (2021)).

Padronizando essa quantidade por \(s\), obtemos uma estatística que
segue distribuição t de Student com \(n-2\) graus de liberdade
(\(t_{n-2}\)) sob normalidade dos erros. Assim, o intervalo de predição
de nível \((1-\alpha)\times 100\%\) é

\[
IC\!\left[Y_{\text{novo}}(X_0)\right]
=
\hat{\mu}(X_0)
\pm
t_{n-2;\,1-\alpha/2}\;
s
\sqrt{
1 + \frac{1}{n} + \frac{(X_0-\bar X)^2}{S_{xx}}
}.
\]

Comparando com o intervalo para a média condicional, temos

\[
\hat{\mu}(X_0)
\pm
t_{n-2;\,1-\alpha/2}\;
s
\sqrt{
\frac{1}{n} + \frac{(X_0-\bar X)^2}{S_{xx}}
}.
\]

Daí, vemos claramente a presença do termo adicional \(1\) dentro da
raiz. Esse termo representa a \textbf{variabilidade individual
irreducível} do processo aleatório.

Consequentemente:

\begin{itemize}
\tightlist
\item
  O intervalo de predição é \textbf{sempre mais largo} que o intervalo
  de confiança da média.
\item
  A diferença entre eles é tanto maior quanto maior for \(\sigma^2\).
\item
  Ambos se alargam quando \(X_0\) se afasta de \(\bar X\), refletindo a
  incerteza adicional associada à extrapolação.
\end{itemize}

O intervalo de predição fornece um conjunto de valores plausíveis para
uma \textbf{nova observação individual}, e não para a média
populacional. Em termos frequentistas, se o processo fosse repetido nas
mesmas condições, aproximadamente \((1-\alpha)\times 100\%\) desses
intervalos conteriam a nova observação gerada pelo modelo.

\textbf{Dica prática}

\begin{itemize}
\tightlist
\item
  Use \textbf{intervalo de confiança} quando o objetivo for inferir
  sobre a \textbf{tendência média}.
\item
  Use \textbf{intervalo de predição} quando o interesse for antecipar o
  valor de um \textbf{novo indivíduo}.
\end{itemize}

A figura a seguir ilustra simultaneamente o intervalo de confiança para
a média e o intervalo de predição para nova observação. Comparando as
bandas, perceba que o intervalo de predição é sempre mais largo que o de
confiança. Isso ocorre porque ele incorpora a variabilidade individual
das novas observações, além da incerteza da média.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_inferencia_files/figure-pdf/unnamed-chunk-4-1.pdf}}

}

\caption{Intervalo de confiança vs.~intervalo de predição (95\%).}

\end{figure}%

\textbf{Advertência sobre extrapolação}

As expressões obtidas para o intervalo de predição são válidas para
qualquer valor numérico \(X_0\). No entanto, é fundamental distinguir
entre \textbf{interpolação} (quando \(X_0\) pertence ao intervalo
observado da amostra) e \textbf{extrapolação} (quando \(X_0\) está fora
do domínio observado de \(X\)).

Seja o intervalo amostral observado \[
X_{(min)} \leq X_i \leq X_{(max)}.
\]

Quando \(X_0 \in [X_{(min)}, X_{(max)}]\), o modelo está sendo utilizado
em uma região sustentada pelos dados. Nesse caso, embora o intervalo
possa se alargar à medida que \(X_0\) se afasta de \(\bar X\), a
inferência permanece ancorada na informação empírica disponível.

Por outro lado, quando \(X_0\) está fora desse intervalo, ocorre
\textbf{extrapolação}. Nessa situação, a validade formal da expressão
algébrica do intervalo permanece, mas sua confiabilidade prática pode
ser comprometida, pois o modelo passa a depender fortemente da suposição
de linearidade fora da região observada. Pequenas violações da forma
funcional podem produzir erros substanciais de predição.

Como destacam Kutner et al. (2005) e Montgomery, Peck, e Vining (2021),
a regressão linear deve ser utilizada com cautela fora do domínio
amostral, pois o comportamento da relação entre \(X\) e \(Y\) além dos
dados observados não é garantido pelo modelo ajustado. Assim, intervalos
de predição em extrapolação tendem a ser não apenas mais largos, mas
também potencialmente menos representativos do processo real.

\chapter{Testes de hipóteses e
ANOVA}\label{testes-de-hipuxf3teses-e-anova}

A construção de intervalos de confiança fornece uma forma de expressar a
incerteza nos estimadores. Outra abordagem complementar é a dos
\textbf{testes de hipóteses} e \textbf{análise de variância (ANOVA)},
que avaliam formalmente se os coeficientes do modelo diferem
significativamente de determinados valores, em especial do zero. No
Modelo de Regressão Linear Simples (MRLS) com erros normais, os testes
se apoiam nas distribuições exatas dos estimadores discutidas
anteriormente.

Sob as hipóteses clássicas do MRLS normal: (a) linearidade na forma
funcional, (b) independência dos erros, (b) homocedasticidade e (d)
normalidade, os estimadores de mínimos quadrados coincidem com os
estimadores de máxima verossimilhança, e possuem distribuições amostrais
exatas baseadas na distribuição normal e na distribuição \(t\) de
Student (Hoffmann (2016); Montgomery, Peck, e Vining (2021)). Em
particular, condicionalmente aos valores observados de \(X\), temos

\[
\hat\beta_1 \sim N\!\left(\beta_1,\; \frac{\sigma^2}{S_{xx}}\right),
\]

e, como \(\sigma^2\) é desconhecida e estimada por
\(s^2 = SQ_{Res}/(n-2)\), a padronização conduz à distribuição
\(t_{n-2}\), resultado central para a inferência clássica em regressão
linear simples (Kutner et al. (2005); Casella e Berger (2002)).

É importante enfatizar que os testes de hipóteses são construídos
\emph{dentro do modelo}. A validade exata da distribuição \(t\) depende
da normalidade dos erros; na ausência dessa hipótese, os resultados
passam a ter caráter assintótico. Assim, significância estatística deve
sempre ser interpretada à luz das suposições estruturais do modelo.

\section{\texorpdfstring{Testes marginais para \(\beta_1\) e
\(\beta_0\)}{Testes marginais para \textbackslash beta\_1 e \textbackslash beta\_0}}\label{testes-marginais-para-beta_1-e-beta_0}

\subsection{\texorpdfstring{Teste para a
\(\beta_1\)}{Teste para a \textbackslash beta\_1}}\label{teste-para-a-beta_1}

No caso da inclinação, o teste é:

\[
H_0: \beta_1 = \beta_{1,0} 
\quad \text{vs} \quad 
H_1: \beta_1 \neq \beta_{1,0},
\]

com estatística de teste dada por

\[
T = \frac{\hat\beta_1 - \beta_{1,0}}{s / \sqrt{S_{xx}}} \;\sim\; t_{n-2}.
\]

Esse teste é especialmente relevante quando \(\beta_{1,0}=0\), situação
em que verificamos se existe associação linear entre \(X\) e \(Y\). Em
termos práticos, ele responde à pergunta: \emph{vale a pena incluir}
\(X\) para explicar \(Y\)? Se \(|T|\) ultrapassa o valor crítico da
distribuição \(t_{n-2}\), rejeitamos \(H_0\) e concluímos que a
inclinação é estatisticamente diferente de zero.

Do ponto de vista conceitual, testar \(\beta_1=0\) equivale a testar se
a melhor reta ajustada possui inclinação nula, isto é, se o modelo
reduz-se a \(Y_i=\beta_0+\varepsilon_i\). Portanto, o teste compara dois
modelos aninhados: o modelo completo (com inclinação livre) e o modelo
restrito (com \(\beta_1=0\)). Essa interpretação como comparação entre
modelos é fundamental para compreender a ligação posterior com a
estatística \(F\) (Draper e Smith (1998); Kutner et al. (2005)).

Além disso, há equivalência formal entre o teste \(t\) bilateral ao
nível \(\alpha\) e o intervalo de confiança \((1-\alpha)\) para
\(\beta_1\): rejeitar \(H_0\) é equivalente a verificar que
\(\beta_{1,0}\) não pertence ao intervalo de confiança correspondente
Casella e Berger (2002). O \(p\)-valor, por sua vez, é definido como

\[
p = P\left(|T| \ge |t_{obs}| \mid H_0 \right),
\]

e quantifica evidência contra \(H_0\) dentro da estrutura probabilística
assumida.

\subsection{\texorpdfstring{Teste para a
\(\beta_0\)}{Teste para a \textbackslash beta\_0}}\label{teste-para-a-beta_0}

De modo análogo, para o intercepto temos:

\[
H_0: \beta_0 = \beta_{0,0} 
\quad \text{vs} \quad 
H_1: \beta_0 \neq \beta_{0,0},
\]

com estatística de teste

\[
T = \frac{\hat\beta_0 - \beta_{0,0}}{s \sqrt{\tfrac{1}{n}+\tfrac{\bar X^2}{S_{xx}}}} \;\sim\; t_{n-2}.
\]

Esse teste é menos central do ponto de vista prático, mas pode ser
importante quando se deseja avaliar se o valor médio de \(Y\) para
\(X=0\) coincide com alguma referência teórica ou prática.

Conceitualmente, \(\beta_0 = E(Y\mid X=0)\) dentro do modelo linear.
Assim, sua interpretação depende criticamente de \(X=0\) ter significado
no fenômeno estudado e estar dentro do intervalo de observação dos
dados. Caso contrário, o intercepto pode representar apenas uma
extrapolação matemática da reta ajustada, ainda que perfeitamente bem
definido do ponto de vista estatístico (Montgomery, Peck, e Vining
(2021); Weisberg (2005)).

\begin{quote}
\textbf{Apêndice de Demonstrações \{\#demo\}:} as distribuições exatas
das estatísticas \(T\) decorrem da normalidade dos erros, da
independência entre \(\hat\beta_j\) e \(SQ_{Res}\) e da relação entre
variância residual e distribuição qui-quadrado, conforme desenvolvimento
clássico da inferência em modelos lineares (Casella e Berger (2002);
Kutner et al. (2005)).
\end{quote}

\section{Análise de Variância no
MRLS}\label{anuxe1lise-de-variuxe2ncia-no-mrls}

\subsection{Decomposição da soma de
quadrados}\label{decomposiuxe7uxe3o-da-soma-de-quadrados}

O teste \(F\) pode ser entendido a partir da decomposição da
variabilidade total em \(Y\):

\[
SQ_{Total} = SQ_{Reg} + SQ_{Res}.
\]

Aqui, \(SQ_{Total} = \sum_{i=1}^n (Y_i - \bar Y)^2\) mede a
variabilidade total das observações em torno da média. Essa
variabilidade pode ser separada em duas partes:

\begin{itemize}
\tightlist
\item
  \textbf{variabilidade explicada pela regressão}
\end{itemize}

\[
SQ_{Reg} = \sum_{i=1}^n (\hat Y_i - \bar Y)^2
\]

\begin{itemize}
\tightlist
\item
  \textbf{Variabilidade não explicada}
\end{itemize}

\[
SQ_{Res} = \sum_{i=1}^n (Y_i - \hat Y_i)^2,
\]

associada aos resíduos. Em termos geométricos, a \(SQ_{Reg}\)
corresponde à projeção de \(Y\) no espaço gerado por \(X\), enquanto
\(SQ_{Res}\) corresponde ao componente ortogonal (erro).

Formalmente, essa decomposição decorre da ortogonalidade entre resíduos
e valores ajustados no método dos mínimos quadrados. No MRLS, tem-se

\[
\sum_{i=1}^n \hat\varepsilon_i (\hat Y_i - \bar Y) = 0,
\]

o que implica que a variabilidade total pode ser particionada sem termo
de cruzamento. Em notação matricial, essa propriedade está associada ao
fato de que o vetor de resíduos é ortogonal ao espaço coluna da matriz
de projeto \(\mathbf{X}\), isto é,
\(\mathbf{X}^\top \hat{\boldsymbol{\varepsilon}} = \mathbf{0}\)
(Harville (2000); Searle (2016)).

\begin{quote}
\textbf{Apêndice de Demonstrações \{\#demo\}:} a identidade
\(SQ_{Total}=SQ_{Reg}+SQ_{Res}\) é obtida expandindo
\(\sum (Y_i-\bar Y)^2\) em termos de \((\hat Y_i-\bar Y)\) e
\((Y_i-\hat Y_i)\) e utilizando a ortogonalidade dos resíduos.
\end{quote}

Essa decomposição mostra que o ajuste por regressão não apenas fornece
estimativas pontuais, mas também permite quantificar de forma clara
quanto da variabilidade total de \(Y\) é capturada pela relação linear
com \(X\). Quanto maior \(SQ_{Reg}\) em relação a \(SQ_{Total}\), maior
o poder explicativo do modelo.

Do ponto de vista probabilístico, sob \(H_0:\beta_1=0\) e normalidade
dos erros, as somas de quadrados associadas à regressão e aos resíduos,
quando devidamente padronizadas por \(\sigma^2\), seguem distribuições
qui-quadrado independentes com 1 e \(n-2\) graus de liberdade,
respectivamente (Kutner et al. (2005)). Essa independência é a base
formal da estatística \(F\).

\subsection{Quadrados Médios e Estatística
F}\label{quadrados-muxe9dios-e-estatuxedstica-f}

Para formalizar o teste global, cada soma de quadrados é dividida pelos
graus de liberdade correspondentes:

\begin{itemize}
\item
  Quadrado médio da regressão: \[
  QM_{Reg} = \frac{SQ_{Reg}}{1}.
  \]
\item
  Quadrado médio dos resíduos: \[
  QM_{Res} = \frac{SQ_{Res}}{n-2}.
  \]
\end{itemize}

A razão entre eles define a estatística \(F\):

\[
F = \frac{QM_{Reg}}{QM_{Res}} \;\sim\; F_{1,n-2} \quad \text{sob } H_0: \beta_1=0.
\]

Esse teste avalia, portanto, se a proporção de variabilidade explicada
pela regressão é grande o suficiente em comparação com a variabilidade
residual, justificando o uso do modelo.

Interpretativamente, \(QM_{Res}\) é um estimador não viesado de
\(\sigma^2\), enquanto \(QM_{Reg}\) mede a variação explicada \emph{por
grau de liberdade associado ao efeito linear de \(X\)}. Assim, o teste
\(F\) compara um componente sistemático (sinal) com um componente
aleatório (ruído).

Valores elevados de \(F\) indicam que a redução em \(SQ_{Res}\) ao
incluir \(X\) é grande demais para ser atribuída apenas ao acaso
(Montgomery, Peck, e Vining (2021); Weisberg (2005)).

\subsection{Tabela ANOVA do MRLS}\label{tabela-anova-do-mrls}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2533}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1867}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0933}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1600}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3067}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Fonte de variação
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SQ
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
GL
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
QM
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estatística
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regressão & \(SQ_{Reg}\) & 1 & \(QM_{Reg}\) & \(F=QM_{Reg}/QM_{Res}\) \\
Resíduo & \(SQ_{Res}\) & \(n-2\) & \(QM_{Res}\) & \\
Total & \(SQ_{Total}\) & \(n-1\) & & \\
\end{longtable}

A tabela resume de maneira padronizada a decomposição da variabilidade e
fornece a base para a aplicação do teste \(F\). Note que os graus de
liberdade totais satisfazem

\[
(n-1) = 1 + (n-2),
\]

refletindo que dois parâmetros foram estimados no modelo (intercepto e
inclinação).

\subsection{\texorpdfstring{Coeficiente de determinação
\(R^2\)}{Coeficiente de determinação R\^{}2}}\label{coeficiente-de-determinauxe7uxe3o-r2}

Um desdobramento natural dessa análise é o \textbf{coeficiente de
determinação}:

\[
R^2 = \frac{SQ_{Reg}}{SQ_{Total}} = 1 - \frac{SQ_{Res}}{SQ_{Total}}.
\]

Ele mede a proporção da variabilidade total de \(Y\) explicada pela
regressão e está limitado ao intervalo \(0 \leq R^2 \leq 1\). Em termos
práticos, \(R^2 \times 100\%\) indica o percentual da variabilidade de
\(Y\) que é explicado linearmente por \(X\). Quanto maior \(R^2\), maior
o poder explicativo do modelo.

É importante compreender que \(R^2\) é uma medida descritiva da
qualidade de ajuste dentro da amostra observada. Ele \textbf{não implica
causalidade, nem garante desempenho preditivo}. Além disso, em modelos
com múltiplos preditores, \(R^2\) tende a aumentar com a inclusão de
variáveis, mesmo que irrelevantes, motivo pelo qual se introduz
posteriormente o \(R^2\) ajustado (Kutner et al. (2005); Montgomery,
Peck, e Vining (2021)).

No caso do MRLS, há uma relação direta com a estatística descritiva da
correlação linear:

\[
R^2 = r_{XY}^2, 
\quad r_{XY} = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}},
\]

em que \(r_{XY}\) é o coeficiente de correlação amostral entre \(X\) e
\(Y\). Essa igualdade decorre das expressões algébricas do estimador
\(\hat\beta_1\) e das somas de quadrados no caso univariado (Charnet et
al. (2008); Hoffmann (2016)).

Assim, \(R^2\) conecta três dimensões: é a proporção de variabilidade
explicada (ANOVA), é equivalente ao quadrado da correlação linear
(estatística descritiva) e fundamenta a estatística \(F\) da ANOVA por
meio da relação

\[
F = \frac{QM_{Reg}}{QM_{Res}} = \frac{R^2}{1-R^2}(n-2).
\]

\begin{quote}
\textbf{Apêndice de Demonstrações \{\#demo\}:} a relação entre \(F\) e
\(R^2\) é obtida substituindo as definições de \(SQ_{Reg}\) e
\(SQ_{Res}\) na razão \(QM_{Reg}/QM_{Res}\) e utilizando a identidade
\(R^2 = SQ_{Reg}/SQ_{Total}\).
\end{quote}

Em resumo, os testes de hipóteses no MRLS permitem verificar tanto a
significância individual dos coeficientes quanto o poder explicativo
global do modelo. A equivalência entre os testes \(t\) e \(F\), a
decomposição da soma de quadrados e a interpretação do \(R^2\) reforçam
a visão integrada da regressão como técnica que conecta
\textbf{estimação, inferência e análise da variabilidade} em um único
arcabouço teórico (Kutner et al. (2005); Montgomery, Peck, e Vining
(2021)).

\subsection{\texorpdfstring{Equivalência entre \(T^2\) e
\(F\)}{Equivalência entre T\^{}2 e F}}\label{equivaluxeancia-entre-t2-e-f}

Um resultado fundamental é que, no MRLS normal, o teste \(t\) para a
inclinação e o teste \(F\) global da regressão são equivalentes. Quando
a hipótese nula é \(\beta_1=0\), temos:

\[
F = T^2.
\]

Isso ocorre porque o modelo possui apenas um preditor. Em modelos
múltiplos, a situação muda: o teste \(t\) continua avaliando parâmetros
individuais, enquanto o teste \(F\) passa a ter papel central ao
considerar hipóteses conjuntas sobre vários coeficientes.

Para compreender essa equivalência com rigor, observe que, sob
\(H_0:\beta_1=0\), a estatística \(t\) pode ser escrita como

\[
T = \frac{\hat\beta_1}{s/\sqrt{S_{xx}}},
\]

de modo que

\[
T^2 = \frac{\hat\beta_1^2 S_{xx}}{s^2}.
\]

Por outro lado, no MRLS, pode-se mostrar que

\[
SQ_{Reg} = \hat\beta_1^2 S_{xx},
\]

e que

\[
QM_{Res} = s^2.
\]

Logo,

\[
F = \frac{QM_{Reg}}{QM_{Res}}
  = \frac{SQ_{Reg}/1}{s^2}
  = \frac{\hat\beta_1^2 S_{xx}}{s^2}
  = T^2.
\]

Portanto, a estatística \(F\) nada mais é do que o quadrado da
estatística \(t\) quando há apenas um parâmetro de inclinação sendo
testado.

Do ponto de vista distribucional, se

\[
T \sim t_{n-2},
\]

então

\[
T^2 \sim F_{1,n-2},
\]

o que decorre da relação geral entre as distribuições \(t\) e \(F\)
(Casella e Berger (2002)). Assim, a equivalência também se verifica ao
nível das distribuições.

Essa identidade tem uma consequência didática importante: no MRLS, o
teste global da regressão e o teste individual da inclinação são
exatamente o mesmo teste, apenas expressos em escalas diferentes. Em
outras palavras, testar a significância global do modelo é o mesmo que
testar se a inclinação é nula.

\begin{quote}
\textbf{Apêndice de Demonstrações \{\#demo\}:} a identidade
\(SQ_{Reg}=\hat\beta_1^2 S_{xx}\) e a relação distribucional entre \(t\)
e \(F\) podem ser demonstradas a partir das propriedades do MQO e da
definição da distribuição \(F\) como razão de qui-quadrados
independentes Casella e Berger (2002); Harville (2000).
\end{quote}

\chapter{Diagnóstico e Avaliação no
MRLS}\label{diagnuxf3stico-e-avaliauxe7uxe3o-no-mrls}

\section{Por que analisar resíduos?}\label{por-que-analisar-resuxedduos}

Após o ajuste de um modelo de regressão, é essencial verificar se as
\textbf{hipóteses do MRLS} do modelos para os erros aleatórios foram
atendidas. Essa verificação se dá por diversos meios, sendo algumas dela
via a análise dos \textbf{resíduos}.

Os resíduos mais intuitivos são definidos como:

\[
e_i = Y_i - \hat{Y}_i, \quad i=1,2,\dots,n.
\]

Estes resíduos representam a parte de \(Y\) que \textbf{não foi
explicada pelo modelo}. Enquanto os erros verdadeiros \(\varepsilon_i\)
são inobserváveis, os resíduos são acessíveis e servem como suas
aproximações.

Um ponto conceitual importante é distinguir ``hipóteses sobre os erros''
de ``propriedades dos resíduos''. As hipóteses clássicas do MRLS são
formuladas para os \textbf{erros aleatórios} \(\varepsilon_i\)
(componentes não observáveis do mecanismo gerador de dados). Já os
resíduos \(e_i\) são funções dos dados e dos estimadores, logo carregam
restrições algébricas impostas pelo MQO. Assim, mesmo que o MRLS seja
verdadeiro (isto é, as hipóteses sobre \(\varepsilon_i\) sejam
satisfeitas), os resíduos \textbf{não} se comportam como uma amostra
i.i.d. de uma mesma distribuição; em particular, eles são
correlacionados e apresentam variâncias diferentes ao longo de \(i\) a
dependendo da alavancagem (Searle (2016); Harville (2000)).

As principais hipóteses do modelo para os erros (\(\varepsilon\)) do
MRLS são:

\begin{itemize}
\item
  Média zero \((E[\varepsilon_i] = 0)\)
\item
  Variância constante \((Var[\varepsilon_i] = \sigma^2)\)
\item
  Não correlação entre os erros
  \((cov[\varepsilon_i,\varepsilon_j] = 0, \forall i \neq j)\)
\end{itemize}

Podem ser feitas hipóteses adicionais sobre a forma da distribuição dos
erros, como assumir certa assimetria, curtose específica ou até uma
distribuição conhecida.

A suposição (hipótese) distribuição mais considerada para a distribuição
dos erros é:

\begin{itemize}
\tightlist
\item
  Normalidade \((\varepsilon_i \sim N(0,\sigma^2))\).
\end{itemize}

Um modelo só pode ser considerado adequado se os resíduos se comportarem
como erros aleatórios: sem tendência sistemática, com variância
aproximadamente constante, não correlacionaos e, em muitos contextos,
aproximadamente normais. Em prática aplicada, é útil interpretar isso
como: \textbf{(i)} a média condicional foi bem especificada (linearidade
na forma funcional), \textbf{(ii)} a variância condicional não muda de
forma sistemática (homocedasticidade) e \textbf{(iii)} não há estrutura
temporal/espacial remanescente (independência), além de \textbf{(iv)}
normalidade como hipótese adicional que viabiliza inferência exata e
diagnósticos probabilísticos baseados em caudas (Montgomery, Peck, e
Vining (2021); Kutner et al. (2005)).

\section{Tipos de resíduos e
propriedades}\label{tipos-de-resuxedduos-e-propriedades}

\subsection{Resíduos ordinários}\label{resuxedduos-ordinuxe1rios}

O ponto de partida são os \textbf{resíduos ordinários}:

\[
e_i = Y_i - \hat{Y}_i.
\]

Eles indicam o desvio direto entre a observação e a reta ajustada. Por
exemplo, \(e_i > 0\) mostra que o modelo \textbf{subestimou} \(Y_i\),
enquanto \(e_i < 0\) mostra que o modelo \textbf{superestimou}.

Do ponto de vista conceitual, o resíduo é uma \emph{estimativa
observável} do erro aleatório \(\varepsilon_i\). Como \(\varepsilon_i\)
não é observável, toda a etapa de diagnóstico repousa sobre a análise do
comportamento dos \(e_i\). Entretanto, é fundamental compreender que
resíduos \textbf{não são} os erros verdadeiros: eles dependem dos
parâmetros estimados e, portanto, carregam estrutura imposta pelo método
de mínimos quadrados Hoffmann (2016); Montgomery, Peck, e Vining (2021).

\subsubsection{Propriedades básicas dos resíduos
ordinários}\label{propriedades-buxe1sicas-dos-resuxedduos-ordinuxe1rios}

O método dos mínimos quadrados impõe três propriedades estruturais:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Soma nula} \[
  \sum_{i=1}^n e_i = 0.
  \] A reta ajustada sempre passa pelo ponto médio amostral
  \((\bar X, \bar Y)\).
\item
  \textbf{Ortogonalidade com o preditor} \[
  \sum_{i=1}^n e_i X_i = 0.
  \] Não há associação linear entre os resíduos e a variável
  explicativa. Caso existisse, o modelo poderia ser melhorado ajustando
  novamente a inclinação.
\item
  \textbf{Soma de quadrados dos resíduos} \[
  \sum_{i=1}^n e_i^2 = SQ_{Res},
  \] isto é, os resíduos concentram exatamente a variabilidade não
  explicada pelo modelo.
\end{enumerate}

Essas propriedades decorrem diretamente das \textbf{equações normais do
método dos mínimos quadrados} no caso univariado, obtidas pela
minimização de \(\sum (Y_i - \beta_0 - \beta_1 X_i)^2\) em relação a
\(\beta_0\) e \(\beta_1\) (Charnet et al. (2008); Kutner et al. (2005)).

\begin{quote}
\textbf{Apêndice de Demonstrações \{\#demo\}:} as propriedades acima são
obtidas substituindo \(\hat\beta_0\) e \(\hat\beta_1\) nas expressões
dos resíduos e manipulando os somatórios resultantes das equações
normais.
\end{quote}

Essas três propriedades têm implicações importantes: mesmo que os erros
verdadeiros sejam independentes e homocedásticos, os resíduos não são
independentes entre si e tampouco possuem variância constante.

\subsubsection{Esperança, variância, covariância e distribuição dos
resíduos
ordinários}\label{esperanuxe7a-variuxe2ncia-covariuxe2ncia-e-distribuiuxe7uxe3o-dos-resuxedduos-ordinuxe1rios}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Esperança}
\end{enumerate}

\[
E[e_i] = 0.
\]

Sob as hipóteses do MRLS, cada resíduo tem média zero. Isso significa
que, em termos probabilísticos, o modelo não superestima nem subestima
sistematicamente a resposta.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Variância}
\end{enumerate}

\[
Var(e_i) = \sigma^2 (1 - h_{ii}),
\]

em que

\[
h_{ii} = \frac{1}{n} + \frac{(x_i - \bar X)^2}{S_{xx}}.
\]

A quantidade \(h_{ii}\) é chamada de \textbf{alavancagem} da observação
\(i\). Ela mede o quanto o valor de \(X_i\) influencia o próprio ajuste
\(\hat Y_i\).

Observações com valores de \(X_i\) muito afastados da média \(\bar X\)
apresentam maior alavancagem. Como consequência, possuem menor variância
residual, pois ``ancoram'' a reta ajustada com maior intensidade
(Belsley, Kuh, e Welsch (1980); Montgomery, Peck, e Vining (2021)).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Covariância}
\end{enumerate}

\[
Cov(e_i, e_j) = -\sigma^2 h_{ij}, \quad i \neq j,
\]

em que

\[
h_{ij} = \frac{1}{n} + \frac{(x_i - \bar X)(x_j - \bar X)}{S_{xx}}.
\]

Portanto, os resíduos são \textbf{correlacionados entre si}. Isso é
consequência direta do fato de que todos os resíduos dependem dos mesmos
estimadores \(\hat\beta_0\) e \(\hat\beta_1\) e, consequentemente, os
resíduos não podem ser tratados como uma nova amostra independente de
erros aleatórios (Kutner et al. (2005); Weisberg (2005)).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Distribuição}
\end{enumerate}

Se assumimos normalidade para os erros aleatórios,

\[
\varepsilon_i \sim N(0,\sigma^2),
\]

então os resíduos ordinários também seguem distribuição normal, pois são
combinações lineares das variáveis \(\varepsilon_i\):

\[
e_i \sim N\!\big(0,\sigma^2(1-h_{ii})\big).
\]

Essa normalidade é exata sob a hipótese de erros normais. Caso a
normalidade não seja assumida, a distribuição dos resíduos pode ser
aproximada por resultados assintóticos.

\begin{quote}
\textbf{Apêndice de Demonstrações \{\#demo\}:} as expressões de
variância e covariância dos resíduos são obtidas substituindo
\(e_i = Y_i - \hat Y_i\) e utilizando as propriedades das variâncias de
combinações lineares, juntamente com as expressões explícitas de
\(\hat\beta_0\) e \(\hat\beta_1\) Kutner et al. (2005); Montgomery,
Peck, e Vining (2021).
\end{quote}

\subsubsection{Implicações para
diagnóstico}\label{implicauxe7uxf5es-para-diagnuxf3stico}

Esses resultados mostram que, mesmo quando as hipóteses usuais de média
zero, variância constante, não correlação e normalidade para os erros
aleatórios são satisfeitas, os resíduos ordinários apresentam:

\begin{itemize}
\tightlist
\item
  variância não constante (dependente de \(h_{ii}\)),
\item
  correlação entre si,
\item
  dependência dos parâmetros estimados.
\end{itemize}

Portanto, embora úteis para visualização inicial e interpretação direta
do ajuste, os resíduos ordinários não são ideais para comparações
diretas entre observações com diferentes níveis de alavancagem.

Essa limitação motiva a construção de resíduos transformados, como os
\textbf{resíduos padronizados} e os \textbf{resíduos estudentizados},
que ajustam explicitamente a variabilidade individual e permitem
diagnósticos mais adequados de pontos discrepantes e violações das
hipóteses do modelo (Belsley, Kuh, e Welsch (1980); Weisberg (2005)).

\subsection{Resíduos padronizados}\label{resuxedduos-padronizados}

Com o objetivo de tornar os resíduos \textbf{comparáveis entre si},
ajustando a diferença de variâncias individuais, definem-se os
\textbf{resíduos padronizados} como

\[
r_i = \frac{e_i}{s \sqrt{1 - h_{ii}}},
\quad \text{com} \quad
s^2 = \frac{SQ_{Res}}{n-2}.
\]

Aqui, \(e_i\) é o resíduo ordinário, \(h_{ii}\) é a alavancagem da
observação \(i\) e \(s^2\) é o estimador não viesado de \(\sigma^2\). A
ideia central é simples: como

\[
Var(e_i) = \sigma^2 (1 - h_{ii}),
\]

dividir \(e_i\) por uma estimativa de seu desvio-padrão elimina a
heterogeneidade de variâncias e produz uma quantidade adimensional.

Do ponto de vista conceitual, essa padronização desempenha papel análogo
ao de uma estatística \(z\): ela mede o ``tamanho'' do desvio em
unidades de desvio-padrão estimado.

\subsubsection{Propriedades
fundamentais}\label{propriedades-fundamentais}

Sob as hipóteses do MRLS:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Esperança aproximada} \[
  E[r_i] \approx 0.
  \]
\item
  \textbf{Variância aproximada} \[
  Var(r_i) \approx 1.
  \]
\end{enumerate}

A aproximação decorre do fato de que \(s^2\) é uma estimativa de
\(\sigma^2\). Se \(\sigma^2\) fosse conhecido, teríamos exatamente

\[
\frac{e_i}{\sigma \sqrt{1-h_{ii}}} \sim N(0,1),
\]

sob normalidade dos erros.

Entretanto, como \(\sigma^2\) é substituído por \(s^2\), a estatística
passa a envolver uma razão entre variáveis aleatórias dependentes.

\subsubsection{Distribuição dos resíduos
padronizados}\label{distribuiuxe7uxe3o-dos-resuxedduos-padronizados}

Se os erros seguem

\[
\varepsilon_i \sim N(0,\sigma^2),
\]

então, para amostras moderadas ou grandes, vale a aproximação:

\[
r_i \approx t_{n-2}.
\]

A aproximação não é exata porque \(e_i\) e \(s^2\) não são
independentes: ambos dependem das mesmas observações e dos mesmos
estimadores \(\hat\beta_0\) e \(\hat\beta_1\) (Weisberg (2005)).

Em amostras grandes, pela consistência de \(s^2\) para \(\sigma^2\), a
distribuição de \(r_i\) aproxima-se da normal padrão:

\[
r_i \overset{aprox}{\sim} N(0,1).
\]

\begin{quote}
\textbf{Apêndice de Demonstrações \{\#demo\}:} a aproximação
\(r_i \approx t_{n-2}\) decorre da substituição de \(\sigma^2\) por
\(s^2\) na padronização e do fato de que
\((n-2)s^2/\sigma^2 \sim \chi^2_{n-2}\) sob normalidade dos erros.
\end{quote}

\subsubsection{Interpretação
prática}\label{interpretauxe7uxe3o-pruxe1tica}

Os resíduos padronizados permitem comparar observações com diferentes
alavancagens. Um mesmo valor absoluto de resíduo ordinário pode ser
pequeno ou grande dependendo de \(h_{ii}\). A padronização corrige esse
efeito.

Uma regra prática frequentemente utilizada é:

\begin{itemize}
\tightlist
\item
  \(|r_i| > 2\) → possível observação discrepante.
\item
  \(|r_i| > 3\) → forte indício de discrepância.
\end{itemize}

Esses limiares baseiam-se na probabilidade de observar valores extremos
sob uma distribuição aproximadamente normal ou \(t\). Por exemplo, sob
normalidade, a probabilidade de \(|Z|>2\) é aproximadamente 5\%.

Contudo, essa interpretação deve ser feita com cautela:

\begin{itemize}
\tightlist
\item
  Em amostras grandes, é esperado que alguns valores ultrapassem 2
  apenas por variabilidade natural.
\item
  Em amostras pequenas, a aproximação pode ser imprecisa.
\item
  A presença de múltiplos testes simultâneos pode inflar a taxa de
  falsos positivos.
\end{itemize}

\subsubsection{Limitações
conceituais}\label{limitauxe7uxf5es-conceituais}

Apesar de mais informativos que os resíduos ordinários, os resíduos
padronizados ainda apresentam uma limitação importante: o denominador
\(s\) é calculado utilizando \textbf{todas as observações}, inclusive a
própria observação \(i\).

Assim, um ponto extremo pode inflar \(s\), reduzindo artificialmente seu
próprio resíduo padronizado, fenômeno conhecido como \emph{masking}
(mascaramento) (Belsley, Kuh, e Welsch (1980)).

Essa limitação motiva a definição dos \textbf{resíduos estudentizados
externos}, nos quais a variância é estimada excluindo-se a própria
observação sob análise.

Em síntese:

\begin{itemize}
\tightlist
\item
  \textbf{Resíduos ordinários} medem o erro bruto.
\item
  \textbf{Resíduos padronizados} tornam os erros comparáveis.
\item
  A padronização é essencial para diagnóstico formal de outliers e para
  construção de gráficos de resíduos mais informativos.
\end{itemize}

Nos próximos tópicos, veremos como a estudentização externa corrige a
dependência entre numerador e denominador e fornece uma estatística com
distribuição \(t\) exata sob as hipóteses do modelo.

\subsection{Resíduos estudentizados
(externos)}\label{resuxedduos-estudentizados-externos}

Os \textbf{resíduos estudentizados externos} (também chamados de
\emph{externally studentized residuals} ou \emph{deleted residuals})
foram propostos no contexto de diagnóstico de regressão para contornar a
dependência entre numerador e denominador presente nos resíduos
padronizados (Belsley, Kuh, e Welsch (1980); Weisberg (2005)).

Eles são definidos por

\[
t_i^* = \frac{e_i}{s_{(i)} \sqrt{1 - h_{ii}}},
\]

em que:

\begin{itemize}
\tightlist
\item
  \(e_i\) é o resíduo ordinário da observação \(i\);
\item
  \(h_{ii}\) é a alavancagem da observação \(i\);
\item
  \(s_{(i)}^2\) é o estimador da variância do erro calculado
  \textbf{excluindo a i-ésima observação}.
\end{itemize}

Isto é, \(s_{(i)}^2\) é obtido ajustando o modelo com \(n-1\)
observações, removendo o ponto \(i\). Assim, o denominador não sofre
influência direta da própria observação cujo resíduo está sendo
avaliado.

\subsubsection{Motivação conceitual}\label{motivauxe7uxe3o-conceitual}

Nos resíduos padronizados,

\[
r_i = \frac{e_i}{s \sqrt{1-h_{ii}}},
\]

Como apresentado anteriormente, o estimador \(s^2\) é calculado usando
todas as observações. Se a observação \(i\) for discrepante, ela pode
inflar \(s^2\), reduzindo artificialmente \(|r_i|\) e dificultando sua
própria detecção.

Ao substituir \(s\) por \(s_{(i)}\), eliminamos essa retroalimentação. O
resíduo passa a ser avaliado em relação a um modelo que não foi
influenciado por ele mesmo.

\subsubsection{Distribuição exata}\label{distribuiuxe7uxe3o-exata}

Sob as hipóteses do MRLS com erros normais,

\[
\varepsilon_i \sim N(0,\sigma^2),
\]

temos que

\[
t_i^* \sim t_{n-3}.
\]

A perda de um grau de liberdade adicional (em comparação com
\(t_{n-2}\)) decorre do fato de que a variância foi estimada com \(n-3\)
graus de liberdade no modelo ajustado sem a observação \(i\) (Kutner et
al. (2005); Montgomery, Peck, e Vining (2021)).

Essa é uma propriedade importante: diferentemente dos resíduos
padronizados, aqui a distribuição \(t\) é \textbf{exata} sob normalidade
dos erros.

\begin{quote}
\textbf{Apêndice de Demonstrações \{\#demo\}:} a distribuição exata de
\(t_i^*\) é obtida mostrando que, sob \(H_0\), o numerador é normal e
independente do estimador \(s_{(i)}^2\), o qual é proporcional a uma
variável qui-quadrado com \(n-3\) graus de liberdade.
\end{quote}

\subsubsection{Interpretação
prática}\label{interpretauxe7uxe3o-pruxe1tica-1}

Como \(t_i^*\) segue exatamente uma distribuição \(t\), podemos utilizar
pontos críticos formais para avaliar discrepância individual:

\begin{itemize}
\tightlist
\item
  \(|t_i^*| > t_{1-\alpha/2,\, n-3}\) → evidência de que a observação
  \(i\) é discrepante ao nível \(\alpha\).
\end{itemize}

Na prática:

\begin{itemize}
\tightlist
\item
  \(|t_i^*| > 2\) sugere possível discrepância;
\item
  \(|t_i^*| > 3\) indica forte indício de outlier, especialmente em
  amostras moderadas.
\end{itemize}

Elevando ao quadrado:

\[
t_i^{*2} \sim F_{1,n-3},
\]

pois o quadrado de uma variável com distribuição \(t_k\) segue
distribuição \(F_{1,k}\) (Casella e Berger (2002)). Essa relação conecta
o diagnóstico individual de observações com a lógica dos testes \(F\)
discutidos anteriormente.

\section{Influência, alavancagem e leitura conjunta dos
resíduos}\label{influuxeancia-alavancagem-e-leitura-conjunta-dos-resuxedduos}

A etapa mais importante do diagnóstico no MRLS consiste em integrar três
dimensões distintas, mas complementares:

\begin{itemize}
\tightlist
\item
  discrepância na variável resposta (\(Y\));
\item
  posição extrema na variável explicativa (\(X\));
\item
  impacto global sobre os estimadores do modelo.
\end{itemize}

Essa integração é fundamental para evitar conclusões equivocadas
baseadas apenas no tamanho do resíduo.

\subsection{Relação entre discrepância, alavancagem e
influência}\label{relauxe7uxe3o-entre-discrepuxe2ncia-alavancagem-e-influuxeancia}

É importante distinguir conceitualmente:

\begin{itemize}
\tightlist
\item
  \textbf{Possível outlier em} \(Y\): grande \(|t_i^*|\);
\item
  \textbf{Alta alavancagem:} grande \(h_{ii}\);
\item
  \textbf{Observação influente:} combinação de grande \(|t_i^*|\) e
  grande \(h_{ii}\).
\end{itemize}

Um ponto pode apresentar alto resíduo, mas baixa alavancagem, afetando
pouco a inclinação da reta. Nesse caso, ele é discrepante na resposta,
mas não necessariamente influente.

Por outro lado, uma observação pode ter pequena discrepância em \(Y\),
mas alta alavancagem em \(X\), alterando significativamente a inclinação
estimada \(\hat\beta_1\). Nesse caso, mesmo com resíduo pequeno, o ponto
pode ser estruturalmente influente.

\subsection{Alavancagem no MRLS}\label{alavancagem-no-mrls}

A \textbf{alavancagem} da observação \(i\) é dada por

\[
h_{ii} = \frac{1}{n} + \frac{(X_i - \bar X)^2}{S_{xx}},
\quad \text{com} \quad
S_{xx} = \sum_{j=1}^n (X_j - \bar X)^2.
\]

Ela mede o quanto o valor de \(X_i\) influencia o próprio ajuste
\(\hat{Y}_i\).

Propriedades importantes no MRLS:

\begin{itemize}
\tightlist
\item
  \(0 < h_{ii} < 1\);
\item
  \[
  \sum_{i=1}^n h_{ii} = 2,
  \] pois dois parâmetros são estimados (\(\beta_0\) e \(\beta_1\));
\item
  a média das alavancagens é \(2/n\).
\end{itemize}

Observações com \(X_i\) muito afastado da média \(\bar X\) possuem maior
alavancagem e exercem maior influência geométrica sobre a reta ajustada
(Montgomery, Peck, e Vining (2021); Kutner et al. (2005)).

Uma regra prática comum é considerar como potencialmente alta
alavancagem valores tais que

\[
h_{ii} > \frac{2p}{n},
\]

em que \(p\) é o número de parâmetros do modelo (no MRLS, \(p=2\)).
Assim, valores acima de \(4/n\) merecem atenção especial (Belsley, Kuh,
e Welsch (1980)).

\subsection{Conexão entre alavancagem e variância
residual}\label{conexuxe3o-entre-alavancagem-e-variuxe2ncia-residual}

Recordando que

\[
Var(e_i) = \sigma^2 (1 - h_{ii}),
\]

vemos que observações com maior alavancagem apresentam menor variância
residual. Isso ocorre porque esses pontos ``puxam'' a reta para mais
perto de si.

Portanto, um ponto com alto \(h_{ii}\) pode ter resíduo pequeno não
porque esteja bem ajustado, mas porque influenciou fortemente o ajuste.

Essa distinção é conceitualmente importante:

\begin{itemize}
\tightlist
\item
  \textbf{Resíduo} mede discrepância vertical.
\item
  \textbf{Alavancagem} mede posição extrema em \(X\).
\item
  \textbf{Influência} mede alteração no modelo quando a observação é
  removida.
\end{itemize}

\subsection{Síntese diagnóstica}\label{suxedntese-diagnuxf3stica}

A leitura conjunta pode ser organizada da seguinte forma:

\begin{itemize}
\item
  \textbf{Resíduos grandes + baixa alavancagem}\\
  → outliers na resposta (\(Y\)), com impacto limitado na inclinação.
\item
  \textbf{Resíduos pequenos + alta alavancagem}\\
  → observações potencialmente influentes, mesmo sem grande discrepância
  aparente.
\item
  \textbf{Resíduos grandes + alta alavancagem}\\
  → casos críticos, com forte potencial de distorcer significativamente
  o ajuste.
\end{itemize}

Medidas integradas, como a distância de Cook,

\[
D_i = \frac{t_i^{*2}}{2} \cdot \frac{h_{ii}}{1 - h_{ii}},
\]

quantificam diretamente o quanto os estimadores
\((\hat\beta_0,\hat\beta_1)\) se alterariam caso a observação \(i\)
fosse removida (Belsley, Kuh, e Welsch (1980); Weisberg (2005)).

\subsection{Resumo comparativo dos
resíduos}\label{resumo-comparativo-dos-resuxedduos}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Tipo de resíduo
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Fórmula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(E(.)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(Var(.)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Distribuição
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Ordinário \(e_i\) & \(Y_i - \hat Y_i\) & \(0\) &
\(\sigma^2 (1 - h_{ii})\) & \(N(0, \sigma^2 (1 - h_{ii}))\) \\
Padronizado \(r_i\) & \(\dfrac{e_i}{s \sqrt{1-h_{ii}}}\) & \(\approx 0\)
& \(\approx 1\) & Aprox. \(t_{n-2}\) \\
Estudentizado \(t_i^*\) & \(\dfrac{e_i}{s_{(i)} \sqrt{1-h_{ii}}}\) &
\(0\) & \(1\) & \(t_{n-3}\) \\
\end{longtable}

Em síntese:

\begin{itemize}
\tightlist
\item
  \textbf{Resíduos ordinários} fornecem a discrepância bruta.
\item
  \textbf{Resíduos padronizados} tornam as observações comparáveis.
\item
  \textbf{Resíduos estudentizados externos} permitem inferência formal
  com distribuição \(t\) exata sob normalidade.
\item
  \textbf{Alavancagem} identifica observações estruturalmente extremas.
\item
  \textbf{Medidas de influência} integram discrepância e posição.
\end{itemize}

Somente essa leitura integrada permite avaliar adequadamente a robustez
do ajuste no MRLS e identificar observações com potencial de comprometer
a inferência estatística.

\section{Testes formais dos
resíduos}\label{testes-formais-dos-resuxedduos}

Antes da inspeção gráfica, é possível realizar \textbf{testes
estatísticos formais} aplicados aos resíduos do MRLS. Esses testes não
substituem a análise gráfica, mas fornecem evidência quantitativa sobre
possíveis violações das hipóteses clássicas, especialmente
\textbf{normalidade} e \textbf{independência} dos erros.

É fundamental compreender que tais testes avaliam hipóteses específicas
do modelo (por exemplo, normalidade dos erros), e não a ``qualidade
geral'' da regressão. A interpretação correta exige articulação entre
teoria, estatística e contexto (Kutner et al. (2005); Montgomery, Peck,
e Vining (2021); Weisberg (2005)).

\subsection{Teste para Assimetria
(Skewness)}\label{teste-para-assimetria-skewness}

A estatística de assimetria é definida por

\[
S = \frac{\frac{1}{n}\sum_{i=1}^n (e_i - \bar e)^3}
{\left(\frac{1}{n}\sum_{i=1}^n (e_i - \bar e)^2\right)^{3/2}},
\]

cujo valores de referência são:

\begin{itemize}
\tightlist
\item
  \(S=0\) → simetria;
\item
  \(S>0\) → cauda longa à direita;
\item
  \(S<0\) → cauda longa à esquerda.
\end{itemize}

Sob a hipótese de normalidade dos resíduos, podemos formular as
seguintes hipóteses:

\[
H_0: \text{Distribuição simétrica (} S = 0\text{)}
\]

\[
H_1: \text{Distribuição assimétrica (} S \neq 0\text{)}
\]

Para amostras grandes, vale a aproximação assintótica:

\[
Z_S = \sqrt{\frac{n}{6}}\, S \sim N(0,1).
\] Esse teste verifica se há evidência estatística de assimetria na
distribuição residual. Valores positivos indicam cauda longa à direita;
valores negativos indicam cauda longa à esquerda.

Assimetria residual pode indicar: - variável resposta naturalmente
assimétrica (ex.: tempos, rendas); - necessidade de transformação; -
presença de outliers em apenas um lado da distribuição.

A assimetria detectada estatisticamente pode ser irrelevante do ponto de
vista prático se o impacto sobre estimativas e previsões for pequeno.
Por isso, a análise gráfica (histograma e QQ-plot) é complementar e
essencial (Weisberg (2005); Montgomery, Peck, e Vining (2021)).

\subsection{Teste para Curtose
(Kurtosis)}\label{teste-para-curtose-kurtosis}

A curtose é definida por

\[
K = \frac{\frac{1}{n}\sum_{i=1}^n (e_i - \bar e)^4}
{\left(\frac{1}{n}\sum_{i=1}^n (e_i - \bar e)^2\right)^2},
\]

com os seguinte valores de referência:

\begin{itemize}
\tightlist
\item
  \(K=3\) → normal (mesocúrtica);
\item
  \(K>3\) → caudas pesadas (leptocúrtica);
\item
  \(K<3\) → caudas leves (platicúrtica).
\end{itemize}

Sob a hipótese de normalidade dos resíduos, podemos formular as
seguintes hipóteses:

\[
H_0: K = 3
\]

\[
H_1: K \neq 3
\]

Para amostras grandes:

\[
Z_K = \sqrt{\frac{n}{24}} (K - 3) \sim N(0,1).
\]

Curtose elevada frequentemente sinaliza presença de outliers ou
heterogeneidade de variância. Caudas pesadas significam maior
probabilidade de valores extremos, o que pode afetar inferência e
previsão.

Assim como a assimetria, a curtose deve ser interpretada junto com
resíduos estudentizados e medidas de influência. Muitas vezes, poucos
pontos extremos explicam grande parte da rejeição da normalidade
(Belsley, Kuh, e Welsch (1980); Weisberg (2005)).

\subsection{3. Omnibus Test
(D'Agostino--Pearson)}\label{omnibus-test-dagostinopearson}

O teste Omnibus combina os dois testes anteriores (assimetria e curtose)
em uma única estatística.

Sejam:

\[
Z_1 = Z_S
\quad \text{e} \quad
Z_2 = Z_K.
\]

A estatística do teste é:

\[
OM = Z_1^2 + Z_2^2.
\]

Ou seja, \(Z_1\) é a estatística padronizada da assimetria e \(Z_2\) a
da curtose. Sob \(H_0\) (normalidade), vale assintoticamente:

Para o teste Omnibus, formulmos as seguintes hipóteses: \[
H_0: \text{Resíduos seguem distribuição normal}
\]

\[
H_1: \text{Resíduos não seguem distribuição normal}
\] Sob \(H_0\),

\[
OM \sim \chi^2_{(2)}.
\]

O Omnibus é um teste conjunto: ele detecta qualquer violação que afete
simetria ou curtose. Em vez de avaliar dois testes separados, consolida
evidência em uma única estatística. Adicionalmente, como a distribuição
é assintótica, sua confiabilidade aumenta com o tamanho amostral. Em
amostras pequenas, o teste pode apresentar distorções no nível de
significância.

\subsection{4. Jarque--Bera (JB)}\label{jarquebera-jb}

O teste de Jarque--Bera também combina assimetria e curtose, mas
diretamente em termos de seus estimadores:

\[
JB = \frac{n}{6}\left(S^2 + \frac{(K-3)^2}{4}\right).
\]

Hipóteses do teste são:

\[
H_0: \text{Resíduos seguem distribuição normal}
\]

\[
H_1: \text{Resíduos não seguem distribuição normal}
\]

Sob \(H_0\),

\[
JB \sim \chi^2_{(2)}.
\]

Observe que o JB é equivalente, do ponto de vista assintótico, à soma
dos quadrados das versões padronizadas de \(S\) e \(K-3\).

O JB mede a distância conjunta entre a distribuição empírica dos
resíduos e a normal, considerando forma (assimetria) e peso de caudas
(curtose).

Note que rejeitar normalidade não implica que o modelo linear esteja
incorreto, isso pode indicar apenas que os erros não são gaussianos. A
relevância prática depende do objetivo (estimação, teste, previsão) e do
tamanho da amostra (Casella e Berger (2002); Kutner et al. (2005)).
\#\#\# 5. Durbin--Watson (DW)

O teste de Durbin--Watson verifica autocorrelação serial:

\[
DW = \frac{\sum_{t=2}^n (e_t - e_{t-1})^2}
{\sum_{t=1}^n e_t^2}.
\]

As hipóteses clássicas são:

\[
H_0: \rho = 0 \quad (\text{ausência de autocorrelação})
\]

\[
H_1: \rho \neq 0 \quad (\text{autocorrelação})
\]

A interpretação usual é:

\begin{itemize}
\tightlist
\item
  \(DW \approx 2\) → ausência de autocorrelação;
\item
  \(DW < 2\) → autocorrelação positiva;
\item
  \(DW > 2\) → autocorrelação negativa.
\end{itemize}

O DW mede o quanto os resíduos consecutivos diferem entre si. Se \(e_t\)
e \(e_{t-1}\) forem semelhantes (dependência positiva), o numerador será
pequeno e \(DW\) ficará abaixo de 2.

Este teste é especialmente relevante em dados ordenados temporalmente
(econometria, séries temporais). Em dados sem ordem natural, sua
aplicação é menos informativa (Gujarati (2006)).

Autocorrelação residual pode indicar: - tendência não modelada; -
variáveis omitidas; - estrutura dinâmica inerente ao fenômeno.

Detectar autocorrelação é apenas o início do diagnóstico.

\section{Diagnóstico gráfico do
MRLS}\label{diagnuxf3stico-gruxe1fico-do-mrls}

A análise gráfica dos resíduos é uma das etapas mais importantes na
verificação das hipóteses do MRLS. Os gráficos funcionam como
ferramentas de diagnóstico visual, permitindo identificar padrões que
revelem problemas estruturais no modelo Montgomery, Peck, e Vining
(2021); Kutner et al. (2005); Weisberg (2005).

Em um \textbf{modelo bem especificado}, os resíduos devem se comportar
como \textbf{ruído puro}: dispersão aleatória em torno de zero,
variância aproximadamente constante e sem estrutura aparente. Em termos
práticos, isso significa que, \textbf{condicionado aos valores de}
\(X\), não deve existir informação sistemática remanescente nos resíduos
que pudesse ser capturada por uma reespecificação simples do modelo (por
exemplo, inclusão de termos não lineares ou transformação da resposta)
Montgomery, Peck, e Vining (2021); Weisberg (2005).

A seguir, são descritos os principais gráficos e o que se esperar de
cada um.

\subsection{Resíduos vs ajustados (linearidade e
homoscedasticidade)}\label{resuxedduos-vs-ajustados-linearidade-e-homoscedasticidade}

Este é o gráfico diagnóstico mais usado na prática, pois confronta
diretamente o ``erro'' estimado (resíduo) com o nível de resposta
previsto pelo modelo.

\begin{itemize}
\tightlist
\item
  \textbf{O que se espera}:

  \begin{itemize}
  \tightlist
  \item
    pontos dispersos aleatoriamente em torno da linha horizontal \(0\),
    sem padrão definido.\\
  \item
    amplitude (dispersão vertical) aproximadamente constante ao longo de
    toda a faixa de \(\hat Y_i\).\\
  \item
    poucos pontos ultrapassando as faixas de referência usuais (por
    exemplo, \(|r_i| \approx 2\) ou \(|t_i^*| \approx 2\), dependendo do
    resíduo adotado).
  \end{itemize}
\item
  \textbf{O que indica problema}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{padrão em curva} → sugere que a relação média \(E(Y\mid X)\)
    não está bem representada por uma função linear; pode indicar
    necessidade de termos como \(X^2\) ou outra reespecificação
    funcional.\\
  \item
    \textbf{forma de funil} (variância aumenta ou diminui com
    \(\hat Y_i\)) → indício de heteroscedasticidade (variância não
    constante).\\
  \item
    \textbf{concentração de resíduos positivos (negativos)} em certas
    regiões → modelo subestima (superestima) sistematicamente nessas
    regiões, sugerindo viés local de especificação.\\
  \item
    \textbf{pontos isolados muito afastados} do conjunto principal →
    possível outlier/influência; a confirmação deve ser feita em leitura
    conjunta com resíduos estudentizados \(t_i^*\), alavancagem
    \(h_{ii}\) e medidas de influência como a distância de Cook Belsley,
    Kuh, e Welsch (1980); Weisberg (2005).
  \end{itemize}
\end{itemize}

Para diagnóstico visual, é recomendável utilizar resíduos que sejam
comparáveis entre observações. Assim, em muitos contextos prefere-se
plotar resíduos padronizados (\(r_i\)) ou estudentizados externos
(\(t_i^*\)), em vez de resíduos ordinários (\(e_i\)), pois estes últimos
têm variância dependente da alavancagem \((1-h_{ii})\) (Montgomery,
Peck, e Vining (2021); Kutner et al. (2005)).

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_diagnostico_files/figure-pdf/unnamed-chunk-1-1.pdf}}

}

\caption{Resíduos vs Ajustados --- Esquerda: Ajuste bom (homocedástico);
Direita: Ajuste ruim (funil/heterocedasticidade). Linhas em 0 e ±2.}

\end{figure}%

\subsection{\texorpdfstring{Resíduos vs \(X\) (forma
funcional)}{Resíduos vs X (forma funcional)}}\label{resuxedduos-vs-x-forma-funcional}

Este gráfico é conceitualmente muito próximo ao anterior, mas desloca o
foco: em vez de relacionar os resíduos com os valores ajustados
\(\hat Y_i\), relaciona-os diretamente com a variável explicativa
\(X_i\).

\begin{itemize}
\tightlist
\item
  \textbf{O que se espera}:

  \begin{itemize}
  \tightlist
  \item
    aleatoriedade semelhante ao gráfico anterior, mas agora em função de
    \(X\).\\
  \item
    dispersão aproximadamente constante ao longo de toda a faixa de
    \(X\).\\
  \item
    ausência de estruturas sistemáticas associadas a regiões específicas
    de \(X\).
  \end{itemize}
\item
  \textbf{O que indica problema}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{estruturas em forma de arco ou curva} → o efeito de \(X\)
    pode ser não linear; o modelo linear \(E(Y\mid X)=\beta_0+\beta_1X\)
    pode estar omitindo termos relevantes (por exemplo, \(X^2\) ou outra
    transformação).\\
  \item
    \textbf{padrões em ``S'' ou mudança de inclinação} → possível quebra
    de regime ou efeito estrutural não capturado.\\
  \item
    \textbf{faixas onde a dispersão muda} → variação da variância
    conforme \(X\), sugerindo heteroscedasticidade.\\
  \item
    \textbf{concentração de pontos extremos em regiões específicas de}
    \(X\) → possível influência associada a valores extremos da variável
    explicativa.
  \end{itemize}
\end{itemize}

Enquanto o gráfico resíduos vs ajustados enfatiza o comportamento do
erro em relação à resposta prevista, o gráfico resíduos vs \(X\)
enfatiza a adequação da \textbf{forma funcional} da regressão. Ele
permite avaliar diretamente se a hipótese de linearidade entre \(X\) e a
média condicional de \(Y\) é plausível (Montgomery, Peck, e Vining
(2021); Kutner et al. (2005)).

Este gráfico é especialmente informativo quando \(X\) possui
interpretação física, econômica ou temporal clara. Nesses casos, padrões
sistemáticos ao longo de \(X\) podem revelar efeitos omitidos, mudanças
estruturais ou fenômenos não lineares que não são imediatamente visíveis
no gráfico resíduos vs ajustados.

Assim como no gráfico anterior, recomenda-se utilizar resíduos
padronizados ou estudentizados para tornar a escala comparável entre
observações, principalmente quando há variação relevante na alavancagem
\(h_{ii}\).

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_diagnostico_files/figure-pdf/unnamed-chunk-2-1.pdf}}

}

\caption{Resíduos vs X --- Esquerda: Ajuste bom (linear); Direita:
Ajuste ruim (não linearidade em arco). Linhas em 0 e ±2.}

\end{figure}%

\subsection{Resíduos estudentizados vs valores ajustados (outliers +
estrutura)}\label{resuxedduos-estudentizados-vs-valores-ajustados-outliers-estrutura}

Este gráfico é uma versão refinada do gráfico resíduos vs ajustados,
utilizando os \textbf{resíduos estudentizados externos} \(t_i^*\). Ele
combina duas dimensões do diagnóstico: discrepância individual e
possível estrutura sistemática.

\begin{itemize}
\tightlist
\item
  \textbf{Por que usar}:

  \begin{itemize}
  \tightlist
  \item
    tornam resíduos comparáveis, pois ajustam pela variância individual
    de cada ponto, incorporando o fator \((1-h_{ii})\) associado à
    alavancagem.\\
  \item
    utilizam uma estimativa da variância \(\sigma^2\) calculada sem a
    observação \(i\) (\(s_{(i)}\)), reduzindo o efeito de mascaramento
    que pode ocorrer quando um ponto extremo influencia a própria
    estimativa de variância.\\
  \item
    possuem, sob normalidade dos erros, distribuição exata \(t_{n-3}\),
    permitindo interpretação inferencial mais precisa (Montgomery, Peck,
    e Vining (2021); Kutner et al. (2005)).
  \end{itemize}
\item
  \textbf{O que se espera}:

  \begin{itemize}
  \tightlist
  \item
    aleatoriedade em torno da linha horizontal \(0\).\\
  \item
    a maioria dos pontos entre \(-2\) e \(+2\), sendo raros valores com
    \(|t_i^*|>3\) em amostras moderadas.\\
  \item
    ausência de padrão sistemático ao longo da faixa de valores
    ajustados.
  \end{itemize}
\item
  \textbf{O que indica problema}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{pontos fora do intervalo} \([-2,2]\) → observações
    potencialmente discrepantes; valores acima de \(|t_i^*|>3\) são
    frequentemente considerados fortemente suspeitos.\\
  \item
    \textbf{estruturas visíveis (curvas, funis)} → possíveis violações
    de linearidade ou homocedasticidade, agora avaliadas com resíduos
    que já consideram diferenças de variância individual.\\
  \item
    \textbf{concentração de valores extremos em regiões de alta
    alavancagem} → possível influência desproporcional sobre os
    estimadores.
  \end{itemize}
\end{itemize}

Os resíduos estudentizados externos medem o quanto cada observação se
afasta do modelo ajustado, levando em conta tanto a variabilidade
residual quanto sua própria posição geométrica no conjunto de dados.
Assim, eles são especialmente adequados para identificar
\textbf{outliers reais}, isto é, observações cuja discrepância não pode
ser explicada apenas por sua alavancagem.

Um ponto com resíduo ordinário grande pode deixar de parecer extremo
após a estudentização se sua variância condicional for naturalmente
maior. Por outro lado, um ponto que permanece extremo mesmo após a
correção por \((1-h_{ii})\) e por \(s_{(i)}\) merece investigação
cuidadosa --- seja por erro de registro, seja por representar um
fenômeno estrutural distinto (Belsley, Kuh, e Welsch (1980); Weisberg
(2005)).

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_diagnostico_files/figure-pdf/unnamed-chunk-3-1.pdf}}

}

\caption{Resíduos estudentizados vs Ajustados --- Esquerda: Ajuste bom;
Direita: Ajuste ruim (curvatura + funil). Linhas em ±2.}

\end{figure}%

\subsection{QQ-plot (normalidade)}\label{qq-plot-normalidade}

O gráfico QQ-plot (quantile--quantile) compara os quantis empíricos dos
resíduos com os quantis teóricos de uma distribuição normal padrão. Ele
é uma das ferramentas mais informativas para avaliar a hipótese de
normalidade dos erros no MRLS (Montgomery, Peck, e Vining (2021); Kutner
et al. (2005); Weisberg (2005)).

\begin{itemize}
\tightlist
\item
  \textbf{O que se espera}:

  \begin{itemize}
  \tightlist
  \item
    pontos aproximadamente alinhados em torno da reta de 45°, indicando
    que os resíduos seguem aproximadamente uma distribuição normal.\\
  \item
    pequenas flutuações aleatórias ao redor da reta, especialmente no
    centro da distribuição.\\
  \item
    ausência de desvios sistemáticos nas caudas.
  \end{itemize}
\item
  \textbf{O que indica problema}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{desvios sistemáticos nas extremidades} → caudas mais pesadas
    (pontos afastados da reta nas pontas) ou mais leves que a normal.\\
  \item
    \textbf{desvios em formato de ``S''} → indício de assimetria dos
    resíduos.\\
  \item
    \textbf{afastamentos persistentes ao longo de toda a reta} →
    possível inadequação global da suposição de normalidade.\\
  \item
    \textbf{pontos isolados muito distantes nas pontas} → presença de
    outliers, que podem ser responsáveis por grande parte da violação
    observada.
  \end{itemize}
\end{itemize}

O QQ-plot compara toda a \textbf{forma da distribuição}. Se os resíduos
forem normais, seus quantis empíricos devem crescer linearmente com os
quantis teóricos da normal. Desvios sistemáticos dessa linearidade
indicam diferenças estruturais entre as distribuições.

É fundamental interpretar o QQ-plot em conjunto com resíduos
estudentizados e medidas de influência. Muitas vezes, poucos pontos
extremos explicam a maior parte do desvio observado nas caudas. Além
disso, pequenas curvaturas no centro do gráfico, especialmente em
amostras grandes, podem não ter relevância prática para a inferência,
sobretudo quando o objetivo principal é previsão e não testes exatos em
pequenas amostras (Casella e Berger (2002); Kutner et al. (2005)).

O QQ-plot, portanto, oferece uma visão global da normalidade e
complementa tanto os testes formais (como Jarque--Bera) quanto os
gráficos de histograma.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_diagnostico_files/figure-pdf/unnamed-chunk-4-1.pdf}}

}

\caption{QQ-plot dos resíduos --- Esquerda: Ajuste bom (erros
\textasciitilde{} Normal); Direita: Ajuste ruim (erros \textasciitilde{}
t com caudas pesadas).}

\end{figure}%

\subsection{Histograma (assimetria e
caudas)}\label{histograma-assimetria-e-caudas}

O histograma dos resíduos é uma ferramenta complementar ao QQ-plot.
Enquanto o QQ-plot enfatiza o alinhamento com a normal teórica por meio
de quantis, o histograma permite visualizar diretamente a \textbf{forma
empírica} da distribuição residual (Montgomery, Peck, e Vining (2021);
Kutner et al. (2005); Weisberg (2005)).

\begin{itemize}
\tightlist
\item
  \textbf{O que se espera}:

  \begin{itemize}
  \tightlist
  \item
    distribuição aproximadamente simétrica em torno de zero.\\
  \item
    formato aproximadamente em sino (curva unimodal e suave).\\
  \item
    maior concentração de valores próximos de \(0\), com frequência
    decrescente nas extremidades.
  \end{itemize}
\item
  \textbf{O que indica problema}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{assimetria} → possível necessidade de transformação na
    resposta (\(Y\)), como \(\log(Y)\) ou \(\sqrt{Y}\), especialmente
    quando a assimetria é estrutural e não causada por poucos pontos
    extremos.\\
  \item
    \textbf{caudas longas} → presença de outliers ou distribuição com
    maior probabilidade de valores extremos do que a normal.\\
  \item
    \textbf{bimodalidade ou múltiplos picos} → possível mistura de
    grupos ou estrutura omitida no modelo (por exemplo, variável
    categórica não incluída).\\
  \item
    \textbf{concentração excessiva no centro com poucas observações nas
    extremidades} → caudas leves (platicurtose), também incompatíveis
    com normalidade.
  \end{itemize}
\end{itemize}

O histograma fornece uma visão direta da densidade empírica dos
resíduos. Em um modelo com erros normais, espera-se que a forma geral
seja compatível com a curva Normal\((0,\sigma^2)\). Desvios sistemáticos
dessa forma indicam diferenças estruturais na distribuição do erro.

Adicionamente, o histograma é sensível à escolha do número de classes
(bins). Diferentes escolhas podem alterar a percepção visual da forma.
Por isso, recomenda-se utilizá-lo em conjunto com o QQ-plot e com
medidas numéricas de assimetria e curtose.

Além disso, é importante lembrar que pequenas assimetrias visuais,
especialmente em amostras grandes, podem não comprometer de forma
relevante a inferência baseada em MQO, cuja robustez assintótica é
discutida em (Casella e Berger (2002); Kutner et al. (2005)).

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_diagnostico_files/figure-pdf/unnamed-chunk-5-1.pdf}}

}

\caption{Histograma dos resíduos --- Esquerda: Ajuste bom (simétrico);
Direita: Ajuste ruim (assimetria à direita). Curva Normal(0,1) de
referência.}

\end{figure}%

\subsection{Resíduos estudentizados vs índice (pontos
atípicos)}\label{resuxedduos-estudentizados-vs-uxedndice-pontos-atuxedpicos}

Este gráfico apresenta os resíduos estudentizados externos \(t_i^*\) em
função do índice da observação \(i\). Ele é particularmente útil para
identificar \textbf{observações discrepantes individuais}, destacando
sua posição relativa no conjunto de dados.

\begin{itemize}
\tightlist
\item
  \textbf{Por que usar}:

  \begin{itemize}
  \tightlist
  \item
    são melhores na detecção de outliers, pois corrigem a influência da
    própria observação ao utilizar a estimativa de variância
    \(s_{(i)}\), calculada sem o ponto \(i\).\\
  \item
    possuem distribuição \(t_{n-3}\) sob normalidade dos erros,
    permitindo interpretação inferencial direta.\\
  \item
    facilitam a visualização de padrões associados à ordem natural dos
    dados (por exemplo, tempo ou sequência experimental) Montgomery,
    Peck, e Vining (2021); Kutner et al. (2005).
  \end{itemize}
\item
  \textbf{O que se espera}:

  \begin{itemize}
  \tightlist
  \item
    quase todos os pontos entre \(-2\) e \(+2\).\\
  \item
    raros pontos ultrapassando \(|t_i^*|>3\), especialmente em amostras
    moderadas.\\
  \item
    ausência de padrões sistemáticos ao longo do índice.
  \end{itemize}
\item
  \textbf{O que indica problema}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{valores extremos em} \(|t_i^*|\) → sugerem observações
    potencialmente discrepantes; quanto maior o valor absoluto, maior a
    evidência de que o ponto não é compatível com a variabilidade
    esperada sob o modelo.\\
  \item
    \textbf{agrupamento de valores extremos em determinadas regiões do
    índice} → pode indicar mudança estrutural, dependência temporal ou
    erro sistemático de medição.\\
  \item
    \textbf{padrões alternados (positivo--negativo--positivo)} →
    possível autocorrelação residual, especialmente quando os dados
    possuem ordem temporal.
  \end{itemize}
\end{itemize}

Este gráfico não apenas identifica outliers, mas também permite
verificar se tais observações estão distribuídas aleatoriamente ao longo
do conjunto de dados ou se seguem algum padrão estrutural.

O significado do eixo ``índice'' depende do contexto. Se os dados
tiverem uma ordem natural (tempo, experimento sequencial, posição
espacial), padrões nesse gráfico podem indicar violação da hipótese de
independência dos erros. Se não houver ordem natural, o gráfico atua
principalmente como ferramenta de localização de observações
discrepantes.

Além disso, um valor extremo em \(t_i^*\) não implica automaticamente
exclusão da observação. Deve-se verificar conjuntamente a alavancagem
\(h_{ii}\) e medidas de influência (como a distância de Cook) antes de
qualquer decisão sobre reespecificação ou remoção de dados (Belsley,
Kuh, e Welsch (1980); Weisberg (2005)).

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_diagnostico_files/figure-pdf/unnamed-chunk-6-1.png}}

}

\caption{Resíduos estudentizados vs índice --- Esquerda: ajuste bom
(Normal, sem outliers); Direita: ajuste ruim (caudas pesadas +
outliers). Linhas em 0 e ±2; pontos extremos destacados.}

\end{figure}%

\subsection{Resíduos estudentizados ao quadrado vs valores ajustados
(heteroscedasticidade /
influência)}\label{resuxedduos-estudentizados-ao-quadrado-vs-valores-ajustados-heteroscedasticidade-influuxeancia}

Este gráfico utiliza \(t_i^{*2}\) (resíduos estudentizados externos ao
quadrado) no eixo vertical e os valores ajustados \(\hat Y_i\) no eixo
horizontal. Ao elevar ao quadrado, eliminamos o sinal e focamos
exclusivamente na \textbf{magnitude da discrepância}, o que é
particularmente útil para investigar padrões de variância.

\begin{itemize}
\tightlist
\item
  \textbf{O que se espera}:

  \begin{itemize}
  \tightlist
  \item
    dispersão aproximadamente uniforme ao longo da faixa de
    \(\hat Y_i\).\\
  \item
    ausência de tendência sistemática crescente ou decrescente.\\
  \item
    pontos distribuídos sem estrutura definida ao redor de um nível
    aproximadamente constante.
  \end{itemize}
\item
  \textbf{O que indica problema}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{crescimento ou redução sistemática} de \(t_i^{*2}\) conforme
    \(\hat Y_i\) aumenta → indício de heteroscedasticidade (variância
    não constante).\\
  \item
    \textbf{estrutura em arco} → possível não linearidade na função
    média.\\
  \item
    \textbf{pontos isolados com valores muito elevados de} \(t_i^{*2}\)
    → observações potencialmente influentes ou discrepantes.
  \end{itemize}
\end{itemize}

Se o modelo satisfaz a hipótese de homocedasticidade, então \(Var(e_i)\)
deve ser constante. Como \(t_i^*\) já corrige por \((1-h_{ii})\) e por
\(s_{(i)}\), padrões sistemáticos em \(t_i^{*2}\) sugerem que a
variância condicional de \(Y\) depende do nível da resposta, ou seja,
\(Var(Y \mid X)\) não é constante.

Ao trabalhar com o quadrado do resíduo, pequenas diferenças tornam-se
mais visíveis. Por isso, esse gráfico frequentemente revela tendências
de variância que não são tão evidentes no gráfico simples resíduos vs
ajustados.

A inclusão de uma curva suave (por exemplo, LOESS) auxilia na
visualização de tendências médias na magnitude dos resíduos. Se essa
curva apresentar inclinação clara ou formato sistemático, há evidência
visual de heteroscedasticidade (Montgomery, Peck, e Vining (2021);
Kutner et al. (2005); Weisberg (2005)).

Este gráfico, portanto, complementa o diagnóstico tradicional,
oferecendo uma perspectiva focada especificamente na estrutura da
variância.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_diagnostico_files/figure-pdf/unnamed-chunk-7-1.png}}

}

\caption{Valores ajustados vs resíduos estudentizados² --- Esquerda:
ajuste bom (dispersão uniforme); Direita: ajuste ruim
(heteroscedasticidade/funil). LOWESS em vermelho.}

\end{figure}%

\subsection{Alavancagem vs resíduos estudentizados (influência /
Cook)}\label{alavancagem-vs-resuxedduos-estudentizados-influuxeancia-cook}

Este gráfico combina duas dimensões centrais do diagnóstico no MRLS:\\
- \textbf{alavancagem} (\(h_{ii}\)), que mede o quão extremo é o valor
de \(X_i\);\\
- \textbf{resíduo estudentizado externo} (\(t_i^*\)), que mede a
discrepância vertical ajustada pela variância condicional.

Ao analisar ambos simultaneamente, obtemos uma visão direta da
\textbf{influência potencial} de cada observação sobre os estimadores do
modelo (Belsley, Kuh, e Welsch (1980); Weisberg (2005); Montgomery,
Peck, e Vining (2021)).

\begin{itemize}
\tightlist
\item
  \textbf{Objetivo}:

  \begin{itemize}
  \tightlist
  \item
    identificar observações influentes, isto é, aquelas que combinam
    alto resíduo e alta alavancagem.\\
  \item
    distinguir entre pontos apenas discrepantes (grande \(|t_i^*|\)) e
    pontos estruturalmente extremos (grande \(h_{ii}\)).\\
  \end{itemize}
\item
  \textbf{O que se espera}:

  \begin{itemize}
  \tightlist
  \item
    maioria dos pontos dentro da ``nuvem central'', isto é, com valores
    moderados de \(h_{ii}\) e \(|t_i^*| \leq 2\).\\
  \item
    poucos pontos próximos do limite usual de alavancagem (por exemplo,
    \(h_{ii} > 2p/n\)).\\
  \item
    ausência de observações simultaneamente extremas em ambas as
    direções.
  \end{itemize}
\item
  \textbf{O que indica problema}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{pontos afastados horizontalmente} (alto \(h_{ii}\)) →
    observações com grande potencial de influenciar a inclinação da
    reta.\\
  \item
    \textbf{pontos afastados verticalmente} (alto \(|t_i^*|\)) →
    observações discrepantes na resposta.\\
  \item
    \textbf{pontos afastados horizontal e verticalmente} → fortes
    candidatos a observações influentes, com impacto potencialmente
    desproporcional sobre os estimadores.\\
  \end{itemize}
\end{itemize}

Influência não é sinônimo de discrepância. Um ponto pode ter grande
resíduo, mas baixa alavancagem, afetando pouco os coeficientes. Da mesma
forma, um ponto pode ter alta alavancagem e resíduo pequeno, mas ainda
assim influenciar a inclinação da reta por estar em região extrema de
\(X\).

Observações influentes não devem ser automaticamente removidas. Elas
podem representar fenômenos legítimos do processo gerador dos dados. O
papel do diagnóstico é identificar e compreender tais pontos, não
eliminá-los mecanicamente.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_diagnostico_files/figure-pdf/unnamed-chunk-8-1.png}}

}

\caption{Alavancagem (h) vs resíduos estudentizados --- Esquerda: ajuste
bom; Direita: ajuste ruim com ponto de alta alavancagem e discrepância.
Área ∝ distância de Cook. Linha vertical: h \textgreater{} 2p/n.}

\end{figure}%

Os gráficos de resíduos são \textbf{mapas visuais do ajuste}. Eles
sintetizam, de forma intuitiva, as hipóteses estruturais do MRLS e
permitem avaliar se o modelo capturou adequadamente a relação entre
\(X\) e \(Y\) (Montgomery, Peck, e Vining (2021); Kutner et al. (2005);
Weisberg (2005)).

Quando o modelo está bem especificado, espera-se que:

\begin{itemize}
\tightlist
\item
  os resíduos flutuem aleatoriamente em torno de zero;\\
\item
  a variância seja aproximadamente constante ao longo de toda a faixa de
  \(X\) ou \(\hat Y\);\\
\item
  a distribuição seja aproximadamente normal (quando a inferência exata
  via \(t\) e \(F\) é relevante);\\
\item
  não existam observações com influência desproporcional sobre os
  estimadores.
\end{itemize}

Essas características indicam que, condicionalmente a \(X\), o modelo
não deixou estrutura sistemática não explicada.

Quando há padrões, eles indicam possíveis caminhos de correção:

\begin{itemize}
\tightlist
\item
  \textbf{Curvaturas} nos gráficos → possível inadequação da forma
  funcional; pode ser necessária a inclusão de termos como \(X^2\),
  transformações em \(X\) (por exemplo, \(\log(X)\)) ou outra
  reespecificação da função média.\\
\item
  \textbf{Variância crescente ou decrescente} → indício de
  heteroscedasticidade; transformações em \(Y\) (como \(\log(Y)\) ou
  \(\sqrt{Y}\)) ou métodos que acomodem variância não constante podem
  ser considerados.\\
\item
  \textbf{Assimetria na distribuição dos resíduos} → possível
  necessidade de transformação na resposta ou presença de outliers que
  devem ser investigados.\\
\item
  \textbf{Observações influentes} → revisão individual do ponto,
  verificação de erros de registro ou análise substantiva do fenômeno
  representado.
\end{itemize}

Cada padrão visual corresponde a uma hipótese específica do modelo.
Assim, o diagnóstico gráfico não é apenas uma etapa técnica, mas uma
verificação das suposições matemáticas que fundamentam a inferência no
MRLS.

Transformações não devem ser aplicadas de forma automática ou mecânica.
Elas devem ser justificadas teoricamente, interpretadas no contexto do
problema e validadas por novo ciclo de diagnóstico após o reajuste do
modelo. O processo é iterativo: ajustar → diagnosticar → reespecificar →
diagnosticar novamente.

\section{Aspectos computacionais para resíduos no
R}\label{aspectos-computacionais-para-resuxedduos-no-r}

Para a análise gráfica de resíduos no \textbf{MRLS}, podemos usar
principalmente os pacotes:

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{stats}} → para ajustar o modelo com \texttt{lm()} e
  extrair resíduos e ajustados (\texttt{residuals()},
  \texttt{fitted()}).
\item
  \textbf{\texttt{ggplot2}} → para construir gráficos com
  \texttt{geom\_point()}, \texttt{geom\_hline()},
  \texttt{geom\_vline()}, \texttt{facet\_wrap()}.
\item
  \textbf{\texttt{broom}} → para organizar saídas do modelo em data
  frames (\texttt{augment()}), incluindo resíduos e ajustados.
\item
  \textbf{\texttt{car}} (opcional) → para alguns diagnósticos e gráficos
  prontos (ex.: \texttt{residualPlots()}).
\item
  \textbf{\texttt{ggfortify}} (opcional) → para gráficos diagnósticos
  automáticos a partir de objetos \texttt{lm} (\texttt{autoplot()}).
\item
  \textbf{\texttt{stats}} + \textbf{\texttt{ggplot2}} → para QQ-plot
  (via \texttt{qqnorm()/qqline()} ou construção manual para
  \texttt{ggplot2}).
\item
  \textbf{\texttt{MASS}} (opcional) → para simulações com distribuições
  alternativas quando necessário.
\item
  \textbf{\texttt{lmtest}} / \textbf{\texttt{sandwich}} (opcional) →
  para testes formais (Breusch--Pagan etc.) e erros-padrão robustos.
\end{itemize}

A seguir, quais funções e pacotes usar em cada gráfico:

\textbf{1. Resíduos vs.~Valores Ajustados}

\begin{itemize}
\tightlist
\item
  Objetivo: verificar aleatoriedade e homocedasticidade.
\item
  Funções/objetos:

  \begin{itemize}
  \tightlist
  \item
    \texttt{modelo\ \textless{}-\ lm(Y\ \textasciitilde{}\ X,\ data=df)}
  \item
    \texttt{fitted\ \textless{}-\ fitted(modelo)}
  \item
    \texttt{resid\ \ \textless{}-\ resid(modelo)}
  \end{itemize}
\item
  Em \texttt{ggplot2}: \texttt{geom\_point()} +
  \texttt{geom\_hline(yintercept=0,\ ...)}.
\end{itemize}

\textbf{2. Resíduos vs.~Variável Explicativa (}\(X\))

\begin{itemize}
\tightlist
\item
  Objetivo: avaliar linearidade da relação entre \(Y\) e \(X\).
\item
  Funções/objetos:

  \begin{itemize}
  \tightlist
  \item
    \texttt{resid\ \textless{}-\ resid(modelo)}
  \end{itemize}
\item
  Em \texttt{ggplot2}: \texttt{geom\_point()} com
  \texttt{aes(x=X,\ y=resid)} +
  \texttt{geom\_hline(yintercept=0,\ ...)}.
\end{itemize}

\textbf{3. Resíduos Estudentizados vs.~Valores Ajustados}

\begin{itemize}
\tightlist
\item
  Objetivo: detectar outliers e padrões (considerando alavancagem).
\item
  Funções/objetos:

  \begin{itemize}
  \tightlist
  \item
    \texttt{stud\ \textless{}-\ rstudent(modelo)} (resíduos
    estudentizados externos)
  \item
    \texttt{fitted\ \textless{}-\ fitted(modelo)}
  \end{itemize}
\item
  Em \texttt{ggplot2}: \texttt{geom\_point()} +
  \texttt{geom\_hline(yintercept=c(-2,2),\ ...)}.
\end{itemize}

\textbf{4. QQ-Plot dos Resíduos}

\begin{itemize}
\tightlist
\item
  Objetivo: verificar normalidade.
\item
  Funções/objetos:

  \begin{itemize}
  \tightlist
  \item
    \texttt{resid\ \textless{}-\ resid(modelo)}
  \item
    Base R: \texttt{qqnorm(resid);\ qqline(resid)}
  \item
    Para \texttt{ggplot2}:
    \texttt{qq\ \textless{}-\ qqnorm(resid,\ plot.it=FALSE)} e então
    \texttt{geom\_point()} + \texttt{geom\_abline()}.
  \end{itemize}
\end{itemize}

\textbf{5. Histograma dos Resíduos}

\begin{itemize}
\tightlist
\item
  Objetivo: verificar forma aproximada da distribuição.
\item
  Funções/objetos:

  \begin{itemize}
  \tightlist
  \item
    \texttt{resid\_pad\ \textless{}-\ scale(resid(modelo))} (opcional)
  \end{itemize}
\item
  Em \texttt{ggplot2}:

  \begin{itemize}
  \tightlist
  \item
    \texttt{geom\_histogram(aes(y=after\_stat(density)))}
  \item
    Curva Normal de referência: \texttt{dnorm(x,\ 0,\ 1)} via
    \texttt{geom\_line()} com uma grade \texttt{x}.
  \end{itemize}
\end{itemize}

\textbf{6. Resíduos Estudentizados vs.~Índice da Observação}

\begin{itemize}
\tightlist
\item
  Objetivo: identificar observações discrepantes.
\item
  Funções/objetos:

  \begin{itemize}
  \tightlist
  \item
    \texttt{stud\ \textless{}-\ rstudent(modelo)}
  \item
    \texttt{idx\ \textless{}-\ seq\_along(stud)}
  \end{itemize}
\item
  Em \texttt{ggplot2}: \texttt{geom\_point()} +
  \texttt{geom\_hline(yintercept=c(-2,2),\ ...)}.
\end{itemize}

\textbf{7. Valores Ajustados vs.~Resíduos Estudentizados²}

\begin{itemize}
\tightlist
\item
  Objetivo: investigar heterocedasticidade.
\item
  Funções/objetos:

  \begin{itemize}
  \tightlist
  \item
    \texttt{fitted\ \textless{}-\ fitted(modelo)}
  \item
    \texttt{t2\ \textless{}-\ rstudent(modelo)\^{}2}
  \end{itemize}
\item
  Suavização:

  \begin{itemize}
  \tightlist
  \item
    \texttt{geom\_smooth(method="loess",\ se=FALSE,\ ...)} (substitui o
    LOWESS do Python de forma direta).
  \end{itemize}
\end{itemize}

\textbf{8. Alavancagem vs.~Resíduos Estudentizados (Influence Plot)}

\begin{itemize}
\tightlist
\item
  Objetivo: detectar observações influentes.
\item
  Funções/objetos:

  \begin{itemize}
  \tightlist
  \item
    \texttt{h\ \textless{}-\ hatvalues(modelo)} (alavancagem)
  \item
    \texttt{stud\ \textless{}-\ rstudent(modelo)} (resíduo
    estudentizado)
  \item
    \texttt{ck\ \textless{}-\ cooks.distance(modelo)} (distância de
    Cook)
  \end{itemize}
\item
  Em \texttt{ggplot2}: \texttt{geom\_point(aes(size=ck))} para tornar a
  área proporcional a Cook + linhas de referência (ex.:
  \texttt{geom\_vline()} com regra 2(p+1)/n).
\end{itemize}

\textbf{Resumo didático:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ajuste o modelo com \texttt{lm()}:

  \begin{itemize}
  \tightlist
  \item
    \texttt{modelo\ \textless{}-\ lm(Y\ \textasciitilde{}\ X,\ data\ =\ df)}
  \end{itemize}
\item
  Obtenha resíduos e ajustados:

  \begin{itemize}
  \tightlist
  \item
    \texttt{resid\ \ \ \textless{}-\ resid(modelo)}
  \item
    \texttt{fitted\ \ \textless{}-\ fitted(modelo)}
  \end{itemize}
\item
  Obtenha diagnósticos extras:

  \begin{itemize}
  \tightlist
  \item
    \texttt{stud\ \textless{}-\ rstudent(modelo)}\strut \\
  \item
    \texttt{h\ \ \ \ \textless{}-\ hatvalues(modelo)}\strut \\
  \item
    \texttt{ck\ \ \ \textless{}-\ cooks.distance(modelo)}
  \end{itemize}
\item
  (Opcional) Organize tudo em um único data frame (facilita gráficos):

  \begin{itemize}
  \tightlist
  \item
    \texttt{broom::augment(modelo)} (traz \texttt{.fitted},
    \texttt{.resid}, \texttt{.std.resid}, \texttt{.hat},
    \texttt{.cooksd})
  \end{itemize}
\end{enumerate}

Com esses objetos, é possível montar todos os oito gráficos de resíduos
apresentados neste capítulo.

\chapter{Transformações nas
Variáveis}\label{transformauxe7uxf5es-nas-variuxe1veis}

\section{Motivação}\label{motivauxe7uxe3o}

Quando os \textbf{gráficos de resíduos} revelam padrões sistemáticos,
heteroscedasticidade ou violações de normalidade, é sinal de que a
especificação do modelo pode estar inadequada. Em termos formais, isso
significa que ao menos uma das hipóteses centrais do MRLS ---
linearidade da função média, homoscedasticidade ou normalidade dos erros
--- pode não estar sendo satisfeita (Montgomery, Peck, e Vining (2021);
Kutner et al. (2005)).

A regressão linear simples assume que

\[
E(Y \mid X) = \beta_0 + \beta_1 X
\quad \text{e} \quad
Var(Y \mid X) = \sigma^2,
\]

ou seja, uma \textbf{relação linear na média} e \textbf{variância
constante} condicionalmente a \(X\). Quando os resíduos exibem padrões
como funil (variância crescente), curvaturas sistemáticas ou forte
assimetria, o modelo ajustado não está capturando adequadamente a
estrutura do processo gerador dos dados.

Uma das abordagens mais tradicionais para lidar com tais situações é
aplicar \textbf{transformações matemáticas} às variáveis, de modo a:

\begin{itemize}
\tightlist
\item
  estabilizar a variância;
\item
  aproximar a normalidade dos erros;
\item
  linearizar a relação entre as variáveis;
\item
  tornar a interpretação estatística mais coerente com a estrutura do
  fenômeno estudado.
\end{itemize}

Há duas formas de realizar tais transformações:\\
(a) na variável resposta (\(Y\)) e\\
(b) na variável explicativa (\(X\)).

Note que, ao aplicarmos uma transformação de variáveis, estamos
implicitamente redefinindo o modelo estatístico. Por exemplo, ao
transformar \(Y\) em \(\log(Y)\), deixamos de modelar diretamente
\(E(Y \mid X)\) e passamos a modelar \(E(\log Y \mid X)\). Isso altera a
interpretação dos coeficientes, a distribuição assumida para os erros e,
em certos casos, a própria classe de modelos implícita (Weisberg (2005);
Kutner et al. (2005)).

Portanto, transformações devem ser motivadas por evidência empírica
(diagnóstico de resíduos) e, sempre que possível, por fundamentação
teórica do fenômeno estudado. Aplicá-las de forma automática pode
melhorar métricas numéricas de ajuste, mas comprometer a interpretação
do modelo.

Além disso, a aplicação de uma transformação implica um novo ciclo
completo de análise:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ajustar o modelo transformado.\\
\item
  Reavaliar os resíduos.\\
\item
  Verificar se as hipóteses agora são plausíveis.
\end{enumerate}

Esse processo é inerentemente iterativo.

Quando a transformação envolve apenas mudanças algébricas simples (como
logaritmo ou raiz quadrada), as justificativas formais podem ser
entendidas via propriedades de variância e distribuições conhecidas;
demonstrações mais técnicas dessas propriedades podem ser consultadas no
Apêndice de Demonstrações \{\#demo\}.

Nas seções seguintes, analisaremos separadamente as transformações
aplicadas à variável resposta e à variável explicativa, enfatizando sua
motivação estatística e suas implicações interpretativas.

\section{\texorpdfstring{Transformações na variável resposta
(\(Y\))}{Transformações na variável resposta (Y)}}\label{transformauxe7uxf5es-na-variuxe1vel-resposta-y}

As transformações na variável resposta têm como objetivo principal
modificar a \textbf{estrutura probabilística condicional de \(Y\) dado
\(X\)}. Em muitos contextos aplicados, a violação das hipóteses do MRLS
decorre não da forma funcional da média, mas do comportamento da
variância ou da distribuição dos erros.

Recordando que o MRLS assume \(Var(Y \mid X) = \sigma^2\),qualquer
evidência de que

\[
Var(Y \mid X) \neq \text{constante}
\]

ou que os erros apresentam forte assimetria ou caudas pesadas pode
motivar uma transformação em \(Y\) (Montgomery, Peck, e Vining (2021);
Kutner et al. (2005); Weisberg (2005)).

Para exemplificar, suponha que a variância condicional dependa da média:

\[
Var(Y \mid X) = g\big(E(Y \mid X)\big),
\]

para alguma função \(g(\cdot)\) crescente. Isso ocorre, por exemplo:

\begin{itemize}
\tightlist
\item
  quando a variância é proporcional à média (dados de contagem);
\item
  quando a variância cresce aproximadamente com o quadrado da média
  (dados positivos e assimétricos);
\item
  quando há padrão de funil nos resíduos vs.~ajustados.
\end{itemize}

Nesse cenário, podemos buscar uma transformação \(Y^* = h(Y)\) tal que

\[
Var(Y^* \mid X) \approx \text{constante}.
\]

Esse princípio é conhecido como \textbf{estabilização da variância}, e
sua justificativa formal pode ser obtida por expansão de Taylor de
primeira ordem (ver Apêndice de Demonstrações \{\#demo\}).

É importante destacar que transformar \(Y\) altera simultaneamente: - a
escala da resposta; - a forma da distribuição condicional; - a
interpretação dos coeficientes.

Portanto, não estamos apenas ``melhorando resíduos'', mas redefinindo o
modelo estatístico.

\subsection{Transformação e distribuição
implícita}\label{transformauxe7uxe3o-e-distribuiuxe7uxe3o-impluxedcita}

Se ajustamos o modelo

\[
Y_i^* = h(Y_i) = \beta_0^* + \beta_1^* X_i + \varepsilon_i,
\quad \varepsilon_i \sim N(0,\sigma^2),
\]

então estamos assumindo normalidade \textbf{na escala transformada}.

Isso implica que, na escala original, \(Y\) possui distribuição derivada
da transformação inversa. Por exemplo:

\begin{itemize}
\tightlist
\item
  se \(h(Y)=\log Y\), então \(Y\) é log-normal condicionalmente a \(X\);
\item
  se \(h(Y)=\sqrt Y\), a distribuição resultante não é normal na escala
  original, mas a variância pode se tornar aproximadamente constante;
\item
  se \(h(Y)=1/Y\), a estrutura média passa a ser modelada em termos do
  inverso da resposta.
\end{itemize}

Essa mudança tem implicações importantes para:

\begin{itemize}
\tightlist
\item
  interpretação de coeficientes;
\item
  construção de intervalos de confiança;
\item
  previsão na escala original.
\end{itemize}

Ao transformar \(Y\), a hipótese de normalidade passa a ser avaliada na
nova escala. Isso significa que testes \(t\), testes \(F\) e intervalos
de confiança são válidos sob a suposição de normalidade dos erros na
escala transformada, não necessariamente na escala original (Kutner et
al. (2005)).

Nas subseções seguintes, discutiremos transformações específicas, a
saber, logaritmo, raiz quadrada, inverso e potência.

\subsection{\texorpdfstring{Logaritmo
(\(Y^* = \log Y\))}{Logaritmo (Y\^{}* = \textbackslash log Y)}}\label{logaritmo-y-log-y}

A transformação logarítmica é uma das mais utilizadas em regressão,
especialmente quando \(Y\) é \textbf{positiva} e apresenta
\textbf{assimetria à direita} ou variância crescente com o nível médio.

Se ajustamos o modelo

\[
Y_i^* = \log(Y_i) = \beta_0^* + \beta_1^* X_i + \varepsilon_i,
\quad \varepsilon_i \sim N(0, \sigma^2),
\]

estamos assumindo normalidade dos erros \textbf{na escala logarítmica}.
Isso implica que, condicionalmente a \(X_i\), a variável original
\(Y_i\) segue uma \textbf{distribuição log-normal} (Kutner et al.
(2005); Weisberg (2005)).

Mais precisamente, se

\[
\log(Y_i) \mid X_i \sim N(\mu_i, \sigma^2),
\]

então

\[
Y_i \mid X_i \sim \text{LogNormal}(\mu_i, \sigma^2).
\]

A transformação logarítmica converte relações multiplicativas em
aditivas. Se, na escala original,

\[
Y = \alpha \exp(\beta X) \cdot U,
\]

onde \(U\) é um termo multiplicativo de erro, então

\[
\log Y = \log \alpha + \beta X + \log U,
\]

ou seja, o modelo passa a ter estrutura linear com erro aditivo na
escala transformada.

Isso significa que o logaritmo é particularmente adequado quando:

\begin{itemize}
\tightlist
\item
  a variabilidade é proporcional ao nível médio;
\item
  o crescimento é aproximadamente exponencial;
\item
  o erro atua de forma \textbf{multiplicativa} na escala original.
\end{itemize}

No modelo

\[
E[\log(Y) \mid X] = \beta_0^* + \beta_1^* X,
\]

o coeficiente \(\beta_1^*\) representa a variação \textbf{aditiva no
logaritmo da resposta} para cada unidade adicional em \(X\).

Na escala original,

\[
E(Y\mid X) = \exp(\beta_0^*) \cdot \exp(\beta_1^* X).
\]

Assim, um aumento de 1 unidade em \(X\) multiplica o valor esperado de
\(Y\) por \(\exp(\beta_1^*)\).

Para valores pequenos de \(\beta_1^*\), podemos usar a aproximação

\[
\exp(\beta_1^*) \approx 1 + \beta_1^*,
\]

o que leva à interpretação aproximada de que \(\beta_1^* \times 100\%\)
representa uma variação percentual em \(Y\) para cada unidade de \(X\).

Essa interpretação percentual é uma aproximação válida apenas quando
\(|\beta_1^*|\) é pequeno. O efeito exato é multiplicativo e deve ser
interpretado via \(\exp(\beta_1^*)\).

\subsubsection{Variância e
estabilização}\label{variuxe2ncia-e-estabilizauxe7uxe3o}

Se a variância cresce aproximadamente de forma proporcional ao quadrado
da média,

\[
Var(Y \mid X) \propto [E(Y \mid X)]^2,
\]

então a transformação logarítmica tende a produzir variância
aproximadamente constante na escala transformada.

\textbf{Advertência sobre retransformação}

Ao obter previsões na escala original, não é correto simplesmente
calcular

\[
\widehat{Y} = \exp(\widehat{\log Y}),
\]

pois, para variável log-normal,

\[
E(Y \mid X) = \exp(\mu + \tfrac{1}{2}\sigma^2),
\]

e não apenas \(\exp(\mu)\) (Casella e Berger (2002)). Ignorar esse termo
pode introduzir \textbf{viés de retransformação}.

\textbf{Quando usar}

\begin{itemize}
\tightlist
\item
  Resíduos vs.~ajustados exibem padrão de funil.
\item
  Histograma dos resíduos apresenta assimetria à direita.
\item
  QQ-plot mostra caudas pesadas superiores.
\item
  Relação parece exponencial na escala original.
\end{itemize}

\textbf{Gráficos sugeridos para verificação:}

\begin{itemize}
\item
  Resíduos vs.~\(\hat Y\) (o que verificar: formato de funil; resultado
  esperado após transformação: dispersão homogênea).~
\item
  QQ-plot dos resíduos (o que verificar: caudas pesadas; resultado
  esperado após transformação: alinhamento mais próximo à reta).~
\item
  Histograma dos resíduos (o que verificar: assimetria; aresultado
  esperado após transformação: formato mais próximo da normal).
\end{itemize}

\subsection{\texorpdfstring{Raiz quadrada
(\(Y^* = \sqrt{Y}\))}{Raiz quadrada (Y\^{}* = \textbackslash sqrt\{Y\})}}\label{raiz-quadrada-y-sqrty}

A transformação pela raiz quadrada é frequentemente utilizada quando
\(Y\) representa \textbf{contagens} ou frequências.

Por exemplo, se \(Y_i \sim \text{Poisson}(\mu_i)\), então

\[
E(Y_i) = \mu_i,
\quad
Var(Y_i) = \mu_i,
\]

ou seja, a variância é proporcional à média.

Aplicando a transformação

\[
Y_i^* = \sqrt{Y_i},
\]

e utilizando aproximação por expansão de Taylor, obtém-se

\[
Var(Y_i^*) \approx \frac{1}{4},
\]

que é aproximadamente constante e independe de \(\mu_i\) (ver Apêndice
de Demonstrações \{\#demo\}).

Note que a raiz quadrada reduz a assimetria típica de distribuições de
contagem e estabiliza a variância quando esta é proporcional à média.

O modelo passa então a explicar a variação na \textbf{raiz da resposta},
não na resposta original. Isso altera a escala e deve ser explicitado ao
interpretar coeficientes.

\textbf{Quando usar}

\begin{itemize}
\tightlist
\item
  Dados de contagem.
\item
  Variância aproximadamente proporcional à média.
\item
  Histograma com cauda longa à direita.
\end{itemize}

\textbf{Gráficos sugeridos para verificação:}

\begin{itemize}
\item
  Histograma dos resíduos (o que verificar: cauda longa; resultado
  esperado após transformação: simetria).~
\item
  Resíduos vs.~\(\hat Y\) (o que verificar: variância crescente;
  resultado esperado após transformação: dispersão estável).
\end{itemize}

\subsection{\texorpdfstring{Inverso
(\(Y^* = 1/Y\))}{Inverso (Y\^{}* = 1/Y)}}\label{inverso-y-1y}

A transformação inversa é útil quando a relação entre \(Y\) e \(X\)
apresenta comportamento do tipo \textbf{decaimento rápido em direção a
um limite}.

Se a relação original for não linear, por exemplo,

\[
Y = \frac{1}{\alpha + \beta X},
\]

então

\[
\frac{1}{Y} = \alpha + \beta X,
\]

que é linear.

Essa transformação é comum em fenômenos físicos e biológicos nos quais a
resposta diminui rapidamente e depois se estabiliza.

O modelo ajustado explica o comportamento do \textbf{inverso da
resposta}. A interpretação substantiva deve ser feita com cuidado, pois
aumentos em \(X\) passam a produzir efeitos lineares sobre \(1/Y\), e
não diretamente sobre \(Y\).

\textbf{Quando usar}

Modelos onde a resposta diminui de forma não linear (ex.: tempo de
reação decrescendo com aumento de dose).

\textbf{Gráficos sugeridos para verificação:}

\begin{itemize}
\item
  Resíduos vs.~\(X\) (o que verificar: padrão curvilíneo; resultado
  esperado após transformação: dispersão aleatória).~
\item
  QQ-plot dos resíduos (o que verificar: forte desvio; resultado
  esperado após transformação: mais alinhado).
\end{itemize}

\subsection{\texorpdfstring{Quadrado
(\(Y^* = Y^2\))}{Quadrado (Y\^{}* = Y\^{}2)}}\label{quadrado-y-y2}

A transformação quadrática pode ser útil quando a variabilidade é maior
em valores pequenos de \(Y\) ou quando a relação é convexa na escala
original.

Ao elevar \(Y\) ao quadrado, ampliamos diferenças em níveis mais altos
da resposta, o que pode reduzir padrões estruturais nos resíduos.

Essa transformação modifica significativamente a escala e deve ser
utilizada apenas quando há justificativa empírica clara nos gráficos de
resíduos.

\textbf{Gráficos sugeridos para verificação:}

\begin{itemize}
\item
  Resíduos vs.~\(\hat Y\) (o que verificar: padrão em arco; resultado
  esperado após transformação: dispersão homogênea).~
\item
  Histograma dos resíduos (o que verificar: concentração em torno de 0;
  resultado esperado após transformação: mais espalhado).
\end{itemize}

\section{\texorpdfstring{Transformações na variável explicativa
(\(X\))}{Transformações na variável explicativa (X)}}\label{transformauxe7uxf5es-na-variuxe1vel-explicativa-x}

As transformações na variável explicativa têm como objetivo principal
\textbf{linearizar a relação entre \(X\) e a média condicional de
\(Y\)}, preservando, quando possível, a estrutura de variância constante
dos erros.

Recordando que o MRLS assume \(E(Y \mid X) = \beta_0 + \beta_1 X\),
portanto, qualquer evidência de que a relação média entre \(Y\) e \(X\)
não é linear, por exemplo, presença de curvatura sistemática no gráfico
de resíduos vs.~\(X\), sugere que a especificação funcional pode estar
inadequada (Montgomery, Peck, e Vining (2021); Kutner et al. (2005)).

Nesse caso, buscamos uma transformação \(X^* = h(X)\) tal que

\[
E(Y \mid X^*) = \beta_0 + \beta_1 X^*
\]

represente melhor a estrutura média do fenômeno.

Transformar \(X\) significa alterar a \textbf{forma funcional da
regressão} em relação à variável \(X\), mas não a natureza
probabilística da variável resposta. Diferentemente das transformações
em \(Y\), aqui a distribuição condicional de \(Y\) não é redefinida;
apenas modificamos a forma como a média depende da variável explicativa.

Portanto:

\begin{itemize}
\tightlist
\item
  Transformações em \(Y\) → alteram a escala da resposta e a estrutura
  probabilística.\\
\item
  Transformações em \(X\) → alteram a forma funcional da média em
  relação à variável \(X\).
\end{itemize}

\subsection{Motivação estatística}\label{motivauxe7uxe3o-estatuxedstica}

Se o gráfico de resíduos vs.~\(X\) exibe:

\begin{itemize}
\tightlist
\item
  padrão em arco (concavidade ou convexidade),
\item
  forma em S,
\item
  tendência sistemática crescente ou decrescente,
\end{itemize}

então o modelo linear \(\beta_0 + \beta_1 X\) não está capturando
adequadamente a relação entre as variáveis. A solução é buscar uma
transformação \(h(X)\) tal que a nova relação seja aproximadamente
linear.

Essa estratégia é coerente com o princípio geral de modelagem
estatística: \textbf{especificar corretamente a função média antes de
avaliar a variância dos erros} (Weisberg (2005)).

\subsection{\texorpdfstring{Logaritmo
(\(X^* = \log X\))}{Logaritmo (X\^{}* = \textbackslash log X)}}\label{logaritmo-x-log-x}

A transformação logarítmica em \(X\) é apropriada quando a relação entre
\(Y\) e \(X\) apresenta comportamento de crescimento proporcional ou lei
de potência.

Suponha que a relação verdadeira seja

\[
Y = \alpha X^\beta + \varepsilon.
\]

Tomando logaritmo em ambos os lados (desconsiderando momentaneamente o
erro),

\[
\log Y = \log \alpha + \beta \log X.
\]

Se também transformarmos \(Y\), obtemos o chamado modelo
\textbf{log-log}, amplamente utilizado em econometria.

No entanto, mesmo sem transformar \(Y\), pode ser apropriado modelar

\[
Y = \beta_0 + \beta_1 \log X + \varepsilon,
\]

quando o efeito de \(X\) diminui à medida que \(X\) cresce.

Observe que no modelo

\[
Y = \beta_0 + \beta_1 \log X,
\]

o coeficiente \(\beta_1\) representa a variação média em \(Y\) associada
a uma variação proporcional em \(X\). Mais especificamente, um aumento
percentual em \(X\) está associado a uma variação aproximadamente linear
em \(Y\).

Portanto, a transformação logarítmica em \(X\) não implica mudança na
distribuição de \(Y\).

\textbf{Gráfico sugerido:}

\begin{itemize}
\tightlist
\item
  Resíduos vs.~\(X\) (o que verificar: curva sistemática; resultado
  esperado após transformação: dispersão aleatória).
\end{itemize}

\subsection{\texorpdfstring{Inverso
(\(X^* = 1/X\))}{Inverso (X\^{}* = 1/X)}}\label{inverso-x-1x}

A transformação inversa é adequada quando o efeito de \(X\) é muito
forte para valores pequenos e diminui rapidamente conforme \(X\)
aumenta.

Exemplo típico:

\begin{itemize}
\item
  fenômenos de aprendizado;
\item
  processos de saturação;
\item
  respostas que se estabilizam para grandes valores de \(X\).
\end{itemize}

Se a relação verdadeira for aproximadamente

\[
Y = \alpha + \frac{\beta}{X},
\]

então

\[
Y = \alpha + \beta X^*
\]

com \(X^* = 1/X\).

A transformação inversa captura comportamentos de \textbf{retorno
decrescente} ou aproximação assintótica.

\textbf{Gráfico sugerido:}

\begin{itemize}
\tightlist
\item
  Resíduos vs.~\(X\) (o que verificar: curva decrescente; resultado
  esperado após transformação: sem padrão).
\end{itemize}

\subsection{\texorpdfstring{Potências e termos polinomiais (\(X^2\),
\(X^3\),
\ldots)}{Potências e termos polinomiais (X\^{}2, X\^{}3, \ldots)}}\label{potuxeancias-e-termos-polinomiais-x2-x3}

Quando a relação apresenta curvatura suave, uma alternativa é expandir o
modelo com termos polinomiais:

\[
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \cdots + \varepsilon.
\]

Esse modelo continua sendo linear nos parâmetros (condição essencial do
MRLS), embora não seja linear em \(X\).

Temos então que:

\begin{itemize}
\item
  \(\beta_2 > 0\) → concavidade para cima (convexidade).
\item
  \(\beta_2 < 0\) → concavidade para baixo.
\end{itemize}

Apesar de introduzir não linearidade em \(X\), o modelo permanece linear
em relação aos parâmetros \(\beta_j\). Isso garante que:

\begin{itemize}
\item
  os estimadores de MQO continuam válidos;
\item
  as propriedades inferenciais permanecem as mesmas.
\end{itemize}

Essa distinção entre \textbf{linearidade nos parâmetros} e
\textbf{linearidade na variável explicativa} é conceitualmente central
em regressão linear.

\textbf{Gráfico sugerido:}

\begin{itemize}
\tightlist
\item
  Resíduos vs.~\(X\) (o que verificar: arco ou S-curva; resultado
  esperado após transformação: resíduos aleatórios).
\end{itemize}

\subsection{\texorpdfstring{Transformar \(X\) ou adicionar
polinômios?}{Transformar X ou adicionar polinômios?}}\label{transformar-x-ou-adicionar-polinuxf4mios}

Há duas estratégias principais para lidar com curvaturas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Transformar \(X\) (por exemplo, \(\log X\), \(1/X\)).
\item
  Incluir termos polinomiais (\(X^2\), \(X^3\)).
\end{enumerate}

A escolha depende de:

\begin{itemize}
\item
  coerência teórica;
\item
  interpretação desejada;
\item
  estabilidade numérica do ajuste.
\end{itemize}

Transformações simples costumam ter interpretação mais direta;
polinômios oferecem maior flexibilidade, mas podem introduzir
instabilidade em extrapolações.

Logo, as transformações na variável explicativa:

\begin{itemize}
\tightlist
\item
  buscam linearizar a função média;
\item
  não alteram a estrutura probabilística da resposta;
\item
  preservam a estrutura básica do MRLS;
\item
  exigem reavaliação completa dos resíduos após o ajuste.
\end{itemize}

\section{Guia prático: escolhendo a
transformação}\label{guia-pruxe1tico-escolhendo-a-transformauxe7uxe3o}

Após compreender as motivações teóricas para transformar \(Y\) ou \(X\),
surge a questão prática: \textbf{como decidir qual transformação
aplicar?}

A escolha não deve ser arbitrária nem baseada apenas em melhora numérica
de \(R^2\). O critério central é a adequação às hipóteses do modelo e à
estrutura substantiva do fenômeno (Montgomery, Peck, e Vining (2021);
Kutner et al. (2005)).

\textbf{Etapa 1 --- Diagnóstico inicial}

Antes de qualquer transformação, ajusta-se o modelo na escala original:

\[
Y_i = \beta_0 + \beta_1 X_i + e_i.
\]

Em seguida, analisam-se os resíduos:

\begin{itemize}
\tightlist
\item
  Resíduos vs.~ajustados → verificar homoscedasticidade.
\item
  Resíduos vs.~\(X\) → verificar linearidade.
\item
  QQ-plot → verificar normalidade.
\item
  Histograma → avaliar assimetria e caudas.
\end{itemize}

A transformação deve responder a um padrão empírico específico observado
nos resíduos. Se não há padrão sistemático, não há justificativa para
transformar.

\textbf{Etapa 2 --- Identificação do padrão}

Alguns padrões típicos e suas possíveis soluções:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3103}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2759}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4138}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Padrão observado
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Possível causa
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Transformação sugerida
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Funil (variância cresce com média) & \(Var(Y \mid X)\) proporcional à
média ou ao quadrado da média & \(\log(Y)\) ou \(\sqrt{Y}\) \\
Curvatura em arco & Relação não linear entre \(Y\) e \(X\) & \(X^2\) ou
\(\log(X)\) \\
Assimetria à direita & Distribuição assimétrica positiva &
\(\log(Y)\) \\
Efeito forte em valores pequenos de \(X\) & Retorno decrescente &
\(1/X\) \\
\end{longtable}

Cada transformação responde a um mecanismo estrutural distinto. Não se
trata de ``corrigir o gráfico'', mas de modelar melhor o processo
subjacente.

\textbf{Etapa 3 --- Ajuste do modelo transformado}

Após escolher a transformação, ajusta-se o novo modelo, por exemplo:

\[
\log(Y_i) = \beta_0^* + \beta_1^* X_i + \varepsilon_i,
\]

ou

\[
Y_i = \beta_0 + \beta_1 \log(X_i) + \varepsilon_i.
\]

Nesse momento, todo o ciclo de verificação deve ser repetido:

\begin{itemize}
\tightlist
\item
  análise gráfica dos resíduos;
\item
  testes formais (quando apropriado);
\item
  avaliação da coerência interpretativa.
\end{itemize}

\textbf{Etapa 4 --- Avaliação comparativa}

A comparação entre modelos deve considerar:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Qualidade dos resíduos (homoscedasticidade, normalidade).
\item
  Estabilidade dos coeficientes.
\item
  Interpretação substantiva.
\item
  Intervalos de confiança e precisão inferencial.
\end{enumerate}

Um modelo com menor \(R^2\) pode ser preferível se satisfaz melhor as
hipóteses do MRLS e apresenta resíduos estruturalmente adequados.

\section{Exemplo ilustrativo: antes e depois da
transformação}\label{exemplo-ilustrativo-antes-e-depois-da-transformauxe7uxe3o}

Para consolidar as ideias discutidas até aqui, apresentamos um exemplo
ilustrativo que mostra, de forma integrada, a \textbf{necessidade
estatística} e a \textbf{eficácia inferencial} de uma transformação na
variável resposta.

\subsection{Cenário e especificação dos
modelos}\label{cenuxe1rio-e-especificauxe7uxe3o-dos-modelos}

Suponha que estejamos interessados em modelar a relação entre
\textbf{horas de prática de estudo (\(X\))} e a \textbf{nota obtida em
uma prova (\(Y\))}. Intuitivamente, espera-se que estudantes que dedicam
mais horas de estudo tendam a alcançar notas maiores. Contudo, é
plausível que:

\begin{itemize}
\tightlist
\item
  o ganho médio de desempenho seja aproximadamente exponencial;
\item
  a variabilidade das notas aumente à medida que \(X\) aumenta.
\end{itemize}

Essa estrutura implica que a variância condicional pode depender do
nível médio da resposta, violando a hipótese de homoscedasticidade do
MRLS.

Consideremos dois modelos concorrentes.

\textbf{Modelo A (sem transformação):}

\[
Y_i = \beta_0 + \beta_1 X_i + e_i.
\]

Esse modelo assume:

\[
E(Y_i \mid X_i) = \beta_0 + \beta_1 X_i,
\qquad
Var(Y_i \mid X_i) = \sigma^2.
\]

\textbf{Modelo B (com transformação logarítmica):}

\[
Y_i^* = \log(Y_i) = \beta_0^* + \beta_1^* X_i + \varepsilon_i,
\qquad
\varepsilon_i \sim \mathcal{N}(0,\sigma^2).
\]

Nesse caso, assumimos normalidade e homoscedasticidade \textbf{na escala
logarítmica}. Implicitamente, isso significa que, na escala original,

\[
Y_i \mid X_i \sim \text{LogNormal}(\mu_i, \sigma^2),
\]

o que altera a estrutura probabilística do modelo.

\subsection{Geração do conjunto de dados e análise
descritiva}\label{gerauxe7uxe3o-do-conjunto-de-dados-e-anuxe1lise-descritiva}

Para ilustrar essa situação, considere que a relação verdadeira é linear
na escala logarítmica:

\[
\log(Y_i) = 1.0 + 0.20 X_i + u_i,
\quad
u_i \sim N(0,\sigma^2).
\]

A partir dessa equação, obtemos

\[
Y_i = \exp(1.0 + 0.20 X_i + u_i),
\]

o que gera:

\begin{itemize}
\tightlist
\item
  crescimento exponencial na média;
\item
  variância crescente com \(X\);
\item
  assimetria à direita na escala original.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2025}\NormalTok{)}

\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{120}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n)}

\CommentTok{\# Relação linear na escala log(Y)}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \FloatTok{0.6}
\NormalTok{logY  }\OtherTok{\textless{}{-}} \FloatTok{1.0} \SpecialCharTok{+} \FloatTok{0.20}\SpecialCharTok{*}\NormalTok{X }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, sigma)}
\NormalTok{Y     }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(logY)}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{X =}\NormalTok{ X,}
  \AttributeTok{Y =}\NormalTok{ Y,}
  \AttributeTok{logY =} \FunctionTok{log}\NormalTok{(Y)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Estatísticas descritivas}

Devem ser apresentadas:

\begin{itemize}
\tightlist
\item
  primeiras linhas da base;
\item
  estatísticas resumidas para \(X\), \(Y\) e \(\log(Y)\).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
       X     Y  logY
   <dbl> <dbl> <dbl>
1 0       3.95 1.37 
2 0.0840  2.82 1.04 
3 0.168   4.47 1.50 
4 0.252   6.13 1.81 
5 0.336   3.63 1.29 
6 0.420   2.68 0.986
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       X              Y               logY       
 Min.   : 0.0   Min.   : 1.141   Min.   :0.1318  
 1st Qu.: 2.5   1st Qu.: 4.384   1st Qu.:1.4779  
 Median : 5.0   Median : 6.833   Median :1.9217  
 Mean   : 5.0   Mean   : 9.724   Mean   :1.9833  
 3rd Qu.: 7.5   3rd Qu.:12.803   3rd Qu.:2.5497  
 Max.   :10.0   Max.   :40.396   Max.   :3.6987  
\end{verbatim}

\textbf{Ajuste dos modelos}

Ajustamos ambos os modelos via MQO:

\begin{itemize}
\tightlist
\item
  Modelo A: \(Y \sim X\)\\
\item
  Modelo B: \(\log(Y) \sim X\)
\end{itemize}

Comparar:

\begin{itemize}
\tightlist
\item
  estimativas dos coeficientes;
\item
  erros-padrão;
\item
  \(R^2\);
\item
  estatísticas \(t\) e \(F\).
\end{itemize}

Note que os \(R^2\) não são diretamente comparáveis entre escalas
distintas, pois medem proporções de variabilidade em espaços diferentes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mA }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ df)}
\NormalTok{mB }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(logY }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ df)}

\FunctionTok{cat}\NormalTok{(}\StringTok{"=== MODELO A (Y \textasciitilde{} X) ===}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
=== MODELO A (Y ~ X) ===
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{summary}\NormalTok{(mA))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Y ~ X, data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.8050  -4.1989  -0.8159   1.8826  24.5691 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   2.0801     1.2119   1.716   0.0887 .  
X             1.5288     0.2095   7.298 3.68e-11 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.68 on 118 degrees of freedom
Multiple R-squared:  0.311, Adjusted R-squared:  0.3052 
F-statistic: 53.27 on 1 and 118 DF,  p-value: 3.677e-11
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{=== MODELO B (logY \textasciitilde{} X) ===}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

=== MODELO B (logY ~ X) ===
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{summary}\NormalTok{(mB))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = logY ~ X, data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.43792 -0.41121 -0.01437  0.36590  1.62093 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.15379    0.10872  10.612  < 2e-16 ***
X            0.16590    0.01879   8.828 1.16e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5992 on 118 degrees of freedom
Multiple R-squared:  0.3978,    Adjusted R-squared:  0.3927 
F-statistic: 77.94 on 1 and 118 DF,  p-value: 1.164e-14
\end{verbatim}

\textbf{ANOVA}

Apresentar as tabelas ANOVA para ambos os modelos.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{=== ANOVA — MODELO A ===}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

=== ANOVA — MODELO A ===
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{anova}\NormalTok{(mA))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: Y
           Df Sum Sq Mean Sq F value    Pr(>F)    
X           1 2376.6 2376.59  53.266 3.677e-11 ***
Residuals 118 5264.9   44.62                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{=== ANOVA — MODELO B ===}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

=== ANOVA — MODELO B ===
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{anova}\NormalTok{(mB))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: logY
           Df Sum Sq Mean Sq F value    Pr(>F)    
X           1 27.984 27.9837  77.938 1.164e-14 ***
Residuals 118 42.368  0.3591                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\subsection{Diagnóstico gráfico: antes e
depois}\label{diagnuxf3stico-gruxe1fico-antes-e-depois}

O contraste entre os modelos deve ser avaliado por meio dos gráficos
diagnósticos.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Padronização visual para TODOS os gráficos do exemplo}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(tidyr)}

\NormalTok{base\_size }\OtherTok{\textless{}{-}} \DecValTok{12}

\NormalTok{theme\_diag }\OtherTok{\textless{}{-}} \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =}\NormalTok{ base\_size) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{panel.grid.minor =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{panel.grid.major.x =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{strip.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{face =} \StringTok{"bold"}\NormalTok{),}
    \AttributeTok{strip.background =} \FunctionTok{element\_rect}\NormalTok{(}\AttributeTok{fill =} \StringTok{"grey95"}\NormalTok{, }\AttributeTok{color =} \ConstantTok{NA}\NormalTok{),}
    \AttributeTok{plot.title.position =} \StringTok{"plot"}\NormalTok{,}
    \AttributeTok{plot.caption.position =} \StringTok{"plot"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\subsubsection{Dispersão da resposta
vs.~explicativa}\label{dispersuxe3o-da-resposta-vs.-explicativa}

\begin{itemize}
\tightlist
\item
  Modelo A: espera-se tendência crescente com dispersão aumentando
  (funil).
\item
  Modelo B: dispersão aproximadamente constante e relação linear clara
  na escala transformada.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_plot }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(X, Y, logY) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(Y, logY), }\AttributeTok{names\_to =} \StringTok{"resp"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"valor"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{resp =} \FunctionTok{ifelse}\NormalTok{(resp }\SpecialCharTok{==} \StringTok{"Y"}\NormalTok{,}
                  \StringTok{"Modelo A: escala original (Y)"}\NormalTok{,}
                  \StringTok{"Modelo B: escala log (log(Y))"}\NormalTok{)}
\NormalTok{  )}

\FunctionTok{ggplot}\NormalTok{(df\_plot, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X, }\AttributeTok{y =}\NormalTok{ valor)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.9}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{resp, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Horas de estudo (X)"}\NormalTok{,}
    \AttributeTok{y =} \ConstantTok{NULL}
\NormalTok{  ) }\SpecialCharTok{+}
\NormalTok{  theme\_diag}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{mrls_transf_files/figure-pdf/unnamed-chunk-6-1.pdf}}

\subsubsection{Resíduos vs.~valores
ajustados}\label{resuxedduos-vs.-valores-ajustados}

\begin{itemize}
\tightlist
\item
  Modelo A: padrão de funil e possível curvatura.
\item
  Modelo B: resíduos aleatórios em torno de zero.
\end{itemize}

A ausência de estrutura no segundo caso indica que

\[
Var(\varepsilon_i \mid X_i) \approx \sigma^2.
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resA }\OtherTok{\textless{}{-}} \FunctionTok{resid}\NormalTok{(mA); fitA }\OtherTok{\textless{}{-}} \FunctionTok{fitted}\NormalTok{(mA)}
\NormalTok{resB }\OtherTok{\textless{}{-}} \FunctionTok{resid}\NormalTok{(mB); fitB }\OtherTok{\textless{}{-}} \FunctionTok{fitted}\NormalTok{(mB)}

\NormalTok{dados }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{fit =}\NormalTok{ fitA, }\AttributeTok{res =}\NormalTok{ resA, }\AttributeTok{painel =} \StringTok{"Modelo A: escala original (Y)"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{fit =}\NormalTok{ fitB, }\AttributeTok{res =}\NormalTok{ resB, }\AttributeTok{painel =} \StringTok{"Modelo B: escala log (log(Y))"}\NormalTok{)}
\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(dados, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fit, }\AttributeTok{y =}\NormalTok{ res)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.85}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{painel, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free\_x"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Valores ajustados"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Resíduo"}
\NormalTok{  ) }\SpecialCharTok{+}
\NormalTok{  theme\_diag}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{mrls_transf_files/figure-pdf/unnamed-chunk-7-1.pdf}}

\subsubsection{Resíduos estudentizados
vs.~ajustados}\label{resuxedduos-estudentizados-vs.-ajustados}

\begin{itemize}
\tightlist
\item
  Modelo A: maior número de pontos fora do intervalo \([-2,2]\).
\item
  Modelo B: poucos pontos extremos, ausência de padrão.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{studA }\OtherTok{\textless{}{-}} \FunctionTok{rstudent}\NormalTok{(mA)}
\NormalTok{studB }\OtherTok{\textless{}{-}} \FunctionTok{rstudent}\NormalTok{(mB)}

\NormalTok{dados }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{fit =}\NormalTok{ fitA, }\AttributeTok{stud =}\NormalTok{ studA, }\AttributeTok{painel =} \StringTok{"Modelo A: escala original (Y)"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{fit =}\NormalTok{ fitB, }\AttributeTok{stud =}\NormalTok{ studB, }\AttributeTok{painel =} \StringTok{"Modelo B: escala log (log(Y))"}\NormalTok{)}
\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(dados, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fit, }\AttributeTok{y =}\NormalTok{ stud)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.85}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.7}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{painel, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free\_x"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Valores ajustados"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Resíduo estudentizado (t*)"}
\NormalTok{  ) }\SpecialCharTok{+}
\NormalTok{  theme\_diag}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{mrls_transf_files/figure-pdf/unnamed-chunk-8-1.pdf}}

\subsubsection{QQ-plot}\label{qq-plot}

\begin{itemize}
\tightlist
\item
  Modelo A: desvios sistemáticos nas caudas.
\item
  Modelo B: alinhamento próximo à reta de 45°.
\end{itemize}

Isso sugere que

\[
\varepsilon_i \sim N(0,\sigma^2)
\]

é plausível apenas na escala logarítmica.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qq\_df }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(r, painel)\{}
\NormalTok{  q }\OtherTok{\textless{}{-}} \FunctionTok{qqnorm}\NormalTok{(r, }\AttributeTok{plot.it =} \ConstantTok{FALSE}\NormalTok{)}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{theo =}\NormalTok{ q}\SpecialCharTok{$}\NormalTok{x, }\AttributeTok{samp =}\NormalTok{ q}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{painel =}\NormalTok{ painel)}
\NormalTok{\}}

\NormalTok{dados }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{qq\_df}\NormalTok{(resA, }\StringTok{"Modelo A: QQ{-}plot dos resíduos (Y)"}\NormalTok{),}
  \FunctionTok{qq\_df}\NormalTok{(resB, }\StringTok{"Modelo B: QQ{-}plot dos resíduos (log(Y))"}\NormalTok{)}
\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(dados, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ theo, }\AttributeTok{y =}\NormalTok{ samp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.85}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{intercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{slope =} \DecValTok{1}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.7}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{painel, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Quantis teóricos (Normal)"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Quantis amostrais (resíduos)"}
\NormalTok{  ) }\SpecialCharTok{+}
\NormalTok{  theme\_diag}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{mrls_transf_files/figure-pdf/unnamed-chunk-9-1.pdf}}

\subsubsection{Histograma}\label{histograma}

\begin{itemize}
\tightlist
\item
  Modelo A: assimetria à direita e caudas pesadas.
\item
  Modelo B: simetria aproximadamente normal.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dados }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{r =}\NormalTok{ resA, }\AttributeTok{painel =} \StringTok{"Modelo A: resíduos (Y)"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{r =}\NormalTok{ resB, }\AttributeTok{painel =} \StringTok{"Modelo B: resíduos (log(Y))"}\NormalTok{)}
\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(dados, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =} \FunctionTok{after\_stat}\NormalTok{(density)),}
                 \AttributeTok{bins =} \DecValTok{14}\NormalTok{,}
                 \AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{,}
                 \AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{painel, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Resíduo"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Densidade"}
\NormalTok{  ) }\SpecialCharTok{+}
\NormalTok{  theme\_diag}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{mrls_transf_files/figure-pdf/unnamed-chunk-10-1.pdf}}

\subsubsection{\texorpdfstring{Resíduos
vs.~\(X\)}{Resíduos vs.~X}}\label{resuxedduos-vs.-x}

\begin{itemize}
\tightlist
\item
  Modelo A: indícios de curvatura e variância crescente.
\item
  Modelo B: dispersão uniforme.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dados }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{X =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{X, }\AttributeTok{res =}\NormalTok{ resA, }\AttributeTok{painel =} \StringTok{"Modelo A: resíduos vs X (Y)"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{X =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{X, }\AttributeTok{res =}\NormalTok{ resB, }\AttributeTok{painel =} \StringTok{"Modelo B: resíduos vs X (log(Y))"}\NormalTok{)}
\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(dados, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X, }\AttributeTok{y =}\NormalTok{ res)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.85}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{painel, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Horas de estudo (X)"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Resíduo"}
\NormalTok{  ) }\SpecialCharTok{+}
\NormalTok{  theme\_diag}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{mrls_transf_files/figure-pdf/unnamed-chunk-11-1.pdf}}

\subsection{Comparação numérica e
conclusão}\label{comparauxe7uxe3o-numuxe9rica-e-conclusuxe3o}

\textbf{Modelo A (escala original)}

\begin{itemize}
\tightlist
\item
  \(R^2\) aparentemente satisfatório.
\item
  Violação da homoscedasticidade.
\item
  Desvios de normalidade.
\item
  Inferência potencialmente comprometida.
\end{itemize}

\textbf{Modelo B (escala logarítmica)}

\begin{itemize}
\tightlist
\item
  Melhor adequação estrutural dos resíduos.
\item
  Testes \(t\) e \(F\) mais confiáveis.
\item
  Intervalos de confiança coerentes com as hipóteses do modelo.
\end{itemize}

Um modelo pode apresentar bom \(R^2\) e, ainda assim, violar hipóteses
fundamentais. A validade da inferência depende da adequação dos
resíduos, não apenas do poder explicativo.

\subsubsection{Interpretação}\label{interpretauxe7uxe3o}

No Modelo B,

\[
\log(Y) = \beta_0^* + \beta_1^* X.
\]

Logo,

\[
Y = \exp(\beta_0^*) \exp(\beta_1^* X).
\]

O efeito de \(X\) é \textbf{multiplicativo}:

\[
\exp(\beta_1^*)
\]

representa o fator pelo qual \(Y\) é multiplicado para cada unidade
adicional em \(X\).

Entretanto, para previsões na escala original,

\[
E(Y \mid X) = \exp\left(\mu + \tfrac{1}{2}\sigma^2\right),
\]

e não simplesmente \(\exp(\mu)\) (Casella e Berger (2002)). Ignorar esse
termo introduz \textbf{viés de retransformação}

Este exemplo evidencia que:

\begin{itemize}
\tightlist
\item
  a análise de resíduos é central na validação do modelo;
\item
  transformações podem corrigir violações estruturais;
\item
  a interpretação deve sempre respeitar a escala do modelo ajustado;
\item
  a escolha da transformação deve ser guiada por diagnóstico e
  fundamentação teórica.
\end{itemize}

Em regressão, a forma funcional adequada é aquela que torna os resíduos
compatíveis com as hipóteses do modelo --- não necessariamente aquela
que produz o maior \(R^2\).

\chapter{Comparação de Modelos}\label{comparauxe7uxe3o-de-modelos}

\section{Motivação e princípios de
comparação}\label{motivauxe7uxe3o-e-princuxedpios-de-comparauxe7uxe3o}

Após discutir a análise de resíduos, surge uma questão natural:
\textbf{como escolher entre diferentes modelos candidatos?} Nem sempre
um único modelo é suficiente; muitas vezes ajustamos \textbf{várias
alternativas} e precisamos compará-las.

A comparação de modelos, porém, não é um ``campeonato de métricas''. Ela
deve ser entendida como uma decisão estatística guiada por três ideias
centrais: \textbf{adequação}, \textbf{parcimônia} e \textbf{finalidade}
(explicar vs prever). Em particular, comparar modelos significa avaliar
\textbf{trocas (trade-offs)}: um modelo pode ajustar melhor os dados,
mas à custa de maior complexidade e menor estabilidade, especialmente em
amostras moderadas/pequenas.

A comparação de modelos envolve duas dimensões principais:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Qualidade estatística do ajuste} -- medida por critérios
  formais (ex.: \(R^2\), teste \(F\), AIC, BIC).\\
\item
  \textbf{Pertinência substantiva} -- se o modelo faz sentido em relação
  ao fenômeno estudado, é parcimonioso e interpretável.
\end{enumerate}

Em regressão, ``modelo melhor'' \textbf{não significa} ``modelo com o
maior ajuste numérico''. O ponto é: um modelo é um \textbf{compromisso}
entre (i) representar a estrutura sistemática de \(Y\) explicada por
\(X\) e (ii) não incorporar estrutura espúria (ruído) como se fosse
sinal. Critérios como AIC e BIC foram propostos justamente para
explicitar essa troca: melhorar o ajuste (via log-verossimilhança)
\textbf{custa} complexidade (número de parâmetros), e essa penalização é
uma maneira de desencorajar sobreajuste (Akaike (1974); Schwarz (1978);
Burnham e Anderson (2002)).

Um ponto central é que \textbf{não adianta comparar dois modelos se
ambos têm resíduos inadequados}. O primeiro filtro deve ser sempre o
diagnóstico dos resíduos. Uma vez que os modelos estejam pelo menos
aproximadamente bem especificados, podemos partir para critérios
comparativos.

Essa ordem (``diagnóstico \(\rightarrow\) comparação'') é importante
porque muitos critérios formais assumem que o modelo está
\textbf{minimamente compatível} com as hipóteses estruturais do MRLS
(por exemplo: relação média aproximadamente linear na escala adotada,
variância aproximadamente constante e ausência de padrões sistemáticos
evidentes nos resíduos). Se essas condições falham, é comum observar
situações enganosas como:

\begin{itemize}
\tightlist
\item
  um modelo com \(R^2\) alto, mas com \textbf{padrão em funil}
  (heteroscedasticidade), o que compromete inferência usual e previsões
  com incerteza mal calibrada;
\item
  um modelo com ligeira melhora em AIC/BIC após adicionar termos, mas
  com \textbf{curvatura residual persistente}, sinalizando que a forma
  funcional ainda está incorreta;
\item
  um modelo ``mais complexo'' que parece melhor no ajuste dentro da
  amostra, mas piora a capacidade de generalização (sobreajuste).
\end{itemize}

\subsection{Finalidade da comparação: explicar ou
prever}\label{finalidade-da-comparauxe7uxe3o-explicar-ou-prever}

Antes de escolher um critério, é útil explicitar o objetivo:

\begin{itemize}
\tightlist
\item
  \textbf{Explicação/interpretação:} prioriza parâmetros estáveis e
  interpretáveis e tende a valorizar parcimônia (frequentemente BIC é
  usado como referência em seleção mais ``conservadora'').\\
\item
  \textbf{Predição:} prioriza desempenho fora da amostra; critérios
  baseados em verossimilhança com penalização moderada (como AIC) são
  frequentemente usados como aproximações de desempenho preditivo, mas
  idealmente devem ser complementados por validação (ex.: validação
  cruzada) quando isso fizer parte do desenho analítico (Burnham e
  Anderson (2002)).
\end{itemize}

Nesta seção, organizamos os principais critérios formais e práticos de
comparação, seguidos de exemplos ilustrativos.

\section{Critérios clássicos de
comparação}\label{crituxe9rios-cluxe1ssicos-de-comparauxe7uxe3o}

\subsection{\texorpdfstring{Coeficiente de determinação
\(R^2\)}{Coeficiente de determinação R\^{}2}}\label{coeficiente-de-determinauxe7uxe3o-r2-1}

O \(R^2\) mede a proporção da variabilidade de \(Y\) explicada pelo
modelo:

\[
R^2 = 1 - \frac{SQ_{Res}}{SQ_{Tot}},
\]

em que \(SQ_{Res} = \sum_{i=1}^n (Y_i-\hat Y_i)^2\) é a soma de
quadrados dos resíduos e \(SQ_{Tot} = \sum_{i=1}^n (Y_i-\bar Y)^2\) é a
soma de quadrados total.

O \(R^2\) responde à pergunta: ``\textbf{quanto da variabilidade total
observada em \(Y\) foi capturada pelo componente sistemático do
modelo?}''. Ele é, portanto, uma medida \textbf{descritiva de ajuste}:
compara o erro do modelo com o erro de um ``modelo nulo'' que sempre
prediz \(\bar Y\).

Como a decomposição \[
SQ_{Tot}=SQ_{Reg}+SQ_{Res}
\] vale no MRLS com intercepto, também podemos escrever \[
R^2=\frac{SQ_{Reg}}{SQ_{Tot}},
\] isto é, a fração da variabilidade total explicada pela regressão.

\subsubsection{Propriedades e
limitações}\label{propriedades-e-limitauxe7uxf5es}

É tentador ler \(R^2\) como ``o modelo é bom/ruim'', mas isso é perigoso
por três razões:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{\(R^2\) não diagnostica adequação das hipóteses}: um modelo
  pode ter \(R^2\) alto e ainda assim apresentar resíduos com curvatura,
  heteroscedasticidade ou dependência. Por isso, o diagnóstico de
  resíduos vem antes.
\item
  \textbf{\(R^2\) não mede capacidade preditiva fora da amostra}: ele é
  calculado na amostra usada no ajuste.
\item
  \textbf{\(R^2\) depende da variabilidade de \(Y\)}: em bases com pouca
  variação em \(Y\), mesmo modelos úteis podem ter \(R^2\) modestos; e
  em bases com grande variação em \(Y\), \(R^2\) pode ser alto sem que o
  modelo seja substantivamente satisfatório.
\end{enumerate}

\textbf{Vantagens e limitaçoes }

\begin{itemize}
\tightlist
\item
  \textbf{Vantagem:} fornece uma medida intuitiva de ajuste.\\
\item
  \textbf{Limitação 1 (monotonicidade):} em modelos de MQO com
  intercepto, \(R^2\) \textbf{nunca diminui} quando adicionamos
  regressores ao conjunto de candidatos. Isso ocorre porque, ao
  adicionar parâmetros, o MQO minimiza \(SQ_{Res}\) em um espaço maior,
  logo \(SQ_{Res}\) só pode diminuir (ou permanecer igual).\\
\item
  \textbf{Limitação 2 (comparabilidade):} \(R^2\) \textbf{não é
  comparável} entre modelos com e sem intercepto, pois a decomposição de
  somas de quadrados muda. Em particular, quando o intercepto é omitido,
  o ``modelo nulo'' implícito deixa de ser \(Y=\bar Y\), e
  interpretações usuais de \(R^2\) podem se tornar enganosas.
\item
  \textbf{Limitação 3 (escala da resposta):} se você transforma \(Y\)
  (por exemplo, \(\log(Y)\)), o \(R^2\) passa a descrever ajuste
  \textbf{na escala transformada}, não na escala original.
\end{itemize}

\begin{quote}
\textbf{Observação:} no MRLS (com intercepto), vale a relação
\(R^2=r_{XY}^2\), conectando a medida de ajuste com a intensidade de
associação linear entre \(X\) e \(Y\).
\end{quote}

\subsection{\texorpdfstring{Coeficiente de determinação ajustado
\(R^2_{aj}\)}{Coeficiente de determinação ajustado R\^{}2\_\{aj\}}}\label{coeficiente-de-determinauxe7uxe3o-ajustado-r2_aj}

Para penalizar modelos excessivamente complexos, define-se:

\[
R^2_{aj} = 1 - \frac{SQ_{Res}/(n-p)}{SQ_{Tot}/(n-1)},
\]

em que \(n\) é o tamanho da amostra e \(p\) é o número de parâmetros
(incluindo o intercepto).

O \(R^2_{aj}\) substitui ``proporção de variância explicada'' por uma
comparação entre \textbf{variâncias estimadas}:

\begin{itemize}
\tightlist
\item
  \(SQ_{Res}/(n-p)\) é o estimador de \(\sigma^2\) (variância do erro)
  sob o modelo candidato;
\item
  \(SQ_{Tot}/(n-1)\) é a variância amostral de \(Y\).
\end{itemize}

Assim, \(R^2_{aj}\) pergunta: ``\textbf{o quanto a variância residual
estimada caiu em relação à variância total de \(Y\), levando em conta
quantos parâmetros eu usei para isso?}''.

Ao contrário do \(R^2\), o \(R^2_{aj}\) \textbf{pode diminuir} quando
adicionamos regressoras. Isso é desejável: se um novo termo reduz pouco
o \(SQ_{Res}\), a penalização por perder graus de liberdade pode
dominar, sinalizando que o ganho de ajuste não compensa a complexidade.

\subsubsection{Vantagens e
limitações}\label{vantagens-e-limitauxe7uxf5es}

\begin{itemize}
\tightlist
\item
  \textbf{Vantagem:} permite comparar modelos com diferentes números de
  parâmetros, desde que estejam na mesma escala da resposta e com
  intercepto.\\
\item
  \textbf{Limitação:} em amostras pequenas, ainda pode favorecer modelos
  com leve sobreajuste, pois sua penalização é relativamente moderada.\\
\item
  \textbf{Uso prático:} quando a comparação envolve modelos com
  diferentes estruturas, \(R^2_{aj}\) é preferível ao \(R^2\) simples.
\end{itemize}

\subsection{\texorpdfstring{Teste \(F\) para modelos
aninhados}{Teste F para modelos aninhados}}\label{teste-f-para-modelos-aninhados}

Quando um modelo é \textbf{caso particular de outro} (modelo restrito
vs.~modelo completo), podemos usar:

\[
F = \frac{(SQ_{Res,\,restrito} - SQ_{Res,\,completo})/q}{SQ_{Res,\,completo}/(n-p)},
\]

em que:

\begin{itemize}
\tightlist
\item
  \(q\) é o número de restrições impostas (equivalentemente, o número de
  parâmetros ``removidos'' quando passamos do completo para o restrito);
\item
  \(p\) é o número de parâmetros no modelo completo (incluindo
  intercepto).
\end{itemize}

O teste \(F\) para modelos aninhados avalia se a redução em \(SQ_{Res}\)
ao passarmos do modelo restrito para o completo é \textbf{grande o
suficiente} para justificar os parâmetros adicionais.

\begin{itemize}
\tightlist
\item
  No numerador está o ``ganho médio'' de ajuste por restrição relaxada:
  \((SQ_{Res,restrito}-SQ_{Res,completo})/q\).
\item
  No denominador está uma estimativa de \(\sigma^2\) no modelo completo:
  \(SQ_{Res,completo}/(n-p)\).
\end{itemize}

Se o ganho médio (numerador) é grande comparado ao ruído residual
esperado (denominador), a estatística \(F\) fica grande e rejeitamos o
modelo restrito.

\subsubsection{Hipóteses e decisão}\label{hipuxf3teses-e-decisuxe3o}

\begin{itemize}
\tightlist
\item
  \textbf{Hipóteses:}

  \begin{itemize}
  \tightlist
  \item
    \(H_0\): as restrições são válidas (o modelo mais simples é
    suficiente).\\
  \item
    \(H_1\): pelo menos uma restrição é falsa (o modelo completo melhora
    o ajuste de forma relevante).
  \end{itemize}
\item
  \textbf{Regra:} rejeitar \(H_0\) para valores grandes de \(F\) (ou
  valor-p pequeno).
\item
  \textbf{Uso:} útil para verificar se a inclusão de um termo (ou
  intercepto) melhora de forma estatisticamente significativa o
  ajuste.\\
\item
  \textbf{Limitação:} só se aplica a modelos \textbf{aninhados} (isto é,
  quando o modelo restrito pode ser obtido impondo restrições lineares
  nos parâmetros do modelo completo).
\end{itemize}

Adicionalmente, mesmo quando o teste \(F\) rejeita \(H_0\), ainda é
necessário verificar:

\begin{itemize}
\tightlist
\item
  se o termo adicional tem interpretação substantiva coerente;
\item
  se a inclusão introduz instabilidade (por exemplo, poucos dados
  sustentando um termo);
\item
  se o ganho é relevante na prática (e não apenas detectável por tamanho
  amostral).
\end{itemize}

\section{Critérios de
informação}\label{crituxe9rios-de-informauxe7uxe3o}

Além dos critérios clássicos, que se baseiam em soma de quadrados e
variâncias, temos medidas que incorporam explicitamente a ideia de
\textbf{verossimilhança e parcimônia}. Esses critérios nascem de uma
perspectiva mais geral de modelagem estatística, na qual o modelo é
visto como uma aproximação para a distribuição geradora dos dados.

No contexto do MRLS com erros normais,

\[
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i, 
\quad \varepsilon_i \sim \mathcal{N}(0,\sigma^2),
\]

a estimação por MQO coincide com a estimação por \textbf{máxima
verossimilhança}. Assim, podemos usar a log-verossimilhança avaliada nos
estimadores para comparar modelos.

\subsection{Critério de Informação de Akaike
(AIC)}\label{crituxe9rio-de-informauxe7uxe3o-de-akaike-aic}

\[
AIC = -2\ell(\hat\theta) + 2p,
\]

em que:

\begin{itemize}
\tightlist
\item
  \(\ell(\hat\theta)\) é a log-verossimilhança no ponto estimado;
\item
  \(p\) é o número de parâmetros estimados (incluindo \(\sigma^2\),
  quando apropriado).
\end{itemize}

O AIC pode ser interpretado como uma aproximação para a
\textbf{distância de Kullback--Leibler} entre o modelo candidato e o
verdadeiro mecanismo gerador dos dados (Akaike (1974); Burnham e
Anderson (2002)). Ele busca selecionar o modelo que, entre os
candidatos, minimiza a perda de informação esperada.

A estrutura do critério explicita o compromisso:

\begin{itemize}
\tightlist
\item
  \(-2\ell(\hat\theta)\) recompensa melhor ajuste (maior
  verossimilhança);
\item
  \(2p\) penaliza complexidade.
\end{itemize}

Assim, modelos com mais parâmetros precisam ``ganhar''
log-verossimilhança suficiente para compensar a penalização.

\textbf{Interpretação prática}

\begin{itemize}
\tightlist
\item
  \textbf{Regra:} entre modelos comparáveis, preferir o de \textbf{menor
  AIC}.
\item
  O AIC não tem interpretação absoluta; só faz sentido
  \textbf{comparativamente}.
\item
  Diferenças pequenas (por exemplo, menores que 2 unidades) geralmente
  indicam que os modelos têm suporte semelhante nos dados (Burnham e
  Anderson (2002)).
\end{itemize}

O AIC não testa hipóteses do tipo \(H_0\) vs.~\(H_1\). Ele não produz
valor-p nem decisão ``rejeita/não rejeita''. É um critério de
\textbf{seleção por informação}, não um teste clássico de significância.

Além disso, o AIC tende a favorecer modelos levemente mais complexos,
especialmente em amostras pequenas. Em contextos de amostras reduzidas,
versões corrigidas (como AICc) podem ser mais adequadas (ver Burnham e
Anderson (2002)).

\subsection{Critério Bayesiano de Schwarz
(BIC)}\label{crituxe9rio-bayesiano-de-schwarz-bic}

\[
BIC = -2\ell(\hat\theta) + p \log(n).
\]

O BIC possui forma semelhante ao AIC, mas a penalização cresce com
\(\log(n)\). Assim:

\begin{itemize}
\tightlist
\item
  Para amostras grandes, \(\log(n)\) pode ser bem maior que 2;
\item
  A penalização por parâmetro torna-se mais severa à medida que \(n\)
  aumenta.
\end{itemize}

O BIC pode ser interpretado como uma aproximação ao \textbf{log do fator
de Bayes} sob certas condições assintóticas (Schwarz (1978)). Por isso,
ele é frequentemente associado a uma perspectiva de \textbf{seleção do
modelo ``verdadeiro''} dentro do conjunto candidato.

\textbf{Interpretação prática}

\begin{itemize}
\tightlist
\item
  \textbf{Regra:} entre modelos comparáveis, preferir o de \textbf{menor
  BIC}.
\item
  O BIC tende a ser mais conservador que o AIC.
\item
  Em amostras grandes, pode penalizar fortemente modelos com muitos
  parâmetros.
\end{itemize}

\subsection{Boas práticas e comparabilidade entre
escalas}\label{boas-pruxe1ticas-e-comparabilidade-entre-escalas}

\textbf{AIC vs.~BIC: qual usar?}

\begin{itemize}
\tightlist
\item
  \textbf{AIC:} mais orientado à predição e desempenho fora da
  amostra.\\
\item
  \textbf{BIC:} mais orientado à identificação de uma estrutura ``mais
  plausível'' dentro do conjunto de candidatos.
\item
  Compare apenas modelos ajustados na \textbf{mesma escala} da variável
  resposta.
\end{itemize}

A escolha depende do objetivo da análise. Em contextos aplicados, é
comum reportar ambos e discutir a coerência entre eles.

\begin{quote}
\textbf{Nota importante sobre AIC e BIC:}\\
Embora possamos calcular AIC e BIC para qualquer modelo ajustado,
\textbf{não é correto comparar diretamente} os valores de um modelo
ajustado em \(Y\) com outro ajustado em \(\log(Y)\), por exemplo.

Isso acontece porque a transformação da resposta altera a escala e a
própria função de verossimilhança usada no cálculo dos critérios. Assim,
os valores de AIC/BIC ficam em ``bases diferentes''.

\textbf{Como proceder então?}\\
- Compare AIC/BIC apenas entre modelos ajustados \textbf{na mesma escala
da resposta}.\\
- Use análise de resíduos e diagnóstico gráfico para avaliar se a
transformação foi benéfica.\\
- Para fins de \textbf{predição}, utilize métricas de erro calculadas na
escala original de \(Y\) (ex.: RMSE, MAE) para decidir qual modelo é
mais adequado.

Em síntese: AIC e BIC são ferramentas valiosas, mas devem ser usados com
critério. Transformações em \(Y\) exigem avaliação adicional baseada em
resíduos e desempenho preditivo.
\end{quote}

\subsection{Estratégia prática de
comparação}\label{estratuxe9gia-pruxe1tica-de-comparauxe7uxe3o}

Ao comparar modelos alternativos:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Primeiro passo:} verifique os \textbf{resíduos} (condição
  mínima).

  \begin{itemize}
  \tightlist
  \item
    Se um modelo viola homocedasticidade ou apresenta estrutura
    sistemática, mesmo que tenha \(R^2\) alto, não deve ser preferido.
  \end{itemize}
\item
  \textbf{Segundo passo:} use os \textbf{critérios formais}.

  \begin{itemize}
  \tightlist
  \item
    Teste \(F\) quando os modelos são aninhados.\\
  \item
    \(R^2_{aj}\) quando a comparação é dentro da mesma escala.\\
  \item
    AIC/BIC para comparações amplas (mesma resposta, diferentes
    ajustes).
  \end{itemize}
\item
  \textbf{Terceiro passo:} considere a \textbf{interpretação
  substantiva}.

  \begin{itemize}
  \tightlist
  \item
    Um modelo matematicamente melhor pode ser substantivamente
    inadequado.
  \end{itemize}
\end{enumerate}

\section{Exemplos ilustrativos de comparação de
modelos}\label{exemplos-ilustrativos-de-comparauxe7uxe3o-de-modelos}

Para consolidar as ideias, apresentamos dois exemplos didáticos que
demonstram como comparar modelos no contexto do \textbf{MRLS}. Cada
exemplo explora um tipo de decisão prática enfrentada por analistas de
dados.

\subsection{Exemplo 1 --- Intercepto e transformação na
resposta}\label{exemplo-1-intercepto-e-transformauxe7uxe3o-na-resposta}

\textbf{Simulação do Exemplo 1:} gerar \texttt{df1} com as variáveis
\texttt{X} e \texttt{Y} (com \(Y>0\), para permitir \(\log(Y)\)).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2025}\NormalTok{)}

\CommentTok{\# Simulação: consumo (Y) vs tempo de funcionamento (X)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{50}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n)}

\CommentTok{\# Consumo com intercepto \textgreater{} 0 (stand{-}by) e heterocedasticidade moderada}
\NormalTok{mu }\OtherTok{\textless{}{-}} \DecValTok{5} \SpecialCharTok{+} \FloatTok{1.2}\SpecialCharTok{*}\NormalTok{X}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \FloatTok{0.6} \SpecialCharTok{+} \FloatTok{0.05}\SpecialCharTok{*}\NormalTok{X}
\NormalTok{Y }\OtherTok{\textless{}{-}}\NormalTok{ mu }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, sigma)}
\NormalTok{Y }\OtherTok{\textless{}{-}} \FunctionTok{pmax}\NormalTok{(Y, }\FloatTok{0.1}\NormalTok{)  }\CommentTok{\# garantir Y\textgreater{}0 (para log)}

\NormalTok{df1 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{X =}\NormalTok{ X, }\AttributeTok{Y =}\NormalTok{ Y)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  \textbf{Cenário e modelos candidatos:} consumo de energia (\(Y\)) em
  função do tempo de funcionamento (\(X\)), com \(n=50\) observações.
\end{enumerate}

\textbf{Modelos candidatos:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Com intercepto\\
  \[
  Y = \beta_0 + \beta_1 X + \varepsilon
  \] Representa consumo mínimo (stand-by). Esperado em contextos onde
  \(Y>0\) mesmo quando \(X=0\).
\item
  Sem intercepto\\
  \[
  Y = \beta_1 X + \varepsilon
  \] Força a reta a passar pela origem. Só faz sentido se sabemos que
  \(Y=0\) quando \(X=0\).
\item
  Transformado (log da resposta)\\
  \[
  \log Y = \beta_0 + \beta_1 X + \varepsilon
  \]
\end{enumerate}

Nesta configuração com variável transformada, o coeficiente \(\beta_1\)
indica a mudança \textbf{aditiva em} \(\log(Y)\) a cada unidade
adicional em \(X\). Na escala original de \(Y\), isso equivale a dizer
que um aumento de 1 unidade em \(X\) está associado a uma multiplicação
esperada de \(Y\) por \(\exp(\beta_1)\).

Aqui estamos comparando três ``ideias de modelo'' que, apesar de
parecidas na forma, respondem a perguntas ligeiramente diferentes:

\begin{itemize}
\tightlist
\item
  \textbf{Modelo A (com intercepto)} permite que exista um nível médio
  de consumo quando \(X=0\) (consumo residual/stand-by).\\
\item
  \textbf{Modelo A0 (sem intercepto)} declara, como hipótese estrutural,
  que ``se \(X=0\), então \(Y=0\)''. Essa é uma afirmação forte: se ela
  for falsa, o ajuste pode ``compensar'' deslocando a inclinação e
  gerando resíduos estruturados.\\
\item
  \textbf{Modelo B (com \(\log Y\))} muda a escala da resposta e,
  portanto, muda o tipo de erro ``natural'' no modelo (efeitos
  multiplicativos no \(Y\) original tendem a virar efeitos aditivos em
  \(\log(Y)\)).
\end{itemize}

\textbf{Visualização da primeiras linhas da Base de dados}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(df1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 2
      X     Y
  <dbl> <dbl>
1 0      5.37
2 0.204  5.27
3 0.408  5.97
4 0.612  6.54
5 0.816  6.22
6 1.02   6.12
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  \textbf{Estimação e resumos dos modelos}
\end{enumerate}

\textbf{Ajustes e tabela de coeficientes:} ajustar \texttt{mod\_A},
\texttt{mod\_A0} e \texttt{mod\_log}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ajustes}
\NormalTok{mod\_A   }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ df1)       }\CommentTok{\# com intercepto}
\NormalTok{mod\_A0  }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ df1)   }\CommentTok{\# sem intercepto}
\NormalTok{mod\_log }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(Y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ df1)  }\CommentTok{\# resposta em log}

\FunctionTok{cat}\NormalTok{(}\StringTok{"Modelo com intercepto (A):}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Modelo com intercepto (A):
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{coef}\NormalTok{(}\FunctionTok{summary}\NormalTok{(mod\_A)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            Estimate Std. Error  t value     Pr(>|t|)
(Intercept) 5.137603 0.24760614 20.74909 1.297563e-25
X           1.196581 0.04266949 28.04301 2.032694e-31
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{Modelo sem intercepto (A0):}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Modelo sem intercepto (A0):
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{coef}\NormalTok{(}\FunctionTok{summary}\NormalTok{(mod\_A0)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Estimate Std. Error t value     Pr(>|t|)
X 1.959437 0.06767437 28.9539 1.770922e-32
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{Modelo com log(Y) (B):}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Modelo com log(Y) (B):
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{coef}\NormalTok{(}\FunctionTok{summary}\NormalTok{(mod\_log)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             Estimate  Std. Error  t value     Pr(>|t|)
(Intercept) 1.7791870 0.025029186 71.08450 2.673492e-50
X           0.1143677 0.004313232 26.51554 2.548074e-30
\end{verbatim}

\textbf{Comentários:}\\
- O \textbf{Modelo A (com intercepto)} mostra um termo constante
significativo, representando consumo em stand-by (\(Y>0\) em
\(x \approx 0\)).\\
- O \textbf{Modelo A0 (sem intercepto)} ignora o consumo em stand-by e
força a reta a passar pela origem.\\
- O \textbf{Modelo B (log da resposta)} reduz heteroscedasticidade: os
coeficientes mantêm significância e a interpretação passa a ser na
escala da variável transformada.

\textbf{Teste de resíduos (omnibus, Jarque-Bera, skew, kurtosis,
Durbin-Watson, etc.)}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Testes/medidas adicionais dos resíduos}
  \FunctionTok{library}\NormalTok{(lmtest)   }\CommentTok{\# dwtest}
  \FunctionTok{library}\NormalTok{(tseries)  }\CommentTok{\# jarque.bera.test}
  \FunctionTok{library}\NormalTok{(moments)  }\CommentTok{\# skewness, kurtosis}

\NormalTok{res\_tests }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mod, nome)\{}
\NormalTok{  e }\OtherTok{\textless{}{-}} \FunctionTok{resid}\NormalTok{(mod)}

\NormalTok{  jb }\OtherTok{\textless{}{-}} \FunctionTok{jarque.bera.test}\NormalTok{(e)}
\NormalTok{  sk }\OtherTok{\textless{}{-}} \FunctionTok{skewness}\NormalTok{(e)}
\NormalTok{  ku }\OtherTok{\textless{}{-}} \FunctionTok{kurtosis}\NormalTok{(e)  }\CommentTok{\# kurtosis "crua" (Normal \textasciitilde{} 3)}

  \CommentTok{\# Durbin{-}Watson (lmtest)}
\NormalTok{  dw }\OtherTok{\textless{}{-}} \FunctionTok{dwtest}\NormalTok{(mod)}

  \CommentTok{\# Omnibus D\textquotesingle{}Agostino{-}Pearson (quando disponível via fBasics::dagoTest)}
\NormalTok{  omni\_out }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{requireNamespace}\NormalTok{(}\StringTok{"fBasics"}\NormalTok{, }\AttributeTok{quietly =} \ConstantTok{TRUE}\NormalTok{)) \{}
\NormalTok{    omni\_out }\OtherTok{\textless{}{-}}\NormalTok{ fBasics}\SpecialCharTok{::}\FunctionTok{dagoTest}\NormalTok{(e)}
\NormalTok{  \}}

  \FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{==============================}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"Testes dos resíduos —"}\NormalTok{, nome, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"==============================}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}

  \FunctionTok{cat}\NormalTok{(}\StringTok{"Jarque{-}Bera:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  \FunctionTok{print}\NormalTok{(jb)}

  \FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{Assimetria (skewness): "}\NormalTok{, }\FunctionTok{round}\NormalTok{(sk, }\DecValTok{4}\NormalTok{), }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\AttributeTok{sep =} \StringTok{""}\NormalTok{)}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"Curtose (kurtosis):   "}\NormalTok{, }\FunctionTok{round}\NormalTok{(ku, }\DecValTok{4}\NormalTok{), }\StringTok{"  (Normal \textasciitilde{} 3)}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\AttributeTok{sep =} \StringTok{""}\NormalTok{)}

  \FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{Durbin{-}Watson (lmtest::dwtest):}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  \FunctionTok{print}\NormalTok{(dw)}

  \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.null}\NormalTok{(omni\_out)) \{}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{Omnibus (D\textquotesingle{}Agostino{-}Pearson) — fBasics::dagoTest:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
    \FunctionTok{print}\NormalTok{(omni\_out)}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{Omnibus (D\textquotesingle{}Agostino{-}Pearson):}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Pacote \textquotesingle{}fBasics\textquotesingle{} não disponível no ambiente. (Opcional: instalar para executar o omnibus.)}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}

\FunctionTok{res\_tests}\NormalTok{(mod\_A,   }\StringTok{"Modelo A (Y \textasciitilde{} X, com intercepto)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

==============================
Testes dos resíduos — Modelo A (Y ~ X, com intercepto) 
==============================
Jarque-Bera:

    Jarque Bera Test

data:  e
X-squared = 0.39891, df = 2, p-value = 0.8192


Assimetria (skewness): 0.1998
Curtose (kurtosis):   2.8216  (Normal ~ 3)

Durbin-Watson (lmtest::dwtest):

    Durbin-Watson test

data:  mod
DW = 2.165, p-value = 0.6695
alternative hypothesis: true autocorrelation is greater than 0


Omnibus (D'Agostino-Pearson) — fBasics::dagoTest:

Title:
 D'Agostino Normality Test

Test Results:
  STATISTIC:
    Chi2 | Omnibus: 0.4194
    Z3  | Skewness: 0.6388
    Z4  | Kurtosis: 0.1065
  P VALUE:
    Omnibus  Test: 0.8108 
    Skewness Test: 0.523 
    Kurtosis Test: 0.9152 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{res\_tests}\NormalTok{(mod\_A0,  }\StringTok{"Modelo A0 (Y \textasciitilde{} X, sem intercepto)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

==============================
Testes dos resíduos — Modelo A0 (Y ~ X, sem intercepto) 
==============================
Jarque-Bera:

    Jarque Bera Test

data:  e
X-squared = 2.6548, df = 2, p-value = 0.2652


Assimetria (skewness): -0.0498
Curtose (kurtosis):   1.8755  (Normal ~ 3)

Durbin-Watson (lmtest::dwtest):

    Durbin-Watson test

data:  mod
DW = 0.22013, p-value < 2.2e-16
alternative hypothesis: true autocorrelation is greater than 0


Omnibus (D'Agostino-Pearson) — fBasics::dagoTest:

Title:
 D'Agostino Normality Test

Test Results:
  STATISTIC:
    Chi2 | Omnibus: 8.7587
    Z3  | Skewness: -0.1602
    Z4  | Kurtosis: -2.9552
  P VALUE:
    Omnibus  Test: 0.01253 
    Skewness Test: 0.8728 
    Kurtosis Test: 0.003125 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{res\_tests}\NormalTok{(mod\_log, }\StringTok{"Modelo B (log(Y) \textasciitilde{} X)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

==============================
Testes dos resíduos — Modelo B (log(Y) ~ X) 
==============================
Jarque-Bera:

    Jarque Bera Test

data:  e
X-squared = 3.1899, df = 2, p-value = 0.2029


Assimetria (skewness): 0.5779
Curtose (kurtosis):   3.4418  (Normal ~ 3)

Durbin-Watson (lmtest::dwtest):

    Durbin-Watson test

data:  mod
DW = 1.5831, p-value = 0.04934
alternative hypothesis: true autocorrelation is greater than 0


Omnibus (D'Agostino-Pearson) — fBasics::dagoTest:

Title:
 D'Agostino Normality Test

Test Results:
  STATISTIC:
    Chi2 | Omnibus: 4.236
    Z3  | Skewness: 1.7691
    Z4  | Kurtosis: 1.0518
  P VALUE:
    Omnibus  Test: 0.1203 
    Skewness Test: 0.07688 
    Kurtosis Test: 0.2929 
\end{verbatim}

\textbf{Comentários:}

\begin{itemize}
\tightlist
\item
  \textbf{Modelo A (com intercepto):}

  \begin{itemize}
  \tightlist
  \item
    O teste Omnibus e o Jarque-Bera não são significativos (valores-p
    altos), sugerindo que os resíduos não violam fortemente a
    normalidade.\\
  \item
    O Durbin-Watson próximo de 2 indica ausência de autocorrelação
    serial.\\
  \item
    Conclusão: resíduos adequados, modelo consistente.
  \end{itemize}
\item
  \textbf{Modelo A0 (sem intercepto):}

  \begin{itemize}
  \tightlist
  \item
    O Durbin-Watson é muito baixo (≈ 0,2), indicando forte
    autocorrelação positiva dos resíduos.\\
  \item
    Embora Omnibus/Jarque-Bera não rejeitem a normalidade, a dependência
    serial torna o modelo problemático.\\
  \item
    Conclusão: estatisticamente inadequado, reforçando que a exclusão do
    intercepto distorce o ajuste.
  \end{itemize}
\item
  \textbf{Modelo B (log da resposta):}

  \begin{itemize}
  \tightlist
  \item
    Testes de normalidade (Omnibus, JB) continuam não significativos,
    mas a assimetria (Skew negativo) sugere leve desvio.\\
  \item
    O Durbin-Watson ≈ 1,4 aponta alguma autocorrelação positiva.\\
  \item
    Conclusão: apesar das pequenas imperfeições, o modelo
    log-transformado melhora a homocedasticidade em relação ao Modelo A.
  \end{itemize}
\end{itemize}

Em dados simulados sem mecanismo temporal explícito, autocorrelação
forte geralmente é um \textbf{sinal de especificação inadequada} (por
exemplo, restrições erradas como \(\beta_0=0\) podem ``organizar'' os
resíduos e induzir padrões). Em aplicações reais, autocorrelação também
pode ocorrer por dependência temporal genuína; nesse caso, o MRLS pode
precisar ser estendido (tema para capítulos posteriores).

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Modelos aninhados: teste F (A0 vs A)}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Teste F (modelos aninhados): A0 (restrito) vs A (com intercepto)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"ANOVA (Teste F: A0 vs A):}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ANOVA (Teste F: A0 vs A):
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{anova}\NormalTok{(mod\_A0, mod\_A))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: Y ~ 0 + X
Model 2: Y ~ X
  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    
1     49 377.84                                  
2     48  37.90  1    339.94 430.52 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

O modelo sem intercepto (A0) é um \textbf{caso particular} do modelo com
intercepto (A), obtido ao impor a restrição \(\beta_0=0\). Logo, o teste
\(F\) avalia se ``forçar a origem'' piora o ajuste além do esperado por
acaso.

\textbf{Comentários:}\\
- O teste F compara os modelos \textbf{A0 (restrito)} e \textbf{A (com
intercepto)}.\\
- O resultado altamente significativo (valor-p próximo de zero) indica
que o \textbf{intercepto é necessário}.\\
- Assim, rejeitamos o modelo sem intercepto e preferimos o modelo com
intercepto.

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Comparação numérica (}\(R^2\), \(R_{aj}^2\), AIC, BIC)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{cmp1 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{Modelo =} \FunctionTok{c}\NormalTok{(}\StringTok{"A: Y\textasciitilde{}X (c/ intercepto)"}\NormalTok{, }\StringTok{"A0: Y\textasciitilde{}X (sem intercepto)"}\NormalTok{, }
             \StringTok{"B: log(Y)\textasciitilde{}X"}\NormalTok{),}
  \AttributeTok{R2     =} \FunctionTok{c}\NormalTok{(}\FunctionTok{summary}\NormalTok{(mod\_A)}\SpecialCharTok{$}\NormalTok{r.squared,     }\FunctionTok{summary}\NormalTok{(mod\_A0)}\SpecialCharTok{$}\NormalTok{r.squared,     }
             \FunctionTok{summary}\NormalTok{(mod\_log)}\SpecialCharTok{$}\NormalTok{r.squared),}
  \AttributeTok{R2\_aj  =} \FunctionTok{c}\NormalTok{(}\FunctionTok{summary}\NormalTok{(mod\_A)}\SpecialCharTok{$}\NormalTok{adj.r.squared, }\FunctionTok{summary}\NormalTok{(mod\_A0)}\SpecialCharTok{$}\NormalTok{adj.r.squared, }
             \FunctionTok{summary}\NormalTok{(mod\_log)}\SpecialCharTok{$}\NormalTok{adj.r.squared),}
  \AttributeTok{AIC    =} \FunctionTok{c}\NormalTok{(}\FunctionTok{AIC}\NormalTok{(mod\_A), }\FunctionTok{AIC}\NormalTok{(mod\_A0), }\FunctionTok{AIC}\NormalTok{(mod\_log)),}
  \AttributeTok{BIC    =} \FunctionTok{c}\NormalTok{(}\FunctionTok{BIC}\NormalTok{(mod\_A), }\FunctionTok{BIC}\NormalTok{(mod\_A0), }\FunctionTok{BIC}\NormalTok{(mod\_log))}
\NormalTok{)}

\NormalTok{cmp1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 5
  Modelo                      R2 R2_aj   AIC   BIC
  <chr>                    <dbl> <dbl> <dbl> <dbl>
1 A: Y~X (c/ intercepto)   0.942 0.941 134.  140. 
2 A0: Y~X (sem intercepto) 0.945 0.944 247.  251. 
3 B: log(Y)~X              0.936 0.935 -95.1 -89.4
\end{verbatim}

Lembre-se que um \(R^2\) elevado pode coexistir com \textbf{resíduos
ruins}. Isso acontece porque \(R^2\) mede ``quanto o modelo explica'' em
termos de variabilidade total, mas não garante que as hipóteses do MRLS
estejam razoavelmente satisfeitas. Em particular, um modelo pode ter
alto \(R^2\) e ainda assim produzir inferências pouco confiáveis
(erros-padrão distorcidos).

\textbf{Comentários:}\\
- O \textbf{Modelo A0} apresenta \(R^2\) elevado, mas penalizações via
AIC/BIC mostram que é muito inferior (valores bem maiores). Logo,
acredita-se que os coeficientes ficaram distorcidos, superestimando a
inclinação.\\
- O \textbf{Modelo A} combina bom ajuste (\(R^2_{aj}\) alto) e
parcimônia.\\
- O \textbf{Modelo B (log)} apresenta os menores valores de AIC/BIC.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{21}
\tightlist
\item
  \textbf{Diagnóstico gráfico comparativo}
\end{enumerate}

\textbf{Dispersões com retas ajustadas dos dois modelos restantes}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{suppressPackageStartupMessages}\NormalTok{(\{}
  \FunctionTok{library}\NormalTok{(ggplot2)}
  \FunctionTok{library}\NormalTok{(dplyr)}
  \FunctionTok{library}\NormalTok{(tidyr)}
\NormalTok{\})}

\NormalTok{dfA  }\OtherTok{\textless{}{-}}\NormalTok{ df1 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{valor =}\NormalTok{ Y,        }\AttributeTok{modelo =} \StringTok{"A: Y \textasciitilde{} X (c/ intercepto)"}\NormalTok{)}
\NormalTok{dfA0 }\OtherTok{\textless{}{-}}\NormalTok{ df1 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{valor =}\NormalTok{ Y,        }\AttributeTok{modelo =} \StringTok{"A0: Y \textasciitilde{} X (sem intercepto)"}\NormalTok{)}
\NormalTok{dfB  }\OtherTok{\textless{}{-}}\NormalTok{ df1 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{valor =} \FunctionTok{log}\NormalTok{(Y),   }\AttributeTok{modelo =} \StringTok{"B: log(Y) \textasciitilde{} X"}\NormalTok{)}

\NormalTok{df\_plot }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(dfA, dfA0, dfB)}

\FunctionTok{ggplot}\NormalTok{(df\_plot, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X, }\AttributeTok{y =}\NormalTok{ valor)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{linewidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{modelo, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"X"}\NormalTok{, }\AttributeTok{y =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_compara_files/figure-pdf/CHUNK_CM_07_NEW-1.pdf}}

}

\caption{Exemplo 1 --- Dispersão com reta OLS: Modelo A
(Y\textasciitilde X, com intercepto), Modelo A0 (Y\textasciitilde X, sem
intercepto) e Modelo B (log(Y)\textasciitilde X).}

\end{figure}%

\textbf{Comentários:}\\
- Em ambos os \textbf{Modelos (A e B)}, não observa-se aumento relevante
da variabilidade de \(Y\) conforme \(X\) cresce.

\textbf{Resíduos vs.~valores ajustados}

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(ggplot2)}
  \FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{dados }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{fit =} \FunctionTok{fitted}\NormalTok{(mod\_A),   }\AttributeTok{res =} \FunctionTok{resid}\NormalTok{(mod\_A),   }\AttributeTok{modelo =} \StringTok{"A: Y \textasciitilde{} X (c/ intercepto)"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{fit =} \FunctionTok{fitted}\NormalTok{(mod\_A0),  }\AttributeTok{res =} \FunctionTok{resid}\NormalTok{(mod\_A0),  }\AttributeTok{modelo =} \StringTok{"A0: Y \textasciitilde{} X (sem intercepto)"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{fit =} \FunctionTok{fitted}\NormalTok{(mod\_log), }\AttributeTok{res =} \FunctionTok{resid}\NormalTok{(mod\_log), }\AttributeTok{modelo =} \StringTok{"B: log(Y) \textasciitilde{} X"}\NormalTok{)}
\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(dados, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fit, }\AttributeTok{y =}\NormalTok{ res)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{modelo, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free\_x"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Ajustados"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Resíduo"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_compara_files/figure-pdf/CHUNK_CM_08_NEW-1.pdf}}

}

\caption{Exemplo 1 --- Resíduos vs Ajustados: comparação entre A, A0 e
B.}

\end{figure}%

\textbf{Comentário:}\\
Neste gráfico, buscamos uma nuvem aproximadamente aleatória em torno de
0. Três estruturas são especialmente informativas:

\begin{itemize}
\tightlist
\item
  \textbf{Funil} (dispersão aumentando/diminuindo com o nível ajustado):
  sinal de \textbf{heteroscedasticidade}.\\
\item
  \textbf{Curvatura} (padrão em arco): sinal de \textbf{forma funcional
  inadequada} (não linearidade não capturada).\\
\item
  \textbf{Faixas} (bandas horizontais): podem surgir por
  discretização/limitação de medida em \(Y\).
\end{itemize}

Na comparação entre modelos, o melhor candidato é o que \emph{reduz}
essas estruturas, sem criar novas.

\textbf{Resíduos estudentizados vs.~valores ajustados}

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(ggplot2)}
  \FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{dados }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{fit =} \FunctionTok{fitted}\NormalTok{(mod\_A),   }\AttributeTok{stud =} \FunctionTok{rstudent}\NormalTok{(mod\_A),   }\AttributeTok{modelo =} \StringTok{"A: Y \textasciitilde{} X (c/ intercepto)"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{fit =} \FunctionTok{fitted}\NormalTok{(mod\_A0),  }\AttributeTok{stud =} \FunctionTok{rstudent}\NormalTok{(mod\_A0),  }\AttributeTok{modelo =} \StringTok{"A0: Y \textasciitilde{} X (sem intercepto)"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{fit =} \FunctionTok{fitted}\NormalTok{(mod\_log), }\AttributeTok{stud =} \FunctionTok{rstudent}\NormalTok{(mod\_log), }\AttributeTok{modelo =} \StringTok{"B: log(Y) \textasciitilde{} X"}\NormalTok{)}
\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(dados, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fit, }\AttributeTok{y =}\NormalTok{ stud)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{,  }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{modelo, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free\_x"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Ajustados"}\NormalTok{, }\AttributeTok{y =} \StringTok{"t* (estudentizado)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_compara_files/figure-pdf/CHUNK_CM_09_NEW-1.pdf}}

}

\caption{Exemplo 1 --- Resíduos estudentizados vs Ajustados: comparação
entre A, A0 e B.}

\end{figure}%

\textbf{Comentário:}\\
Estes gráficos apresentam informações similares aos ilustradas nos
gráficos anteriores

\textbf{QQ-plot dos resíduos}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(dplyr)}

\CommentTok{\# dados do QQ{-}plot + parâmetros da reta ao estilo qqline (base R)}
\NormalTok{qq\_df\_line }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mod, modelo)\{}
\NormalTok{  r }\OtherTok{\textless{}{-}} \FunctionTok{rstandard}\NormalTok{(mod)}
\NormalTok{  q }\OtherTok{\textless{}{-}} \FunctionTok{qqnorm}\NormalTok{(r, }\AttributeTok{plot.it =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{  df }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{theo =}\NormalTok{ q}\SpecialCharTok{$}\NormalTok{x, }\AttributeTok{samp =}\NormalTok{ q}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{modelo =}\NormalTok{ modelo)}

  \CommentTok{\# mesma regra do qqline: reta baseada nos quartis (25\% e 75\%)}
\NormalTok{  yq }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(r, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{), }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  xq }\OtherTok{\textless{}{-}} \FunctionTok{qnorm}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{))}

\NormalTok{  slope }\OtherTok{\textless{}{-}}\NormalTok{ (yq[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ yq[}\DecValTok{1}\NormalTok{])}\SpecialCharTok{/}\NormalTok{(xq[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ xq[}\DecValTok{1}\NormalTok{])}
\NormalTok{  intercept }\OtherTok{\textless{}{-}}\NormalTok{ yq[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ slope}\SpecialCharTok{*}\NormalTok{xq[}\DecValTok{1}\NormalTok{]}

  \FunctionTok{list}\NormalTok{(}\AttributeTok{df =}\NormalTok{ df, }\AttributeTok{line =} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{modelo =}\NormalTok{ modelo, }\AttributeTok{intercept =}\NormalTok{ intercept, }\AttributeTok{slope =}\NormalTok{ slope))}
\NormalTok{\}}

\NormalTok{outA  }\OtherTok{\textless{}{-}} \FunctionTok{qq\_df\_line}\NormalTok{(mod\_A,   }\StringTok{"A: Y \textasciitilde{} X (c/ intercepto)"}\NormalTok{)}
\NormalTok{outA0 }\OtherTok{\textless{}{-}} \FunctionTok{qq\_df\_line}\NormalTok{(mod\_A0,  }\StringTok{"A0: Y \textasciitilde{} X (sem intercepto)"}\NormalTok{)}
\NormalTok{outB  }\OtherTok{\textless{}{-}} \FunctionTok{qq\_df\_line}\NormalTok{(mod\_log, }\StringTok{"B: log(Y) \textasciitilde{} X"}\NormalTok{)}

\NormalTok{dados }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(outA}\SpecialCharTok{$}\NormalTok{df, outA0}\SpecialCharTok{$}\NormalTok{df, outB}\SpecialCharTok{$}\NormalTok{df)}
\NormalTok{linhas }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(outA}\SpecialCharTok{$}\NormalTok{line, outA0}\SpecialCharTok{$}\NormalTok{line, outB}\SpecialCharTok{$}\NormalTok{line)}

\FunctionTok{ggplot}\NormalTok{(dados, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ theo, }\AttributeTok{y =}\NormalTok{ samp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{data =}\NormalTok{ linhas, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{intercept =}\NormalTok{ intercept, }\AttributeTok{slope =}\NormalTok{ slope),}
              \AttributeTok{linewidth =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{modelo, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Quantis teoricos (Normal)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Residuos padronizados"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_compara_files/figure-pdf/CHUNK_CM_10_NEW-1.pdf}}

}

\caption{Exemplo 1 --- QQ-plot (rstandard) com reta tipo qqline:
comparação entre A, A0 e B.}

\end{figure}%

\textbf{Comentário:}\\
O QQ-plot avalia \emph{normalidade aproximada} por meio do alinhamento
entre os quantis amostrais dos resíduos e os quantis teóricos da Normal.
A interpretação deve ser feita por padrões:

\begin{itemize}
\tightlist
\item
  \textbf{Alinhamento global próximo da reta}: evidência visual a favor
  de resíduos aproximadamente normais (pelo menos no ``miolo'' da
  distribuição).\\
\item
  \textbf{Desvios sistemáticos nas caudas} (pontos afastando-se da reta
  apenas no início e no fim): indicam \textbf{caudas mais pesadas ou
  mais leves} que a Normal; isso costuma afetar sobretudo inferência em
  amostras pequenas.\\
\item
  \textbf{Padrão em ``S''}: sugere \textbf{assimetria} (skewness
  diferente de 0).\\
\item
  \textbf{Um ou poucos pontos muito afastados}: podem ser indício de
  \textbf{outliers} (verificar também resíduos estudentizados e Cook).
\end{itemize}

Ao comparar modelos, prefira o que apresenta \textbf{menos estrutura
sistemática} no QQ-plot, especialmente quando isso é coerente com as
medidas numéricas de assimetria/curtose e com testes como Jarque--Bera.

\textbf{Histograma dos resíduos}

\begin{Shaded}
\begin{Highlighting}[]
  \FunctionTok{library}\NormalTok{(ggplot2)}
  \FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{dados }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{r =} \FunctionTok{resid}\NormalTok{(mod\_A),   }\AttributeTok{modelo =} \StringTok{"A: Y \textasciitilde{} X (c/ intercepto)"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{r =} \FunctionTok{resid}\NormalTok{(mod\_A0),  }\AttributeTok{modelo =} \StringTok{"A0: Y \textasciitilde{} X (sem intercepto)"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{r =} \FunctionTok{resid}\NormalTok{(mod\_log), }\AttributeTok{modelo =} \StringTok{"B: log(Y) \textasciitilde{} X"}\NormalTok{)}
\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(dados, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{12}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{modelo, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Resíduo"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Frequência"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_compara_files/figure-pdf/CHUNK_CM_11_NEW-1.pdf}}

}

\caption{Exemplo 1 --- Histograma dos resíduos: comparação entre A, A0 e
B.}

\end{figure}%

\textbf{Comentário:}\\

O histograma é um diagnóstico auxiliar: ele ajuda a visualizar
\textbf{assimetria} e \textbf{caudas}. Em geral:

\begin{itemize}
\tightlist
\item
  histograma muito assimétrico sugere assimetria nos resíduos;\\
\item
  caudas longas ou ``ombros'' podem sugerir caudas pesadas e/ou
  outliers.
\end{itemize}

A interpretação deve ser combinada com QQ-plot e com medidas como
assimetria/curtose.

\textbf{Resíduos vs.~X}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{suppressPackageStartupMessages}\NormalTok{(\{}
  \FunctionTok{library}\NormalTok{(ggplot2)}
  \FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{\})}

\NormalTok{dados }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{X =}\NormalTok{ df1}\SpecialCharTok{$}\NormalTok{X, }\AttributeTok{res =} \FunctionTok{resid}\NormalTok{(mod\_A),   }\AttributeTok{modelo =} \StringTok{"A: Y \textasciitilde{} X (c/ intercepto)"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{X =}\NormalTok{ df1}\SpecialCharTok{$}\NormalTok{X, }\AttributeTok{res =} \FunctionTok{resid}\NormalTok{(mod\_A0),  }\AttributeTok{modelo =} \StringTok{"A0: Y \textasciitilde{} X (sem intercepto)"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{X =}\NormalTok{ df1}\SpecialCharTok{$}\NormalTok{X, }\AttributeTok{res =} \FunctionTok{resid}\NormalTok{(mod\_log), }\AttributeTok{modelo =} \StringTok{"B: log(Y) \textasciitilde{} X"}\NormalTok{)}
\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(dados, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X, }\AttributeTok{y =}\NormalTok{ res)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{modelo, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"X"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Resíduo"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_compara_files/figure-pdf/CHUNK_CM_12_NEW-1.pdf}}

}

\caption{Exemplo 1 --- Resíduos vs X: comparação entre A, A0 e B.}

\end{figure}%

\textbf{Comentário:}\\
- Ambos os \textbf{Modelos (A e B)} apresentam dispersão uniforme em
torno de zero, corroborando a homocedasticidade.

O diagnóstico gráfico serve como ``primeira triagem'': se houver funil,
curvatura ou caudas muito pesadas, isso aparece visualmente de modo
imediato. Somente depois disso faz sentido dar peso às comparações
numéricas.

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\setcounter{enumi}{5}
\tightlist
\item
  \textbf{Conclusões do exemplo}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  O modelo sem intercepto tende a se ajustar mal, pois ignora o consumo
  em stand-by. Este modelo foi descartado no teste com modelo
  aninhados.\\
\item
  O modelo com intercepto melhora substancialmente o ajuste quando
  comparado com o modelo sem intercepto.\\
\item
  Assim como o modelo com intercepto (A), o modelo com variável
  transformada (B) também mostrou um bom ajuste.\\
\item
  Os resíduos dos modelos A e B mostraram que podemos considerar que
  ambos os modelos foram bem especificados.\\
\item
  O modelo B apresentou AIC/BIC menores. No entanto, como apresentado
  anteriormente, não é correto comparar diretamente os valores de um
  modelo ajustado em \(Y\) com outro ajustado em \(\log(Y)\), pois a
  verossimilhança é diferente.\\
\item
  Considerando que os modelos A e B foram bem especificados e nao teve
  uma métrica de qualidade de ajuste muito favorável a um deles, é
  aconselhado o uso do modelo mais simples e na escala original da
  variável (Modelo A - com intercepto).
\end{itemize}

\subsection{Exemplo 2 --- Escolhendo a melhor variável
explicativa}\label{exemplo-2-escolhendo-a-melhor-variuxe1vel-explicativa}

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  \textbf{Cenário e modelos candidatos} um pesquisador deseja explicar a
  produtividade agrícola (\(Y\)) a partir de três variáveis candidatas:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(X_1\) = quantidade de fertilizante aplicada (kg/ha)\\
\item
  \(X_2\) = volume de irrigação (mm)\\
\item
  \(X_3\) = horas de sol na safra (h)
\end{itemize}

O objetivo é descobrir \textbf{qual dessas variáveis explica melhor}
\(Y\) de forma individual.

Os três MRLS candidatos são:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Modelo A:} \(Y = \beta_0 + \beta_1 X_1 + \varepsilon\)\\
\item
  \textbf{Modelo B:} \(Y = \beta_0 + \beta_2 X_2 + \varepsilon\)\\
\item
  \textbf{Modelo C:} \(Y = \beta_0 + \beta_3 X_3 + \varepsilon\)
\end{enumerate}

\textbf{Passos da análise comparativa:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Ajuste de cada modelo} separadamente.\\
\item
  \textbf{Diagnóstico dos resíduos} em cada caso, verificando
  linearidade, homoscedasticidade e normalidade.\\
\item
  \textbf{Comparação de medidas de ajuste}: \(R^2\), \(R^2_{aj}\), AIC e
  BIC.\\
\item
  \textbf{Discussão substantiva}: qual variável faz mais sentido
  teoricamente como explicativa de \(Y\).
\end{enumerate}

\textbf{Critérios práticos de decisão:}\\
- Se todos os modelos tiverem resíduos adequados, a comparação pode ser
feita pelos critérios formais (AIC/BIC, \(R^2_{aj}\)).\\
- Se apenas um modelo tiver resíduos consistentes com as hipóteses do
MRLS, ele deve ser preferido.\\
- Mesmo que dois modelos apresentem ajustes estatisticamente próximos, a
escolha deve considerar a \textbf{interpretação prática}.

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Simulação dos dados}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{suppressPackageStartupMessages}\NormalTok{(\{}
  \FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{\})}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2025}\NormalTok{)}

\CommentTok{\# Simulação: produtividade (Y) explicada por X1, X2, X3 (candidatas)}
\NormalTok{n2 }\OtherTok{\textless{}{-}} \DecValTok{120}
\NormalTok{X1 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n2,   }\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{)   }\CommentTok{\# fertilizante, 0–100 kg/ha}
\NormalTok{X2 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n2,   }\DecValTok{0}\NormalTok{,  }\DecValTok{60}\NormalTok{)   }\CommentTok{\# irrigação, 0–60 mm}
\NormalTok{X3 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n2, }\DecValTok{200}\NormalTok{, }\DecValTok{350}\NormalTok{)   }\CommentTok{\# horas de sol, 200–350 h}

\CommentTok{\# Verdade de geração}
\NormalTok{Y2 }\OtherTok{\textless{}{-}} \DecValTok{20} \SpecialCharTok{+} \FloatTok{0.45}\SpecialCharTok{*}\NormalTok{X1 }\SpecialCharTok{+} \FloatTok{0.12}\SpecialCharTok{*}\NormalTok{X2 }\SpecialCharTok{+} \FloatTok{0.02}\SpecialCharTok{*}\NormalTok{X3 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n2, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)}

\NormalTok{df2 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{Y =}\NormalTok{ Y2, }\AttributeTok{X1 =}\NormalTok{ X1, }\AttributeTok{X2 =}\NormalTok{ X2, }\AttributeTok{X3 =}\NormalTok{ X3)}

\FunctionTok{head}\NormalTok{(df2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
      Y    X1    X2    X3
  <dbl> <dbl> <dbl> <dbl>
1  68.4  73.3  43.4  239.
2  59.9  47.6  39.9  245.
3  67.5  51.4  44.3  229.
4  59.2  49.8  33.2  332.
5  49.9  78.0  41.1  250.
6  42.0  50.4  43.0  241.
\end{verbatim}

Neste exemplo, as três variáveis candidatas têm escalas diferentes, mas
isso não impede o ajuste de três MRLS separados. O que muda é a
interpretação dos coeficientes e a magnitude dos erros-padrão. Como o
objetivo é ``melhor preditor individual'', estamos comparando
\textbf{três modelos alternativos não aninhados}.

Este exemplo buscar mostrar que, mesmo no contexto de MRLS, a comparação
entre variáveis explicativas pode guiar a seleção do \textbf{melhor
preditor individual}.

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Roteiro de tarefas (atividade guiada)}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Ajuste os três modelos candidatos} separadamente:

  \begin{itemize}
  \tightlist
  \item
    Modelo A: \$ Y = \beta\_0 + \beta\_1 X\_1 + \varepsilon \$\\
  \item
    Modelo B: \$ Y = \beta\_0 + \beta\_2 X\_2 + \varepsilon \$
  \item
    Modelo C: \$ Y = \beta\_0 + \beta\_3 X\_3 + \varepsilon \$
  \end{itemize}
\item
  \textbf{Inspecione os resíduos} de cada modelo:

  \begin{itemize}
  \tightlist
  \item
    Gráficos de resíduos versus ajustados.\\
  \item
    QQ-plot para verificar normalidade.\\
  \item
    Histograma dos resíduos.\\
  \item
    Resíduos versus a variável explicativa \(X\).
  \end{itemize}
\item
  \textbf{Compare as medidas de ajuste} entre os modelos:

  \begin{itemize}
  \tightlist
  \item
    \(R^2_{aj}\) (ajustado)\\
  \item
    AIC\\
  \item
    BIC
  \end{itemize}
\item
  \textbf{Discuta os resultados obtidos}:

  \begin{itemize}
  \tightlist
  \item
    Qual modelo apresentou resíduos mais consistentes com as hipóteses
    do MRLS?\\
  \item
    Qual modelo apresentou melhor desempenho segundo \(R^2_{aj}\), AIC e
    BIC?\\
  \item
    Existe coerência entre os diagnósticos gráficos e as medidas
    numéricas?
  \end{itemize}
\item
  \textbf{Reflexão substantiva:}

  \begin{itemize}
  \tightlist
  \item
    Do ponto de vista prático, qual variável é a melhor candidata a
    explicar a produtividade agrícola (\(Y\)) individualmente e por
    quê?\\
  \item
    Considere plausibilidade causal e relevância no contexto agrícola
    (fertilizante, irrigação ou horas de sol).
  \end{itemize}
\item
  \textbf{Desafio opcional:}

  \begin{itemize}
  \tightlist
  \item
    Re-simule os dados com outro \emph{seed} ou altere o nível de
    ruído.\\
  \item
    Observe se a escolha do melhor modelo permanece a mesma ou se muda.
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ajustes dos três MRLS candidatos}
\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X1, }\AttributeTok{data =}\NormalTok{ df2)}
\NormalTok{m2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X2, }\AttributeTok{data =}\NormalTok{ df2)}
\NormalTok{m3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X3, }\AttributeTok{data =}\NormalTok{ df2)}

\FunctionTok{cat}\NormalTok{(}\StringTok{"=== Modelo A: Y \textasciitilde{} X1 ===}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{); }\FunctionTok{print}\NormalTok{(}\FunctionTok{summary}\NormalTok{(m1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
=== Modelo A: Y ~ X1 ===
\end{verbatim}

\begin{verbatim}

Call:
lm(formula = Y ~ X1, data = df2)

Residuals:
     Min       1Q   Median       3Q      Max 
-30.5982  -6.0204   0.6442   6.9029  27.5385 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 31.49842    1.84972   17.03   <2e-16 ***
X1           0.41940    0.03076   13.63   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 9.73 on 118 degrees of freedom
Multiple R-squared:  0.6117,    Adjusted R-squared:  0.6084 
F-statistic: 185.9 on 1 and 118 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{=== Modelo B: Y \textasciitilde{} X2 ===}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{); }\FunctionTok{print}\NormalTok{(}\FunctionTok{summary}\NormalTok{(m2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

=== Modelo B: Y ~ X2 ===
\end{verbatim}

\begin{verbatim}

Call:
lm(formula = Y ~ X2, data = df2)

Residuals:
    Min      1Q  Median      3Q     Max 
-45.332 -10.528   2.142  12.276  35.247 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 47.69985    2.63414  18.108  < 2e-16 ***
X2           0.19641    0.07436   2.641  0.00937 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 15.17 on 118 degrees of freedom
Multiple R-squared:  0.05583,   Adjusted R-squared:  0.04783 
F-statistic: 6.977 on 1 and 118 DF,  p-value: 0.009374
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{=== Modelo C: Y \textasciitilde{} X3 ===}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{); }\FunctionTok{print}\NormalTok{(}\FunctionTok{summary}\NormalTok{(m3))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

=== Modelo C: Y ~ X3 ===
\end{verbatim}

\begin{verbatim}

Call:
lm(formula = Y ~ X3, data = df2)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.032 -12.257   1.166  11.872  31.353 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 58.31156    8.78394   6.638 1.02e-09 ***
X3          -0.01715    0.03168  -0.541    0.589    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 15.59 on 118 degrees of freedom
Multiple R-squared:  0.002478,  Adjusted R-squared:  -0.005975 
F-statistic: 0.2932 on 1 and 118 DF,  p-value: 0.5892
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cmp2 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{Modelo =} \FunctionTok{c}\NormalTok{(}\StringTok{"A: Y\textasciitilde{}X1"}\NormalTok{, }\StringTok{"B: Y\textasciitilde{}X2"}\NormalTok{, }\StringTok{"C: Y\textasciitilde{}X3"}\NormalTok{),}
  \AttributeTok{R2     =} \FunctionTok{c}\NormalTok{(}\FunctionTok{summary}\NormalTok{(m1)}\SpecialCharTok{$}\NormalTok{r.squared,     }\FunctionTok{summary}\NormalTok{(m2)}\SpecialCharTok{$}\NormalTok{r.squared,     }\FunctionTok{summary}\NormalTok{(m3)}\SpecialCharTok{$}\NormalTok{r.squared),}
  \AttributeTok{R2\_aj  =} \FunctionTok{c}\NormalTok{(}\FunctionTok{summary}\NormalTok{(m1)}\SpecialCharTok{$}\NormalTok{adj.r.squared, }\FunctionTok{summary}\NormalTok{(m2)}\SpecialCharTok{$}\NormalTok{adj.r.squared, }\FunctionTok{summary}\NormalTok{(m3)}\SpecialCharTok{$}\NormalTok{adj.r.squared),}
  \AttributeTok{AIC    =} \FunctionTok{c}\NormalTok{(}\FunctionTok{AIC}\NormalTok{(m1), }\FunctionTok{AIC}\NormalTok{(m2), }\FunctionTok{AIC}\NormalTok{(m3)),}
  \AttributeTok{BIC    =} \FunctionTok{c}\NormalTok{(}\FunctionTok{BIC}\NormalTok{(m1), }\FunctionTok{BIC}\NormalTok{(m2), }\FunctionTok{BIC}\NormalTok{(m3))}
\NormalTok{)}

\NormalTok{cmp2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 5
  Modelo       R2    R2_aj   AIC   BIC
  <chr>     <dbl>    <dbl> <dbl> <dbl>
1 A: Y~X1 0.612    0.608    891.  899.
2 B: Y~X2 0.0558   0.0478   997. 1006.
3 C: Y~X3 0.00248 -0.00598 1004. 1012.
\end{verbatim}

\textbf{Diagnóstico gráfico comparativo:}\\
- dispersão \(Y\) vs cada \(X_j\) com reta OLS (um painel por modelo);\\
- resíduos vs ajustados (um painel por modelo);\\
- QQ-plot (um painel por modelo);\\
- histograma dos resíduos (um painel por modelo);\\
- resíduos vs \(X_j\) (um painel por modelo).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{suppressPackageStartupMessages}\NormalTok{(\{}
  \FunctionTok{library}\NormalTok{(ggplot2)}
  \FunctionTok{library}\NormalTok{(dplyr)}
  \FunctionTok{library}\NormalTok{(tidyr)}
\NormalTok{\})}

\CommentTok{\# Organizar dados em formato longo para facilitar facetas}
\NormalTok{long\_xy }\OtherTok{\textless{}{-}}\NormalTok{ df2 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(X1, X2, X3), }\AttributeTok{names\_to =} \StringTok{"Xname"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"X"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{modelo =} \FunctionTok{recode}\NormalTok{(Xname, }\AttributeTok{X1 =} \StringTok{"Modelo A: Y\textasciitilde{}X1"}\NormalTok{, }\AttributeTok{X2 =} \StringTok{"Modelo B: Y\textasciitilde{}X2"}\NormalTok{, }\AttributeTok{X3 =} \StringTok{"Modelo C: Y\textasciitilde{}X3"}\NormalTok{))}

\CommentTok{\# Função auxiliar para extrair resíduos e ajustados por modelo}
\NormalTok{aug1 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{fit =} \FunctionTok{fitted}\NormalTok{(m1), }\AttributeTok{res =} \FunctionTok{resid}\NormalTok{(m1), }\AttributeTok{modelo =} \StringTok{"Modelo A: Y\textasciitilde{}X1"}\NormalTok{)}
\NormalTok{aug2 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{fit =} \FunctionTok{fitted}\NormalTok{(m2), }\AttributeTok{res =} \FunctionTok{resid}\NormalTok{(m2), }\AttributeTok{modelo =} \StringTok{"Modelo B: Y\textasciitilde{}X2"}\NormalTok{)}
\NormalTok{aug3 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{fit =} \FunctionTok{fitted}\NormalTok{(m3), }\AttributeTok{res =} \FunctionTok{resid}\NormalTok{(m3), }\AttributeTok{modelo =} \StringTok{"Modelo C: Y\textasciitilde{}X3"}\NormalTok{)}
\NormalTok{aug  }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(aug1, aug2, aug3)}

\CommentTok{\# 1) Y vs X com reta OLS}
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(long\_xy, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X, }\AttributeTok{y =}\NormalTok{ Y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.85}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{linewidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{modelo, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free\_x"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{y =} \StringTok{"Y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{12}\NormalTok{)}

\CommentTok{\# 2) Resíduos vs ajustados}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(aug, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fit, }\AttributeTok{y =}\NormalTok{ res)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.85}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{modelo, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free\_x"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Ajustados"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Resíduo"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{12}\NormalTok{)}

\CommentTok{\# 3) QQ{-}plot (estilo plot.lm, which = 2)}

\NormalTok{qq\_df\_lmstyle }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mod, modelo)\{}
\NormalTok{  r }\OtherTok{\textless{}{-}} \FunctionTok{rstandard}\NormalTok{(mod)                 }\CommentTok{\# mesmo tipo de resíduo do plot.lm}
\NormalTok{  q }\OtherTok{\textless{}{-}} \FunctionTok{qqnorm}\NormalTok{(r, }\AttributeTok{plot.it =} \ConstantTok{FALSE}\NormalTok{)}

\NormalTok{  pts }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{theo =}\NormalTok{ q}\SpecialCharTok{$}\NormalTok{x,}
    \AttributeTok{samp =}\NormalTok{ q}\SpecialCharTok{$}\NormalTok{y,}
    \AttributeTok{modelo =}\NormalTok{ modelo}
\NormalTok{  )}

  \CommentTok{\# reta tipo qqline(): passa pelos quartis 25\% e 75\%}
\NormalTok{  yq }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(r, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{))}
\NormalTok{  xq }\OtherTok{\textless{}{-}} \FunctionTok{qnorm}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{))}
\NormalTok{  slope }\OtherTok{\textless{}{-}}\NormalTok{ (yq[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ yq[}\DecValTok{1}\NormalTok{])}\SpecialCharTok{/}\NormalTok{(xq[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ xq[}\DecValTok{1}\NormalTok{])}
\NormalTok{  intercept }\OtherTok{\textless{}{-}}\NormalTok{ yq[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ slope}\SpecialCharTok{*}\NormalTok{xq[}\DecValTok{1}\NormalTok{]}

\NormalTok{  line }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{modelo =}\NormalTok{ modelo,}
    \AttributeTok{intercept =} \FunctionTok{as.numeric}\NormalTok{(intercept),}
    \AttributeTok{slope =} \FunctionTok{as.numeric}\NormalTok{(slope)}
\NormalTok{  )}

  \FunctionTok{list}\NormalTok{(}\AttributeTok{pts =}\NormalTok{ pts, }\AttributeTok{line =}\NormalTok{ line)}
\NormalTok{\}}

\NormalTok{o1 }\OtherTok{\textless{}{-}} \FunctionTok{qq\_df\_lmstyle}\NormalTok{(m1, }\StringTok{"Modelo A: Y\textasciitilde{}X1"}\NormalTok{)}
\NormalTok{o2 }\OtherTok{\textless{}{-}} \FunctionTok{qq\_df\_lmstyle}\NormalTok{(m2, }\StringTok{"Modelo B: Y\textasciitilde{}X2"}\NormalTok{)}
\NormalTok{o3 }\OtherTok{\textless{}{-}} \FunctionTok{qq\_df\_lmstyle}\NormalTok{(m3, }\StringTok{"Modelo C: Y\textasciitilde{}X3"}\NormalTok{)}

\NormalTok{qq\_all  }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(o1}\SpecialCharTok{$}\NormalTok{pts,  o2}\SpecialCharTok{$}\NormalTok{pts,  o3}\SpecialCharTok{$}\NormalTok{pts)}
\NormalTok{qq\_line }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(o1}\SpecialCharTok{$}\NormalTok{line, o2}\SpecialCharTok{$}\NormalTok{line, o3}\SpecialCharTok{$}\NormalTok{line)}

\NormalTok{p3 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(qq\_all, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ theo, }\AttributeTok{y =}\NormalTok{ samp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.85}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ qq\_line,}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{intercept =}\NormalTok{ intercept, }\AttributeTok{slope =}\NormalTok{ slope),}
    \AttributeTok{linewidth =} \FloatTok{0.8}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{modelo, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Quantis teóricos (Normal)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Resíduos padronizados"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{12}\NormalTok{)}

\CommentTok{\# 4) Histogramas dos resíduos}
\NormalTok{res\_all }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{r =} \FunctionTok{resid}\NormalTok{(m1), }\AttributeTok{modelo =} \StringTok{"Modelo A: Y\textasciitilde{}X1"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{r =} \FunctionTok{resid}\NormalTok{(m2), }\AttributeTok{modelo =} \StringTok{"Modelo B: Y\textasciitilde{}X2"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{r =} \FunctionTok{resid}\NormalTok{(m3), }\AttributeTok{modelo =} \StringTok{"Modelo C: Y\textasciitilde{}X3"}\NormalTok{)}
\NormalTok{)}

\NormalTok{p4 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(res\_all, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{12}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{modelo, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Resíduo"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Frequência"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{12}\NormalTok{)}

\CommentTok{\# 5) Resíduos vs X (por modelo)}
\NormalTok{rx }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
\NormalTok{  df2 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{transmute}\NormalTok{(}\AttributeTok{X =}\NormalTok{ X1, }\AttributeTok{res =} \FunctionTok{resid}\NormalTok{(m1), }\AttributeTok{modelo =} \StringTok{"Modelo A: Y\textasciitilde{}X1"}\NormalTok{),}
\NormalTok{  df2 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{transmute}\NormalTok{(}\AttributeTok{X =}\NormalTok{ X2, }\AttributeTok{res =} \FunctionTok{resid}\NormalTok{(m2), }\AttributeTok{modelo =} \StringTok{"Modelo B: Y\textasciitilde{}X2"}\NormalTok{),}
\NormalTok{  df2 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{transmute}\NormalTok{(}\AttributeTok{X =}\NormalTok{ X3, }\AttributeTok{res =} \FunctionTok{resid}\NormalTok{(m3), }\AttributeTok{modelo =} \StringTok{"Modelo C: Y\textasciitilde{}X3"}\NormalTok{)}
\NormalTok{)}

\NormalTok{p5 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(rx, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X, }\AttributeTok{y =}\NormalTok{ res)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.85}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{modelo, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free\_x"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{y =} \StringTok{"Resíduo"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{12}\NormalTok{)}

\CommentTok{\# Mostrar os 5 gráficos (um por vez, na ordem)}
\FunctionTok{print}\NormalTok{(p1)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_compara_files/figure-pdf/CHUNK_CM_16-1.pdf}}

}

\caption{Diagnóstico gráfico comparativo (Exemplo 2): cada linha
corresponde a um modelo (A, B, C).}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(p2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_compara_files/figure-pdf/CHUNK_CM_16-2.pdf}}

}

\caption{Diagnóstico gráfico comparativo (Exemplo 2): cada linha
corresponde a um modelo (A, B, C).}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(p3)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_compara_files/figure-pdf/CHUNK_CM_16-3.pdf}}

}

\caption{Diagnóstico gráfico comparativo (Exemplo 2): cada linha
corresponde a um modelo (A, B, C).}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(p4)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_compara_files/figure-pdf/CHUNK_CM_16-4.pdf}}

}

\caption{Diagnóstico gráfico comparativo (Exemplo 2): cada linha
corresponde a um modelo (A, B, C).}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(p5)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{mrls_compara_files/figure-pdf/CHUNK_CM_16-5.pdf}}

}

\caption{Diagnóstico gráfico comparativo (Exemplo 2): cada linha
corresponde a um modelo (A, B, C).}

\end{figure}%

\begin{itemize}
\tightlist
\item
  Se um modelo ``vence'' nos critérios numéricos, mas apresenta
  funil/curvatura/caudas pesadas, ele deve perder força como
  candidato.\\
\item
  Se dois modelos forem próximos numericamente, a decisão pode depender
  do contexto (medição, causalidade plausível, custo de obter a
  variável, etc.).
\end{itemize}

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Fechamento e síntese}
\end{enumerate}

Os dois exemplos mostraram situações complementares na prática do MRLS.
O \textbf{Exemplo 1} destacou como diferentes especificações de um mesmo
modelo, com intercepto, sem intercepto e com transformação em (Y), podem
levar a conclusões distintas sobre ajuste, resíduos e interpretação. Já
o \textbf{Exemplo 2} explorou a escolha entre diferentes variáveis
candidatas, mostrando como a comparação de modelos separados orienta a
seleção do preditor mais adequado.

Em conjunto, os exemplos reforçam que a escolha do modelo não deve se
basear apenas em indicadores numéricos, mas sim no equilíbrio entre
\textbf{consistência estatística, parcimônia e coerência substantiva}
com o fenômeno estudado.

\chapter{Exercícios e atividades}\label{exercuxedcios-e-atividades-1}

Em breve!!!

\part{Parte III --- Modelo de Regressão Linear Simples (MRLM)}

\chapter{Exercícios e atividades}\label{exercuxedcios-e-atividades-2}

Em breve!!!

\part{Parte IV --- Apêndices}

\chapter{Lista de Siglas e
Símbolos}\label{lista-de-siglas-e-suxedmbolos}

\section{Lista de Siglas}\label{lista-de-siglas}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.7500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Sigla
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Significado
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
MRLS & Modelo de Regressão Linear Simples \\
MRLM & Modelo de Regressão Linear Múltipla \\
MQO / OLS & Mínimos Quadrados Ordinários / \emph{Ordinary Least
Squares} \\
MV & Máxima Verossimilhança \\
WLS & \emph{Weighted Least Squares} (Regressão Linear Ponderada) \\
GLS & \emph{Generalized Least Squares} (MQ Generalizados) \\
BLUE & \emph{Best Linear Unbiased Estimator} (Melhor Estimador Linear
Não-Viesado) \\
GLM & \emph{Generalized Linear Model} (Modelo Linear Generalizado) \\
GAM & \emph{Generalized Additive Model} (Modelo Aditivo Generalizado) \\
GAMLSS & \emph{Generalized Additive Model for Location, Scale and
Shape} \\
ANOVA & Análise de Variância \\
IC & Intervalo de Confiança \\
EP & Erro-Padrão \\
IC95\% & Intervalo de Confiança a 95\% \\
H0, H1 & Hipótese nula, Hipótese alternativa \\
PDF, CDF & Função Densidade de Probabilidade; Função de Distribuição
Acumulada \\
AIC, BIC, AICc & Critérios de Informação (Akaike, Bayesiano, Akaike
corrigido) \\
\(R^2\), \(\bar{R}^2\) & Coeficiente de determinação e ajustado \\
SQTot, SQReg, SQRes & Somas de Quadrados (Total, Regressão, Resíduos) \\
PRESS & \emph{Predicted Residual Sum of Squares} (LOOCV) \\
CV-\(k\) & \emph{k}-fold Cross-Validation \\
VIF & \emph{Variance Inflation Factor} \\
df & Graus de liberdade \\
Var, Cov, Corr & Variância, Covariância, Correlação \\
FDR & \emph{False Discovery Rate} \\
\end{longtable}

\clearpage

\section{Lista de Símbolos}\label{lista-de-suxedmbolos}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3611}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6389}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Símbolo
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Descrição
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(Y,\ \mathbf{Y}\) & Variável resposta (escalar / vetor de
observações) \\
\(X,\ \mathbf{X}\) & Matriz de covariáveis (matriz de planejamento) \\
\(\beta_0,\ \beta_1,\ \dots,\ \beta_p\) & Parâmetros do modelo
(intercepto e inclinações) \\
\(\boldsymbol{\beta}\) & Vetor de parâmetros
\((\beta_0,\ \beta_1,\ \ldots,\ \beta_p)^\top\) \\
\(\varepsilon_i,\ \boldsymbol{\varepsilon}\) & Termo(s) de erro
aleatório (escalar / vetor) \\
\(\hat{\beta}_j,\ \hat{\boldsymbol{\beta}}\) & Estimadores (MQO/MV) dos
parâmetros \\
\(\hat{Y}_i,\ \hat{\mathbf{Y}}\) & Valores ajustados pelo modelo
(escalar / vetor) \\
\(\hat{\varepsilon}_i,\ \hat{\boldsymbol{\varepsilon}}\) & Resíduos
(observado -- ajustado; escalar / vetor) \\
\(\hat{\sigma}^2\) & Estimador da variância residual
(\(\hat{\sigma}^2=\mathrm{SQRes}/(n-p-1)\)) \\
\(\mathbf{H}\) & Matriz ``chapéu'' (projeção):
\(\ \mathbf{H}=\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\) \\
\(\mathbf{M}\) & Matriz dos resíduos:
\(\ \mathbf{M}=\mathbf{I}_n-\mathbf{H}\) \\
\(h_{ii}\) & Alavancagem (diagonal de \(\mathbf{H}\)) \\
\(r_i\) & Resíduo \emph{studentizado}:
\(r_i=\dfrac{\hat{\varepsilon}_i}{\hat{\sigma}\sqrt{1-h_{ii}}}\) \\
\(D_i\) & Distância de Cook \\
\(\mathbf{I}_n\) & Matriz identidade de dimensão \(n\) \\
\(\mathrm{col}(\mathbf{X})\) & Espaço coluna de \(\mathbf{X}\) \\
\(\operatorname{rank}(\mathbf{X})\) & Posto (rank) de \(\mathbf{X}\) \\
\(\mathbf{X}^+\) & Inversa generalizada de Moore--Penrose \\
\(\mathrm{tr}(\mathbf{A})\) & Traço da matriz \(\mathbf{A}\) \\
\(\det(\mathbf{A})\) & Determinante de \(\mathbf{A}\) \\
\(\mathbf{A}^{-1}\), \(\mathbf{A}^\top\) & Inversa e transposta de
\(\mathbf{A}\) \\
\(\mathbb{R}^n\) & Espaço vetorial real \(n\)-dimensional \\
\(\mathcal{N}_n(\boldsymbol{\mu},\boldsymbol{\Sigma})\) & Distribuição
Normal \(n\)-variada \\
\(\boldsymbol{\mu},\ \boldsymbol{\Sigma}\) & Média e matriz de
covariância na Normal multivariada \\
\(n,\ p\) & Nº de observações; Nº de variáveis explicativas \\
\(\bar{X},\ \bar{Y}\) & Médias amostrais de \(X\) e \(Y\) \\
\(S_{xx},\ S_{xy}\) & Somas de quadrados e produto cruzado (MRLS) \\
\(\mathbf{C},\ d\) & Matriz de contrastes e vetor-alvo (testes
conjuntos: \(H_0:\mathbf{C}\boldsymbol{\beta}=d\)) \\
\end{longtable}

\chapter{Estrutura Matricial dos Modelos de Regressão
Linear}\label{estrutura-matricial-dos-modelos-de-regressuxe3o-linear}

A formulação moderna dos modelos de regressão linear é essencialmente
matricial. Essa notação vetorial revela a estrutura geométrica do
problema de estimação, explicita as condições necessárias para
identificabilidade dos parâmetros e permite analisar propriedades
estatísticas dos estimadores de forma sistemática (ver Harville (1997)).

Este apêndice consolida os principais elementos de Álgebra Linear
utilizados ao longo do estudo de regressão, com ênfase nas estruturas
que reaparecem na estimação por mínimos quadrados, na inferência e na
análise de diagnóstico.

\section{Operações Fundamentais com Matrizes e
Vetores}\label{operauxe7uxf5es-fundamentais-com-matrizes-e-vetores}

A linguagem matricial é uma forma compacta de escrever o modelo de
regressão que permite enxergar o problema como um problema geométrico em
\(\mathbb{R}^n\). Cada vetor corresponde a um ponto ou direção nesse
espaço, e cada matriz representa uma transformação linear.

Sejam \(\mathbf{A}\) e \(\mathbf{B}\) matrizes de dimensões compatíveis
e \(\mathbf{x}, \mathbf{y}\) vetores coluna em \(\mathbb{R}^n\).

Para fixar ideias, considere explicitamente:

\[
\mathbf{x} =
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix},
\qquad
\mathbf{y} =
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}.
\]

\subsection{Soma Matricial}\label{soma-matricial}

Se

\[
\mathbf{A} =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix},
\qquad
\mathbf{B} =
\begin{bmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{bmatrix},
\]

então

\[
\mathbf{A} + \mathbf{B}
=
\begin{bmatrix}
a_{11}+b_{11} & a_{12}+b_{12} \\
a_{21}+b_{21} & a_{22}+b_{22}
\end{bmatrix}.
\]

A soma é definida elemento a elemento e exige dimensões idênticas.

\subsection{Produto Matricial}\label{produto-matricial}

Se

\[
\mathbf{A} =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix},
\qquad
\mathbf{B} =
\begin{bmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{bmatrix},
\]

então

\[
\mathbf{A}\mathbf{B}
=
\begin{bmatrix}
a_{11}b_{11}+a_{12}b_{21} & a_{11}b_{12}+a_{12}b_{22} \\
a_{21}b_{11}+a_{22}b_{21} & a_{21}b_{12}+a_{22}b_{22}
\end{bmatrix}.
\]

O produto matricial corresponde à composição de transformações
lineares.\\
No modelo linear

\[
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta},
\]

a matriz \(\mathbf{X}\), de dimensão \(n \times (p+1)\) transforma o
vetor de parâmetros \(\boldsymbol{\beta}\), de dimensão
\((p+1) \times 1\) em um vetor no espaço das respostas. Assim,
\(\mathbf{X}\) pode ser interpretada como uma transformação que leva
parâmetros em \(\mathbb{R}^{p+1}\) para vetores ajustados em
\(\mathbb{R}^n\).

\subsection{Produto Interno e Norma}\label{produto-interno-e-norma}

O produto interno entre vetores é dado por

\[
\mathbf{x}^\top \mathbf{y}
=
\sum_{i=1}^n x_i y_i.
\]

Quando \(\mathbf{x} = \mathbf{y}\), obtemos

\[
\mathbf{x}^\top \mathbf{x}
=
\sum_{i=1}^n x_i^2,
\]

que define o quadrado da norma euclidiana:

\[
\|\mathbf{x}\|^2 = \mathbf{x}^\top \mathbf{x}.
\]

Essa noção de norma é central na regressão, pois a estimação por mínimos
quadrados consiste em minimizar

\[
\|\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}\|^2.
\]

Portanto, o problema de estimação é um problema geométrico de minimizar
distância no espaço \(\mathbb{R}^n\). A formulação geométrica da
regressão em termos de subespaços e projeções é desenvolvida em detalhe
em Harville (1997).

\subsection{Forma Quadrática}\label{forma-quadruxe1tica}

Uma expressão da forma

\[
\mathbf{x}^\top \mathbf{A}\mathbf{x}
\]

é chamada forma quadrática.

Para visualizar, considere

\[
\mathbf{x} =
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix},
\qquad
\mathbf{A} =
\begin{bmatrix}
a & b \\
b & c
\end{bmatrix}.
\]

Então

\[
\mathbf{x}^\top \mathbf{A}\mathbf{x}
=
a x_1^2 + 2b x_1 x_2 + c x_2^2.
\]

Observe que surgem termos quadráticos e termos mistos. Em regressão, as
somas de quadrados explicada e residual podem ser escritas exatamente
como formas quadráticas do vetor \(\mathbf{Y}\) (ver Rencher e
Christensen (2012)).

\subsection{Transposição}\label{transposiuxe7uxe3o}

A transposta de uma matriz é obtida trocando linhas por colunas:

\[
\mathbf{A} =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix}
\quad
\Rightarrow
\quad
\mathbf{A}^\top =
\begin{bmatrix}
a_{11} & a_{21} \\
a_{12} & a_{22}
\end{bmatrix}.
\]

A transposição é essencial para definir produtos internos e garantir que
expressões como \(\mathbf{X}^\top\mathbf{X}\) sejam matrizes quadradas.

\subsection{Inversão}\label{inversuxe3o}

Uma matriz quadrada \(\mathbf{A}\) é invertível se existe
\(\mathbf{A}^{-1}\) tal que

\[
\mathbf{A}^{-1}\mathbf{A} = \mathbf{A}\mathbf{A}^{-1} = \mathbf{I}.
\]

Por exemplo, para uma matriz \(2 \times 2\),

\[
\mathbf{A} =
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix},
\]

se \(ad - bc \neq 0\), então

\[
\mathbf{A}^{-1}
=
\frac{1}{ad-bc}
\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}.
\]

Na regressão, a invertibilidade de \(\mathbf{X}^\top \mathbf{X}\) é
condição necessária para a existência do estimador de mínimos quadrados
único.Para o caso de posto deficiente e o uso de decomposição SVD e
pseudoinversas, ver Golub e Van Loan (2013).

\subsection{Propriedades Importantes}\label{propriedades-importantes}

Duas identidades frequentemente utilizadas são

\[
(\mathbf{A}\mathbf{B})^\top = \mathbf{B}^\top \mathbf{A}^\top,
\qquad
(\mathbf{A}^{-1})^\top = (\mathbf{A}^\top)^{-1}.
\]

Essas propriedades são fundamentais na dedução de resultados como:

\[
\mathrm{Var}(\hat{\boldsymbol{\beta}})
=
\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1},
\]

e na demonstração das propriedades das matrizes de projeção.

Essas operações constituem o mecanismo estrutural que permitirá:

\begin{itemize}
\tightlist
\item
  interpretar o estimador como projeção ortogonal;
\item
  escrever somas de quadrados como formas quadráticas;
\item
  analisar variâncias e covariâncias de estimadores;
\item
  compreender a geometria do ajuste e do diagnóstico.
\end{itemize}

Nos itens seguintes, essas operações serão organizadas dentro da
estrutura específica do modelo linear múltiplo.

\section{Estruturas Matriciais
Relevantes}\label{estruturas-matriciais-relevantes}

Determinados tipos de matrizes surgem de forma recorrente na teoria da
regressão linear. Cada uma delas corresponde a uma propriedade
geométrica ou estatística que será explorada na estimação, na inferência
e na análise de diagnóstico.

A tabela a seguir resume algumas dessas estruturas fundamentais.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3611}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3889}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Tipo
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definição
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Relevância
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Identidade \(\mathbf{I}_n\) & Diagonal principal composta por 1's &
Elemento neutro da multiplicação \\
Simétrica & \(\mathbf{A} = \mathbf{A}^\top\) & Autovalores reais \\
Idempotente & \(\mathbf{A}^2 = \mathbf{A}\) & Projeções \\
Ortogonal & \(\mathbf{A}^\top \mathbf{A} = \mathbf{I}\) & Preserva
norma \\
Diagonal & Elementos fora da diagonal iguais a zero & Simplifica formas
quadráticas \\
Definida positiva & \(\mathbf{x}^\top \mathbf{A}\mathbf{x} > 0\) para
todo \(\mathbf{x}\neq 0\) & Garantia de invertibilidade e convexidade \\
\end{longtable}

A seguir, detalham-se as propriedades e implicações dessas estruturas no
contexto do modelo linear.

\subsection{Matriz Identidade}\label{matriz-identidade}

A matriz identidade de ordem \(n\) é dada por

\[
\mathbf{I}_n =
\begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}.
\]

Ela satisfaz

\[
\mathbf{I}_n \mathbf{x} = \mathbf{x}
\quad
\text{para todo } \mathbf{x} \in \mathbb{R}^n.
\]

No modelo linear, a identidade aparece, por exemplo, na matriz de
covariância dos erros sob homocedasticidade:

\[
\mathrm{Cov}(\boldsymbol{\varepsilon}) = \sigma^2 \mathbf{I}_n.
\]

Sob normalidade, essa estrutura implica independência e variância
constante dos erros.

\subsection{Matrizes Simétricas}\label{matrizes-simuxe9tricas}

Uma matriz é simétrica se

\[
\mathbf{A} = \mathbf{A}^\top.
\]

Por exemplo,

\[
\mathbf{A} =
\begin{bmatrix}
2 & -1 \\
-1 & 3
\end{bmatrix}
\]

é simétrica.

Matrizes simétricas possuem autovalores reais e podem ser diagonalizadas
por matrizes ortogonais. Essa propriedade é crucial para compreender a
decomposição espectral de \(\mathbf{X}^\top \mathbf{X}\) e analisar
problemas como multicolinearidade (ver Harville (1997)).

No modelo linear, as matrizes \(\mathbf{X}^\top \mathbf{X}\),
\(\mathbf{H}\) e \(\mathbf{M}\) são simétricas.

\subsection{Matrizes Idempotentes}\label{matrizes-idempotentes}

Uma matriz é idempotente se

\[
\mathbf{A}^2 = \mathbf{A}.
\]

Isso implica que aplicar a transformação duas vezes produz o mesmo
resultado que aplicá-la uma vez.

Por exemplo, a matriz

\[
\mathbf{P} =
\begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix}
\]

é idempotente.

No modelo linear, a matriz de projeção

\[
\mathbf{H} = \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top
\]

satisfaz

\[
\mathbf{H}^2 = \mathbf{H}.
\]

Isso significa que \(\mathbf{H}\) projeta vetores no subespaço
\(\mathrm{col}(\mathbf{X})\). Uma vez projetado, aplicar novamente a
projeção não altera o vetor.

Essa propriedade é fundamental para compreender:

\begin{itemize}
\tightlist
\item
  decomposição ortogonal;
\item
  independência entre componentes projetadas sob normalidade;
\item
  decomposição da soma de quadrados total.
\end{itemize}

\subsection{Matrizes Ortogonais}\label{matrizes-ortogonais}

Uma matriz \(\mathbf{Q}\) é ortogonal se

\[
\mathbf{Q}^\top \mathbf{Q} = \mathbf{I}.
\]

Isso implica que

\[
\|\mathbf{Q}\mathbf{x}\| = \|\mathbf{x}\|.
\]

Ou seja, matrizes ortogonais preservam comprimentos e ângulos.

Essa propriedade é central na decomposição espectral de matrizes
simétricas:

\[
\mathbf{A} = \mathbf{Q}\boldsymbol{\Lambda}\mathbf{Q}^\top,
\]

em que \(\boldsymbol{\Lambda}\) é diagonal contendo os autovalores.A
análise numérica dessas decomposições é tratada em profundidade em Golub
e Van Loan (2013).

Na regressão, essa decomposição permite analisar:

\begin{itemize}
\tightlist
\item
  a estrutura de \(\mathbf{X}^\top \mathbf{X}\);
\item
  a estabilidade numérica da estimação;
\item
  o efeito da multicolinearidade.
\end{itemize}

\subsection{Matrizes Diagonais}\label{matrizes-diagonais}

Uma matriz diagonal possui a forma

\[
\mathbf{D} =
\begin{bmatrix}
d_1 & 0 & \cdots & 0 \\
0 & d_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & d_n
\end{bmatrix}.
\]

Formas quadráticas envolvendo matrizes diagonais simplificam-se para

\[
\mathbf{x}^\top \mathbf{D}\mathbf{x}
=
\sum_{i=1}^n d_i x_i^2.
\]

Essa simplificação é útil na análise de variâncias e na interpretação de
decomposições espectrais.

\subsection{Matrizes Definidas
Positivas}\label{matrizes-definidas-positivas}

Uma matriz simétrica \(\mathbf{A}\) é definida positiva se

\[
\mathbf{x}^\top \mathbf{A}\mathbf{x} > 0
\quad
\text{para todo } \mathbf{x} \neq \mathbf{0}.
\]

Equivalentemente, todos os seus autovalores são positivos.

Essa propriedade possui consequências fundamentais:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\mathbf{A}\) é invertível;
\item
  a função \(\mathbf{x}^\top \mathbf{A}\mathbf{x}\) é estritamente
  convexa;
\item
  problemas de minimização associados possuem solução única.
\end{enumerate}

No modelo de regressão linear, a matriz

\[
\mathbf{X}^\top \mathbf{X}
\]

é simétrica e definida positiva se, e somente se, as colunas de
\(\mathbf{X}\) forem linearmente independentes. Isso equivale à condição

\[
\operatorname{rank}(\mathbf{X}) = p+1.
\]

Quando essa condição é satisfeita, o estimador de mínimos quadrados

\[
\hat{\boldsymbol{\beta}}
=
(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{Y}
\]

existe e é único.

Se \(\mathbf{X}^\top \mathbf{X}\) não for definida positiva, o problema
de estimação perde identificabilidade, caracterizando multicolinearidade
perfeita.

A compreensão dessas estruturas matriciais permite interpretar o modelo
linear como:

\begin{itemize}
\tightlist
\item
  um problema geométrico de projeção em subespaços;
\item
  um problema analítico de minimização convexa;
\item
  um problema espectral envolvendo autovalores e autovetores.
\end{itemize}

Essas perspectivas convergem na teoria de estimação, inferência e
diagnóstico.

\section{Estrutura Geométrica do Modelo
Linear}\label{estrutura-geomuxe9trica-do-modelo-linear}

Considere o modelo linear múltiplo em notação matricial:

\[
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon},
\]

em que

\begin{itemize}
\tightlist
\item
  \(\mathbf{Y} \in \mathbb{R}^n\) é o vetor de respostas observadas,
\item
  \(\mathbf{X} \in \mathbb{R}^{n \times (p+1)}\) é a matriz de
  planejamento,
\item
  \(\boldsymbol{\beta} \in \mathbb{R}^{p+1}\) é o vetor de parâmetros,
\item
  \(\boldsymbol{\varepsilon} \in \mathbb{R}^n\) é o vetor de erros.
\end{itemize}

Explicitamente, pode-se escrever

\[
\mathbf{X}
=
\begin{bmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1p} \\
1 & x_{21} & x_{22} & \cdots & x_{2p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{np}
\end{bmatrix},
\qquad
\boldsymbol{\beta}
=
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_p
\end{bmatrix}.
\]

Cada coluna de \(\mathbf{X}\) é um vetor em \(\mathbb{R}^n\). Assim,
\(\mathbf{X}\) pode ser vista como um conjunto de \(p+1\) vetores que
geram um subespaço de \(\mathbb{R}^n\).

\subsection{Espaço Coluna e
Identificabilidade}\label{espauxe7o-coluna-e-identificabilidade}

O \textbf{espaço coluna} de \(\mathbf{X}\) é definido como

\[
\mathrm{col}(\mathbf{X})
=
\left\{
\mathbf{X}\boldsymbol{\beta}
:
\boldsymbol{\beta} \in \mathbb{R}^{p+1}
\right\}.
\]

Esse conjunto é um subespaço vetorial de \(\mathbb{R}^n\), cujo posto é

\[
\operatorname{rank}(\mathbf{X}) \leq p+1.
\]

Se as colunas de \(\mathbf{X}\) forem linearmente independentes, então

\[
\operatorname{rank}(\mathbf{X}) = p+1,
\]

e o espaço coluna tem dimensão \(p+1\).

Essa condição é equivalente à positividade definida de
\(\mathbf{X}^\top \mathbf{X}\) e garante a identificabilidade única dos
parâmetros.

\subsection{O Problema de Mínimos Quadrados como Problema de
Projeção}\label{o-problema-de-muxednimos-quadrados-como-problema-de-projeuxe7uxe3o}

O estimador de mínimos quadrados é definido como a solução do problema

\[
\min_{\boldsymbol{\beta}}
\|\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}\|^2.
\]

Geometricamente, isso significa encontrar o vetor em
\(\mathrm{col}(\mathbf{X})\) que esteja mais próximo de \(\mathbf{Y}\)
na métrica euclidiana.

Seja

\[
\hat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\beta}}.
\]

Então

\[
\hat{\mathbf{Y}} \in \mathrm{col}(\mathbf{X})
\]

e

\[
\mathbf{Y} - \hat{\mathbf{Y}}
\perp
\mathrm{col}(\mathbf{X}).
\]

Ou seja,

\[
\mathbf{X}^\top(\mathbf{Y} - \hat{\mathbf{Y}}) = \mathbf{0}.
\]

Essa condição é exatamente a forma matricial das equações normais.

\subsection{Interpretação Ortogonal e Soma
Direta}\label{interpretauxe7uxe3o-ortogonal-e-soma-direta}

Seja \(V = \mathbb{R}^n\) equipado com o produto interno usual \[
\langle \mathbf{x}, \mathbf{y} \rangle = \mathbf{x}^\top \mathbf{y}.
\]

Se \(S\) é um subespaço de \(\mathbb{R}^n\), define-se seu complemento
ortogonal como

\[
S^\perp
=
\left\{
\mathbf{z} \in \mathbb{R}^n :
\mathbf{z}^\top \mathbf{s} = 0
\ \text{para todo } \mathbf{s} \in S
\right\}.
\]

Diz-se que \(\mathbb{R}^n\) é a \textbf{soma direta ortogonal} de dois
subespaços \(S\) e \(T\) se:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  todo vetor de \(\mathbb{R}^n\) pode ser escrito como soma de um vetor
  em \(S\) e um vetor em \(T\);
\item
  essa decomposição é única;
\item
  \(S\) e \(T\) são ortogonais, isto é,
  \(\mathbf{s}^\top \mathbf{t} = 0\) para todo \(\mathbf{s}\in S\) e
  \(\mathbf{t}\in T\).
\end{enumerate}

Nessa situação escreve-se

\[
\mathbb{R}^n = S \oplus T,
\]

em que o símbolo \(\oplus\) indica soma direta.

No contexto do modelo linear, tomando

\[
S = \mathrm{col}(\mathbf{X}),
\qquad
T = \mathrm{col}(\mathbf{X})^\perp,
\]

obtém-se

\[
\mathbb{R}^n
=
\mathrm{col}(\mathbf{X})
\oplus
\mathrm{col}(\mathbf{X})^\perp.
\]

Isso significa que qualquer vetor \(\mathbf{Y} \in \mathbb{R}^n\) pode
ser decomposto de maneira única como

\[
\mathbf{Y}
=
\hat{\mathbf{Y}}
+
\hat{\boldsymbol{\varepsilon}},
\]

onde

\begin{itemize}
\tightlist
\item
  \(\hat{\mathbf{Y}} \in \mathrm{col}(\mathbf{X})\),
\item
  \(\hat{\boldsymbol{\varepsilon}} \in \mathrm{col}(\mathbf{X})^\perp\).
\end{itemize}

A ortogonalidade implica

\[
\hat{\mathbf{Y}}^\top \hat{\boldsymbol{\varepsilon}} = 0,
\]

ou, equivalentemente,

\[
\mathbf{X}^\top \hat{\boldsymbol{\varepsilon}} = \mathbf{0}.
\]

Essa decomposição é puramente geométrica e independe de qualquer
hipótese probabilística sobre os erros. Ela constitui o núcleo
estrutural do método de mínimos quadrados e fundamenta:

\begin{itemize}
\tightlist
\item
  a decomposição da soma de quadrados total;
\item
  a contagem de graus de liberdade;
\item
  a independência entre componentes projetadas sob normalidade.
\end{itemize}

Assim, o modelo linear pode ser interpretado como a decomposição
ortogonal do vetor de respostas em duas componentes pertencentes a
subespaços complementares.

\subsection{Relação com Posto e
Dimensão}\label{relauxe7uxe3o-com-posto-e-dimensuxe3o}

Se \(\operatorname{rank}(\mathbf{X}) = r\), então:

\begin{itemize}
\tightlist
\item
  \(\dim(\mathrm{col}(\mathbf{X})) = r\),
\item
  \(\dim(\mathrm{col}(\mathbf{X})^\perp) = n - r\).
\end{itemize}

No modelo linear completo com intercepto e colunas independentes,

\[
r = p+1,
\]

e, portanto, o espaço residual tem dimensão

\[
n - (p+1).
\]

Essa contagem de dimensões será reinterpretada mais adiante como graus
de liberdade na decomposição das somas de quadrados.

\subsection{Conexão com
Diagnóstico}\label{conexuxe3o-com-diagnuxf3stico}

A estrutura geométrica permite compreender diversos elementos de
diagnóstico:

\begin{itemize}
\tightlist
\item
  Vetores de alta alavancagem correspondem a observações cuja projeção
  sobre \(\mathrm{col}(\mathbf{X})\) é dominante.
\item
  Resíduos grandes correspondem a componentes significativas no
  subespaço ortogonal.
\item
  A decomposição da variabilidade total decorre da ortogonalidade entre
  componentes projetadas.
\end{itemize}

O modelo linear é, portato, uma decomposição geométrica do vetor de
respostas em dois componentes ortogonais.

\section{Matrizes de Projeção e Decomposição
Ortogonal}\label{matrizes-de-projeuxe7uxe3o-e-decomposiuxe7uxe3o-ortogonal}

A matriz de projeção associada ao modelo linear múltiplo é definida por

\[
\mathbf{H}
=
\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top.
\]

Essa matriz desempenha papel relevante na teoria da regressão, pois
formaliza algebricamente a projeção ortogonal sobre o subespaço
\(\mathrm{col}(\mathbf{X})\).

\subsubsection{Verificação das Propriedades
Estruturais}\label{verificauxe7uxe3o-das-propriedades-estruturais}

\textbf{Simetria}

\[
\mathbf{H}^\top
=
\left[
\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top
\right]^\top
=
\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top
=
\mathbf{H}.
\]

Utilizou-se o fato de que \(\mathbf{X}^\top \mathbf{X}\) é simétrica e
que a transposta de um produto inverte a ordem dos fatores.

\textbf{Idempotência}

\[
\mathbf{H}^2
=
\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top
\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top.
\]

Como

\[
\mathbf{X}^\top \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}
=
\mathbf{I},
\]

segue que

\[
\mathbf{H}^2
=
\mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top
=
\mathbf{H}.
\]

A idempotência caracteriza transformações que, uma vez aplicadas, não
alteram mais o vetor.

\subsubsection{Interpretação
Geométrica}\label{interpretauxe7uxe3o-geomuxe9trica}

Para qualquer vetor \(\mathbf{y} \in \mathbb{R}^n\),

\[
\mathbf{H}\mathbf{y}
\in
\mathrm{col}(\mathbf{X}).
\]

Além disso,

\[
\mathbf{y} - \mathbf{H}\mathbf{y}
\in
\mathrm{col}(\mathbf{X})^\perp.
\]

Portanto,

\[
\mathbf{y}
=
\mathbf{H}\mathbf{y}
+
(\mathbf{I}-\mathbf{H})\mathbf{y}.
\]

Definindo

\[
\mathbf{M} = \mathbf{I}_n - \mathbf{H},
\]

obtém-se a projeção complementar sobre o subespaço ortogonal.

\subsubsection{Propriedades da Matriz
Residual}\label{propriedades-da-matriz-residual}

A matriz

\[
\mathbf{M} = \mathbf{I}_n - \mathbf{H}
\]

satisfaz:

\begin{itemize}
\tightlist
\item
  \(\mathbf{M}^\top = \mathbf{M}\),
\item
  \(\mathbf{M}^2 = \mathbf{M}\),
\item
  \(\mathbf{H}\mathbf{M} = \mathbf{0}\),
\item
  \(\mathbf{M}\mathbf{H} = \mathbf{0}\).
\end{itemize}

Essas propriedades garantem que os subespaços são ortogonais e
complementares.

\subsubsection{Decomposição do Vetor de
Respostas}\label{decomposiuxe7uxe3o-do-vetor-de-respostas}

Aplicando as matrizes ao vetor \(\mathbf{Y}\):

\[
\mathbf{Y}
=
\mathbf{H}\mathbf{Y}
+
\mathbf{M}\mathbf{Y}
=
\hat{\mathbf{Y}}
+
\hat{\boldsymbol{\varepsilon}}.
\]

em que

\begin{itemize}
\tightlist
\item
  \(\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}\) pertence ao espaço coluna;
\item
  \(\hat{\boldsymbol{\varepsilon}} = \mathbf{M}\mathbf{Y}\) pertence ao
  complemento ortogonal.
\end{itemize}

A ortogonalidade implica

\[
\hat{\mathbf{Y}}^\top \hat{\boldsymbol{\varepsilon}} = 0.
\]

Essa identidade é a base da decomposição da soma de quadrados total.

\subsubsection{Traço, Posto e
Autovalores}\label{trauxe7o-posto-e-autovalores}

Para matrizes idempotentes e simétricas, os autovalores são apenas 0 ou
1.

Se

\[
\operatorname{rank}(\mathbf{X}) = p+1,
\]

então:

\begin{itemize}
\tightlist
\item
  \(\mathbf{H}\) possui \(p+1\) autovalores iguais a 1,
\item
  e \(n-(p+1)\) autovalores iguais a 0.
\end{itemize}

Assim,

\[
\mathrm{tr}(\mathbf{H}) = p+1.
\]

De forma análoga,

\[
\mathrm{tr}(\mathbf{M}) = n - p - 1.
\]

Como o traço de uma matriz idempotente simétrica coincide com seu posto,
essas quantidades correspondem às dimensões dos subespaços projetados
(ver Harville (1997)).

\subsubsection{Conexão com Somas de
Quadrados}\label{conexuxe3o-com-somas-de-quadrados}

A soma de quadrados ajustada pode ser escrita como

\[
\mathbf{Y}^\top \mathbf{H} \mathbf{Y}.
\]

A soma de quadrados residual é

\[
\mathbf{Y}^\top \mathbf{M} \mathbf{Y}.
\]

Como \(\mathbf{H}\) e \(\mathbf{M}\) projetam sobre subespaços
ortogonais,

\[
\mathbf{Y}^\top \mathbf{Y}
=
\mathbf{Y}^\top \mathbf{H} \mathbf{Y}
+
\mathbf{Y}^\top \mathbf{M} \mathbf{Y}.
\]

Essa identidade é puramente geométrica e antecede qualquer consideração
probabilística.

\subsubsection{Relação com
Diagnóstico}\label{relauxe7uxe3o-com-diagnuxf3stico}

A diagonal de \(\mathbf{H}\) contém as alavancagens:

\[
h_{ii} = (\mathbf{H})_{ii}.
\]

Essas quantidades medem o grau de influência estrutural da \(i\)-ésima
observação no ajuste, pois determinam o peso da projeção sobre o espaço
coluna (ver Weisberg (2005)).

Valores elevados de \(h_{ii}\) indicam observações que ocupam posições
extremas no espaço das covariáveis.

A matriz de projeção sintetiza, portanto, três dimensões fundamentais do
modelo linear:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estrutura geométrica (projeção ortogonal);
\item
  Estrutura algébrica (idempotência e posto);
\item
  Estrutura estatística (somas de quadrados e graus de liberdade).
\end{enumerate}

\section{Diferenciação Matricial e Estimação por Mínimos
Quadrados}\label{diferenciauxe7uxe3o-matricial-e-estimauxe7uxe3o-por-muxednimos-quadrados}

A estimação por mínimos quadrados consiste na minimização da soma de
quadrados dos resíduos,

\[
S(\boldsymbol{\beta}) 
=
\|\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}\|^2
=
(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})^\top
(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}).
\]

Essa função é uma forma quadrática em \(\boldsymbol{\beta}\). Para
compreender sua estrutura, é útil expandi-la explicitamente:

\[
S(\boldsymbol{\beta})
=
\mathbf{Y}^\top \mathbf{Y}
-
2\boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{Y}
+
\boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{X} \boldsymbol{\beta}.
\]

Observa-se que:

\begin{itemize}
\tightlist
\item
  \(\mathbf{Y}^\top \mathbf{Y}\) é constante em relação a
  \(\boldsymbol{\beta}\);
\item
  \(\boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{Y}\) é termo linear;
\item
  \(\boldsymbol{\beta}^\top \mathbf{X}^\top \mathbf{X} \boldsymbol{\beta}\)
  é forma quadrática.
\end{itemize}

A função objetivo é, portanto, um polinômio quadrático convexo sempre
que \(\mathbf{X}^\top \mathbf{X}\) for definida positiva.

\subsection{Derivadas Matriciais
Fundamentais}\label{derivadas-matriciais-fundamentais}

As identidades de cálculo matricial utilizadas a seguir são
sistematizadas em Abadir e Magnus (2005).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \[
  \frac{\partial (\mathbf{a}^\top \mathbf{x})}{\partial \mathbf{x}}
  =
  \mathbf{a}
  \]
\item
  \[
  \frac{\partial (\mathbf{x}^\top \mathbf{A}\mathbf{x})}{\partial \mathbf{x}}
  =
  (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}
  \]
\end{enumerate}

Em particular, se \(\mathbf{A}\) é simétrica,

\[
\frac{\partial (\mathbf{x}^\top \mathbf{A}\mathbf{x})}{\partial \mathbf{x}}
=
2\mathbf{A}\mathbf{x}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  \[
  \frac{\partial (\mathbf{x}^\top \mathbf{A}\mathbf{b})}{\partial \mathbf{x}}
  =
  \mathbf{A}^\top \mathbf{b}
  \]
\item
  \[
  \frac{\partial \mathrm{tr}(\mathbf{A}\mathbf{X})}{\partial \mathbf{X}}
  =
  \mathbf{A}^\top
  \]
\item
  \[
  \frac{\partial
  (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})^\top
  (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})
  }
  {\partial \boldsymbol{\beta}}
  =
  -2\mathbf{X}^\top(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}).
  \]
\end{enumerate}

\subsection{Derivação das Equações
Normais}\label{derivauxe7uxe3o-das-equauxe7uxf5es-normais}

Aplicando a derivada à função \(S(\boldsymbol{\beta})\) (ver Abadir e
Magnus (2005)):

\[
\nabla_{\boldsymbol{\beta}} S(\boldsymbol{\beta})
=
-2\mathbf{X}^\top(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}).
\]

Igualando o gradiente a zero:

\[
\mathbf{X}^\top(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}) = 0.
\]

Reorganizando,

\[
\mathbf{X}^\top \mathbf{X}\hat{\boldsymbol{\beta}}
=
\mathbf{X}^\top \mathbf{Y}.
\]

Essas são as \textbf{equações normais}.

Se \(\mathbf{X}^\top \mathbf{X}\) é invertível, isto é, se as colunas de
\(\mathbf{X}\) são linearmente independentes, obtém-se a solução única:

\[
\hat{\boldsymbol{\beta}}
=
(\mathbf{X}^\top \mathbf{X})^{-1}
\mathbf{X}^\top \mathbf{Y}.
\]

\subsection{Interpretação
Algébrica}\label{interpretauxe7uxe3o-alguxe9brica}

A condição

\[
\mathbf{X}^\top(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}})=0
\]

implica

\[
\mathbf{X}^\top \hat{\boldsymbol{\varepsilon}} = 0,
\]

ou seja, o vetor residual é ortogonal a cada coluna de \(\mathbf{X}\).

\subsection{Interpretação
Geométrica}\label{interpretauxe7uxe3o-geomuxe9trica-1}

A minimização de \(S(\boldsymbol{\beta})\) equivale a resolver

\[
\min_{\mathbf{v} \in \mathrm{col}(\mathbf{X})}
\|\mathbf{Y} - \mathbf{v}\|^2.
\]

Portanto,

\[
\hat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\beta}}
\]

é a projeção ortogonal de \(\mathbf{Y}\) sobre
\(\mathrm{col}(\mathbf{X})\). Essa caracterização independe de qualquer
hipótese probabilística.

\section{Estrutura Matricial para Diagnóstico e
Inferência}\label{estrutura-matricial-para-diagnuxf3stico-e-inferuxeancia}

A formulação matricial do modelo linear não se limita à obtenção do
estimador de mínimos quadrados. Ela estrutura integralmente os
procedimentos de diagnóstico e prepara o terreno para a inferência
estatística.

Recordemos a decomposição fundamental:

\[
\mathbf{Y}
=
\mathbf{H}\mathbf{Y}
+
\mathbf{M}\mathbf{Y}
=
\hat{\mathbf{Y}}
+
\hat{\boldsymbol{\varepsilon}}.
\]

Essa identidade organiza o vetor de respostas em duas componentes
ortogonais pertencentes a subespaços complementares.

\subsection{\texorpdfstring{Alavancagem e Estrutura da Matriz
\(\mathbf{H}\)}{Alavancagem e Estrutura da Matriz \textbackslash mathbf\{H\}}}\label{alavancagem-e-estrutura-da-matriz-mathbfh}

A diagonal da matriz de projeção

\[
h_{ii} = (\mathbf{H})_{ii}
\]

mensura o quanto a \(i\)-ésima observação contribui estruturalmente para
sua própria projeção.

Explicitamente,

\[
h_{ii}
=
\mathbf{x}_i^\top
(\mathbf{X}^\top \mathbf{X})^{-1}
\mathbf{x}_i,
\]

em que \(\mathbf{x}_i^\top\) é a \(i\)-ésima linha da matriz
\(\mathbf{X}\).

Propriedades fundamentais:

\begin{itemize}
\tightlist
\item
  \(0 \le h_{ii} \le 1\);
\item
  \(\sum_{i=1}^n h_{ii} = p+1\);
\item
  observações com valores elevados de \(h_{ii}\) ocupam posições
  extremas no espaço das covariáveis.
\end{itemize}

\subsection{Estrutura dos Resíduos}\label{estrutura-dos-resuxedduos}

Os resíduos podem ser escritos como

\[
\hat{\boldsymbol{\varepsilon}}
=
\mathbf{M}\mathbf{Y}.
\]

Como \(\mathbf{M}\) é simétrica e idempotente,

\[
\mathbf{M}^2 = \mathbf{M},
\qquad
\mathbf{M}^\top = \mathbf{M}.
\]

Além disso,

\[
\mathbf{X}^\top \hat{\boldsymbol{\varepsilon}} = \mathbf{0},
\]

o que garante que os resíduos são ortogonais às colunas de
\(\mathbf{X}\).

Essa ortogonalidade fundamenta:

\begin{itemize}
\tightlist
\item
  a decomposição das somas de quadrados;
\item
  a independência geométrica entre componentes ajustadas e residuais;
\item
  a contagem de graus de liberdade.
\end{itemize}

\subsection{Somas de Quadrados em Forma
Matricial}\label{somas-de-quadrados-em-forma-matricial}

A soma de quadrados total pode ser escrita como

\[
\mathbf{Y}^\top \mathbf{Y}.
\]

A soma de quadrados explicada pelo modelo é

\[
\mathbf{Y}^\top \mathbf{H} \mathbf{Y}.
\]

A soma de quadrados residual é

\[
\mathbf{Y}^\top \mathbf{M} \mathbf{Y}.
\]

Como \(\mathbf{H}\) e \(\mathbf{M}\) projetam sobre subespaços
ortogonais,

\[
\mathbf{Y}^\top \mathbf{Y}
=
\mathbf{Y}^\top \mathbf{H} \mathbf{Y}
+
\mathbf{Y}^\top \mathbf{M} \mathbf{Y}.
\]

Essa identidade é puramente algébrica e independe de qualquer suposição
probabilística.

\subsection{Postos e Graus de
Liberdade}\label{postos-e-graus-de-liberdade}

Como visto anteriormente,

\[
\mathrm{tr}(\mathbf{H}) = p+1,
\qquad
\mathrm{tr}(\mathbf{M}) = n - p - 1.
\]

Para matrizes simétricas idempotentes, o traço coincide com o posto.
Portanto:

\begin{itemize}
\tightlist
\item
  o subespaço ajustado tem dimensão \(p+1\);
\item
  o subespaço residual tem dimensão \(n-p-1\).
\end{itemize}

Essas dimensões serão reinterpretadas, no contexto probabilístico, como
graus de liberdade associados às somas de quadrados.

\chapter{Distribuição Normal}\label{distribuiuxe7uxe3o-normal}

Este apêndice tem um papel estrutural na fundamentação matemática dos
modelos de regressão linear. A Distribuição Normal, especialmente em sua
forma multivariada, fornece a base probabilística que torna possível
derivar distribuições amostrais exatas para estimadores, contrastes
lineares e estatísticas de teste em amostras finitas. Uma exposição
formal e sistemática dessas propriedades pode ser encontrada em Anderson
(2003) e Casella e Berger (2002).

A ideia central que deve acompanhar o leitor ao longo deste apêndice é a
seguinte:

\begin{quote}
Em regressão, não estudamos variáveis isoladas, mas vetores aleatórios e
suas transformações lineares e quadráticas.
\end{quote}

Essa perspectiva vetorial não é apenas notacional. Ela altera
profundamente a forma de pensar sobre variabilidade, dependência e
inferência.

\section{Distribuição Normal
Univariada}\label{distribuiuxe7uxe3o-normal-univariada}

Uma variável aleatória \(Y\) tem distribuição Normal univariada com
média \(\mu \in \mathbb{R}\) e variância \(\sigma^2 > 0\) se sua função
densidade (fpd) é

\[
f(y; \mu, \sigma^2)
=
\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\!\left(
-\frac{(y - \mu)^2}{2\sigma^2}
\right),
\quad y \in \mathbb{R}.
\] Essa distribuição surge com frequência em modelagem estatística
porque aparece como \textbf{distribuição limite} em muitos contextos,
especialmente quando uma quantidade observada pode ser representada como
a soma (ou média) de um grande número de contribuições aleatórias.

O \textbf{Teorema Central do Limite} estabelece que, sob condições
adequadas, a soma devidamente padronizada de variáveis aleatórias
independentes, ou fracamente dependentes, converge em distribuição para
a Normal, independentemente da forma das distribuições individuais; essa
fundamentação é essencialmente assintótica e aproximada, não
constituindo uma identidade estrutural exata. Uma formulação rigorosa
desse resultado pode ser consultada em Casella e Berger (2002).

No contexto de modelos de regressão, a suposição de Normalidade
\textbf{não é obrigatória} nem define o modelo em si. Um modelo de
regressão linear pode ser formulado sem qualquer hipótese distributiva
explícita sobre o erro, bastando condições sobre esperança, variância e
independência.

A Normalidade é frequentemente adotada porque constitui o \textbf{caso
mais simples e matematicamente tratável}, permitindo obter distribuições
exatas para estimadores, estatísticas de teste e intervalos de confiança
em amostras finitas. Em outros contextos, distribuições alternativas
podem ser mais adequadas, levando a extensões naturais da regressão
linear, como os modelos lineares generalizados.

Os parâmetros da Normal univariada admitem interpretações diretas, mas é
importante compreendê-las com precisão estatística.

O parâmetro \(\mu\) representa o \textbf{valor esperado teórico} da
variável aleatória \(Y\), isto é, o ponto em torno do qual a
distribuição se concentra em média. Trata-se de uma quantidade
populacional, definida independentemente de qualquer amostra específica,
e que resume a tendência central do fenômeno sob o modelo probabilístico
adotado.

O parâmetro \(\sigma^2\) representa a \textbf{variância populacional} da
variável aleatória, quantificando a dispersão em torno de \(\mu\). Essa
variabilidade reflete a incerteza inerente ao fenômeno modelado e não
carrega, nesse estágio, qualquer interpretação ligada a explicação ou
não explicação por covariáveis. Essa distinção só surgirá no contexto de
modelos condicionais, como a regressão.

Essas interpretações ficam claras ao observarmos duas propriedades
fundamentais da distribuição Normal:

\begin{itemize}
\item
  Esperança: \[
  \mathbb{E}[Y] = \mu
  \]
\item
  Variância: \[
  \mathrm{Var}(Y) = \sigma^2
  \]
\end{itemize}

Essas igualdades não são meras convenções, mas decorrem da integração
direta da densidade.

Uma característica estrutural importante da Normal é sua estabilidade
por transformações lineares. Se \(Y \sim N(\mu,\sigma^2)\) e definimos

\[
Z = aY + b,
\]

com \(a \neq 0\), então

\[
Z \sim N(a\mu + b, a^2\sigma^2).
\]

Essa propriedade, demonstrada em Casella e Berger (2002), é o primeiro
indício da importância da Normal na regressão: combinações lineares
preservam a forma distributiva.

Uma transformação particularmente importante, tanto do ponto de vista
teórico quanto prático, é a \textbf{padronização}. Definindo

\[
Z = \frac{Y - \mu}{\sigma},
\]

obtém-se uma nova variável aleatória com distribuição

\[
Z \sim N(0,1),
\]

conhecida como \textbf{Normal padrão}.

A padronização desempenha um papel central em inferência estatística
porque remove as unidades de medida e a escala original da variável,
permitindo comparar desvios em termos relativos. Em modelos de
regressão, essa ideia reaparece de forma sistemática: estatísticas de
teste, resíduos padronizados e intervalos de confiança são construídos a
partir de quantidades que mensuram desvios em relação a uma média
teórica, expressos em unidades de desvio-padrão.

Assim, compreender profundamente o significado de \(\mu\), \(\sigma^2\)
e da padronização é essencial para interpretar corretamente os
resultados inferenciais que surgirão nos modelos de regressão.

\section{Distribuição Normal
Bivariada}\label{distribuiuxe7uxe3o-normal-bivariada}

Ao avançarmos para o caso bivariado, deixamos de estudar variáveis
aleatórias isoladas e passamos a lidar explicitamente com
\textbf{dependência entre variáveis aleatórias}. Esse é um passo
conceitual fundamental, pois modelos estatísticos mais complexos e
abrangentes, incluindo os modelos de regressão, são construídos
exatamente a partir de relações entre variáveis.

Considere o vetor aleatório \[
\mathbf{Y} = (Y_1, Y_2)^\top.
\]

Dizemos que \(\mathbf{Y}\) segue uma \textbf{Distribuição Normal
bivariada} se

\[
\mathbf{Y} \sim N_2(\boldsymbol{\mu}, \boldsymbol{\Sigma}),
\]

onde o vetor de médias é dado por

\[
\boldsymbol{\mu} =
\begin{bmatrix}
\mu_1 \\
\mu_2
\end{bmatrix},
\]

e a matriz de covariância é

\[
\boldsymbol{\Sigma} =
\begin{bmatrix}
\sigma_1^2 & \rho\,\sigma_1\sigma_2 \\
\rho\,\sigma_1\sigma_2 & \sigma_2^2
\end{bmatrix}.
\]

Neste ponto ocorre uma mudança conceitual importante. Enquanto no caso
univariado a variância era um único número, agora a \textbf{matriz}
\(\boldsymbol{\Sigma}\) passa a concentrar toda a informação sobre
dispersão e dependência:

\begin{itemize}
\tightlist
\item
  os termos da diagonal (\(\sigma_1^2\) e \(\sigma_2^2\)) descrevem a
  variabilidade individual de cada componente;
\item
  os termos fora da diagonal descrevem a associação linear entre as
  variáveis, resumida pelo coeficiente de correlação \(\rho\).
\end{itemize}

Assim, a estrutura de dependência entre \(Y_1\) e \(Y_2\) não é um
elemento acessório, mas parte integrante da própria definição da
distribuição conjunta.

A função densidade de probabilidade conjunta, como apresentado em
Anderson (2003), pode ser escrita de forma compacta como

\[
f(\mathbf{y})
=
\frac{1}{2\pi |\boldsymbol{\Sigma}|^{1/2}}
\exp\!\left\{
-\frac{1}{2}
(\mathbf{y} - \boldsymbol{\mu})^\top
\boldsymbol{\Sigma}^{-1}
(\mathbf{y} - \boldsymbol{\mu})
\right\}.
\]

Essa expressão merece uma leitura cuidadosa. O termo que aparece no
expoente, \[
(\mathbf{y} - \boldsymbol{\mu})^\top
\boldsymbol{\Sigma}^{-1}
(\mathbf{y} - \boldsymbol{\mu}),
\] é um escalar obtido a partir de vetores e matrizes, resultado de uma
operação que combina transposição, multiplicação matricial e produto
interno.

Neste momento, \textbf{não é necessário compreender formalmente esse
termo como uma ``forma quadrática''} essa noção será estudada com
cuidado em um apêndice específico.

Intuitivamente, essa quantidade mede quão distante o vetor
\(\mathbf{y}\) está do centro \(\boldsymbol{\mu}\), mas \textbf{não
usando a distância euclidiana usual}. Em vez disso, a distância é
avaliada levando em conta a estrutura de variabilidade e dependência
entre as componentes do vetor, codificada na matriz de covariância
\(\boldsymbol{\Sigma}\).

Dessa forma, desvios ao longo de direções em que há maior variabilidade
conjunta são penalizados de maneira diferente de desvios ao longo de
direções com menor variabilidade. É essa ponderação que faz com que a
distribuição apresente contornos elípticos, em vez de circulares.

A formalização matemática desse tipo de expressão, bem como seu papel
central na regressão, nas somas de quadrados e nas estatísticas de
teste, será apresentada posteriormente, quando estudarmos explicitamente
as distribuições associadas a expressões desse tipo.

Geometricamente, isso se traduz no fato de que as curvas de mesma
densidade dessa distribuição são \textbf{elipses centradas em}
\(\boldsymbol{\mu}\).

A forma, o tamanho e a orientação dessas elipses dependem diretamente de
\(\boldsymbol{\Sigma}\):

\begin{itemize}
\tightlist
\item
  quando \(\rho = 0\), as elipses são alinhadas com os eixos
  coordenados;
\item
  quando \(\rho \neq 0\), as elipses tornam-se inclinadas, refletindo a
  associação linear entre \(Y_1\) e \(Y_2\).
\end{itemize}

Essa interpretação geométrica será essencial mais adiante, quando
discutirmos \textbf{projeções, decomposições ortogonais e ajuste de
modelos de regressão}, nos quais a ideia de ``direções relevantes'' no
espaço dos dados desempenha papel central.

Mesmo nesse cenário conjunto, algumas propriedades permanecem familiares
e ajudam a consolidar a intuição:

\begin{itemize}
\tightlist
\item
  As \textbf{distribuições marginais} continuam sendo Normais
  univariadas: \[
  Y_1 \sim N(\mu_1, \sigma_1^2),
  \qquad
  Y_2 \sim N(\mu_2, \sigma_2^2).
  \]
\end{itemize}

Essas marginais mostram que, marginalmente, cada componente do vetor se
comporta como uma variável Normal comum, mas isso \textbf{não elimina} a
possibilidade de dependência entre elas quando observadas conjuntamente.

\begin{itemize}
\tightlist
\item
  As \textbf{distribuições condicionais} também são Normais: \[
  Y_1 \mid Y_2 = y_2
  \sim
  N\!\left(
  \mu_1 + \rho\frac{\sigma_1}{\sigma_2}(y_2 - \mu_2),
  \,
  (1 - \rho^2)\sigma_1^2
  \right).
  \]
\end{itemize}

Aqui aparece uma ideia conceitualmente profunda e extremamente
importante para o que virá depois: \textbf{a média condicional de uma
variável Normal é uma função linear da variável condicionante}.

Essa linearidade não é um artifício do modelo, nem uma escolha
conveniente; ela é uma consequência direta da estrutura da Normalidade
conjunta. Em modelos de regressão, essa propriedade será reinterpretada
como a relação entre a resposta e as covariáveis, agora formulada de
maneira explícita e sistemática.

Portanto, compreender a Normal bivariada é compreender, em um cenário
simples, a origem probabilística da ideia de regressão como relação
média condicional.

\section{Distribuição Normal
Multivariada}\label{distribuiuxe7uxe3o-normal-multivariada}

No caso geral, consideramos um vetor aleatório \[
\mathbf{Y} \in \mathbb{R}^n,
\] que segue uma \textbf{Distribuição Normal multivariada} se

\[
\mathbf{Y} \sim N_n(\boldsymbol{\mu}, \boldsymbol{\Sigma}),
\]

onde \(\boldsymbol{\mu}\) é o vetor de médias e \(\boldsymbol{\Sigma}\)
é a matriz de covariância, simétrica e definida positiva.

A função densidade associada é

\[
f(\mathbf{y})
=
\frac{1}{(2\pi)^{n/2} |\boldsymbol{\Sigma}|^{1/2}}
\exp\!\left\{
-\frac{1}{2}
(\mathbf{y} - \boldsymbol{\mu})^\top
\boldsymbol{\Sigma}^{-1}
(\mathbf{y} - \boldsymbol{\mu})
\right\}.
\]

Neste ponto, é importante fazer uma mudança consciente na forma de
pensar. Não estamos mais lidando com observações isoladas, mas com
\textbf{vetores aleatórios}, e a incerteza passa a ser descrita por
\textbf{estruturas geométricas em espaços de dimensão maior}.

O vetor \(\boldsymbol{\mu}\) representa o centro da distribuição no
espaço \(\mathbb{R}^n\), enquanto a matriz \(\boldsymbol{\Sigma}\)
determina como a variabilidade se organiza em torno desse centro. Mais
especificamente, \(\boldsymbol{\Sigma}\) define:

\begin{itemize}
\tightlist
\item
  direções ao longo das quais a variabilidade conjunta é maior;
\item
  direções ao longo das quais a variabilidade conjunta é menor;
\item
  dependências lineares entre as componentes do vetor.
\end{itemize}

Essas direções não precisam coincidir com os eixos coordenados
originais, e essa observação será fundamental quando discutirmos
projeções e decomposições em regressão múltipla.

A expressão que aparece no expoente da densidade envolve novamente uma
quantidade do tipo

\[
(\mathbf{y} - \boldsymbol{\mu})^\top
\boldsymbol{\Sigma}^{-1}
(\mathbf{y} - \boldsymbol{\mu}),
\]

que produz um escalar a partir de vetores e matrizes. Assim como no caso
bivariado, \textbf{não é necessário, neste momento, compreender
formalmente essa expressão como uma forma quadrática}. Por ora, basta
interpretar essa quantidade como uma medida de distância multivariada
entre \(\mathbf{y}\) e o centro \(\boldsymbol{\mu}\), ajustada pela
estrutura de covariância.

Essa forma de medir distância explica por que as regiões de maior
densidade da Normal multivariada são elipsoides em \(\mathbb{R}^n\),
generalizando as elipses vistas no caso bivariado.

Algumas propriedades fundamentais seguem diretamente dessa definição e
merecem ser destacadas, pois reaparecerão continuamente ao longo do
estudo de modelos de regressão.

A esperança e a covariância do vetor aleatório são dadas por

\[
\mathbb{E}[\mathbf{Y}] = \boldsymbol{\mu},
\qquad
\mathrm{Cov}(\mathbf{Y}) = \boldsymbol{\Sigma}.
\]

Essas expressões formalizam a interpretação de \(\boldsymbol{\mu}\) como
centro da distribuição e de \(\boldsymbol{\Sigma}\) como descrição
completa da variabilidade conjunta.

Uma propriedade absolutamente central da Normal multivariada é sua
\textbf{estabilidade por transformações lineares}. Se tomarmos uma
transformação do tipo

\[
\mathbf{Z} = \mathbf{A}\mathbf{Y} + \mathbf{a},
\]

então a variável transformada também segue uma distribuição Normal
multivariada:

\[
\mathbf{Z} \sim
N_m(\mathbf{A}\boldsymbol{\mu} + \mathbf{a},\,
\mathbf{A}\boldsymbol{\Sigma}\mathbf{A}^\top).
\]

Essa propriedade merece atenção especial. Ela afirma que
\textbf{qualquer combinação linear de um vetor Normal multivariado
continua sendo Normal}, independentemente da dimensão envolvida.

Esse resultado será a pedra angular da teoria de regressão linear.
Quando estudarmos regressão, veremos que os estimadores dos
coeficientes, os valores ajustados e diversos contrastes estatísticos
são obtidos exatamente como transformações lineares do vetor de
respostas. A Normalidade dessas quantidades decorre diretamente desta
propriedade, e não de argumentos ad hoc.

Outra quantidade natural que surge no contexto da Normal multivariada é
a chamada \textbf{distância de Mahalanobis}:

\[
Q =
(\mathbf{Y} - \boldsymbol{\mu})^\top
\boldsymbol{\Sigma}^{-1}
(\mathbf{Y} - \boldsymbol{\mu}),
\]

para o qual vale

\[
Q \sim \chi^2_n.
\]

Mais uma vez, não é necessário aprofundar formalmente esse resultado
neste momento. Conceitualmente, ele afirma que a distância multivariada
entre \(\mathbf{Y}\) e seu centro, quando devidamente padronizada pela
matriz de covariância, possui uma distribuição conhecida.

Esse fato será explorado de forma sistemática em regressão, onde somas
de quadrados, estatísticas de teste e medidas de ajuste surgirão como
casos particulares desse tipo de expressão.

Assim, a Distribuição Normal multivariada fornece não apenas um modelo
probabilístico para vetores de dados, mas também a base matemática para
compreender por que as quantidades centrais da regressão admitem
distribuições explícitas e interpretáveis.

\section{Partição da Normal
Multivariada}\label{partiuxe7uxe3o-da-normal-multivariada}

Um dos recursos mais poderosos da Normal multivariada é a possibilidade
de \textbf{particionar o vetor aleatório} em blocos menores e ainda
assim manter uma descrição probabilística completa e explícita.

Considere o vetor aleatório particionado como

\[
\mathbf{Y} =
\begin{bmatrix}
\mathbf{Y}_1 \\
\mathbf{Y}_2
\end{bmatrix}
\sim
N_n\!\left(
\begin{bmatrix}
\boldsymbol{\mu}_1 \\
\boldsymbol{\mu}_2
\end{bmatrix},
\begin{bmatrix}
\boldsymbol{\Sigma}_{11} & \boldsymbol{\Sigma}_{12} \\
\boldsymbol{\Sigma}_{21} & \boldsymbol{\Sigma}_{22}
\end{bmatrix}
\right).
\]

Aqui, a partição é puramente conceitual: estamos apenas reorganizando o
vetor em dois blocos, sem alterar o modelo probabilístico subjacente.
Ainda assim, essa simples reorganização permite responder a perguntas
fundamentais sobre o comportamento do vetor aleatório.

Em particular, ela nos permite distinguir claramente dois tipos de
informação:

\begin{itemize}
\tightlist
\item
  \textbf{comportamento marginal}, isto é, como cada bloco se distribui
  quando considerado isoladamente;
\item
  \textbf{comportamento condicional}, isto é, como um bloco se distribui
  quando o outro é observado.
\end{itemize}

As distribuições marginais seguem diretamente da definição da Normal
multivariada:

\[
\mathbf{Y}_1 \sim N_{n_1}(\boldsymbol{\mu}_1, \boldsymbol{\Sigma}_{11}),
\qquad
\mathbf{Y}_2 \sim N_{n_2}(\boldsymbol{\mu}_2, \boldsymbol{\Sigma}_{22}).
\]

Essas expressões mostram que, ao ``olharmos apenas para uma parte do
vetor'', o comportamento probabilístico dessa parte continua sendo
Normal, com média e covariância correspondentes aos blocos apropriados
de \(\boldsymbol{\mu}\) e \(\boldsymbol{\Sigma}\) (Anderson (2003);
Casella e Berger (2002)). No entanto, essa visão marginal ignora
completamente a dependência entre os blocos.

A riqueza da Normal multivariada aparece de forma ainda mais clara ao
analisarmos o comportamento \textbf{condicional}. A distribuição de
\(\mathbf{Y}_1\) dado que \(\mathbf{Y}_2 = \mathbf{y}_2\) é

\[
\mathbf{Y}_1 \mid \mathbf{Y}_2 = \mathbf{y}_2
\sim
N_{n_1}\!\left(
\boldsymbol{\mu}_1 +
\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}
(\mathbf{y}_2 - \boldsymbol{\mu}_2),
\,
\boldsymbol{\Sigma}_{11} -
\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}
\boldsymbol{\Sigma}_{21}
\right).
\]

Essa expressão concentra vários conceitos importantes em um único
resultado.

Primeiro, observe que a \textbf{média condicional} de \(\mathbf{Y}_1\)
não é simplesmente \(\boldsymbol{\mu}_1\). Ela é ajustada pelo termo \[
\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}
(\mathbf{y}_2 - \boldsymbol{\mu}_2),
\] que incorpora a informação trazida pela observação de
\(\mathbf{Y}_2\). Esse ajuste depende exclusivamente da estrutura de
covariância entre os blocos, e não de escolhas arbitrárias de modelagem.

Segundo, note que a \textbf{matriz de covariância condicional} \[
\boldsymbol{\Sigma}_{11} -
\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}
\boldsymbol{\Sigma}_{21}
\] é sempre menor, no sentido de variância, do que a covariância
marginal \(\boldsymbol{\Sigma}_{11}\). Isso formaliza matematicamente
uma ideia intuitiva: \textbf{ao observar parte do vetor, reduzimos a
incerteza sobre o restante}.

Esse resultado mostra que, na Normal multivariada, o condicionamento
produz dois efeitos simultâneos e bem definidos:

\begin{itemize}
\item
  a média é deslocada de forma linear em função da parte observada;
\item
  a variabilidade é reduzida de maneira controlada pela estrutura de
  dependência.
\end{itemize}

Essas duas propriedades; linearidade da média condicional e redução da
variância, não são hipóteses adicionais nem aproximações, elas são
consequências diretas da Normalidade conjunta.

Embora ainda não estejamos estudando modelos de regressão, é importante
registrar que essa lógica será reinterpretada mais adiante quando os
coeficientes de um modelo passarem a ser entendidos como \textbf{efeitos
condicionais}, isto é, como variações esperadas em uma componente do
vetor quando outras são mantidas fixas.

Assim, a partição da Normal multivariada fornece o arcabouço
probabilístico que sustenta a noção de regressão como estudo de relações
condicionais, mesmo antes de qualquer equação de regressão ser escrita
explicitamente.

\section{Covariância zero implica
independência}\label{covariuxe2ncia-zero-implica-independuxeancia}

Em geral, para vetores aleatórios arbitrários, a condição de covariância
nula \textbf{não implica independência}. Isto é, pode ocorrer que duas
variáveis tenham covariância igual a zero e, ainda assim, sejam
dependentes.

Entretanto, a família Normal possui uma propriedade estrutural especial:

\begin{quote}
Se um vetor aleatório é Normal multivariado, então quaisquer componentes
(ou combinações lineares de componentes) que tenham covariância zero são
independentes.
\end{quote}

Mais precisamente, se \[
\mathbf{Y} \sim N_n(\boldsymbol{\mu}, \boldsymbol{\Sigma})
\] e \(\mathbf{a}, \mathbf{b} \in \mathbb{R}^n\), então \[
\mathrm{Cov}(\mathbf{a}^\top \mathbf{Y},\, \mathbf{b}^\top \mathbf{Y}) = 0
\quad \Longrightarrow \quad
\mathbf{a}^\top \mathbf{Y}
\text{ e }
\mathbf{b}^\top \mathbf{Y}
\text{ são independentes.}
\]

Essa propriedade é específica da distribuição Normal e não vale em geral
para outras distribuições multivariadas. Uma demonstração pode ser
encontrada em Anderson (2003) e em Casella e Berger (2002).

Essa característica será essencial na regressão linear, pois permite
concluir, sob Normalidade dos erros, que:

\begin{itemize}
\tightlist
\item
  o estimador dos coeficientes é independente do vetor de resíduos;
\item
  diferentes somas de quadrados associadas a projeções ortogonais são
  independentes;
\item
  estatísticas baseadas em decomposições ortogonais possuem
  distribuições independentes.
\end{itemize}

A independência decorre da ortogonalidade geométrica no espaço das
observações, combinada com a estrutura da Normal multivariada.

\subsection{Independência no caso
bivariado}\label{independuxeancia-no-caso-bivariado}

No caso particular bivariado, seja \[
\mathbf{Y} =
\begin{bmatrix}
Y_1 \\
Y_2
\end{bmatrix}
\sim N_2(\boldsymbol{\mu}, \boldsymbol{\Sigma}),
\] com \[
\boldsymbol{\Sigma} =
\begin{bmatrix}
\sigma_1^2 & \rho \sigma_1 \sigma_2 \\
\rho \sigma_1 \sigma_2 & \sigma_2^2
\end{bmatrix}.
\]

Então vale a equivalência:

\[
\rho = 0
\quad \Longleftrightarrow \quad
Y_1 \text{ e } Y_2 \text{ são independentes.}
\]

Essa equivalência é uma consequência direta da forma explícita da
densidade conjunta e constitui uma propriedade distintiva da Normal
bivariada. Em distribuições gerais, correlação zero não implica
independência.

\subsection{Caso marginal}\label{caso-marginal}

Se \[
\mathbf{Y} \sim N_n(\boldsymbol{\mu}, \boldsymbol{\Sigma}),
\] então qualquer subconjunto de componentes de \(\mathbf{Y}\) também
possui distribuição Normal multivariada.

Formalmente, se particionarmos \[
\mathbf{Y} =
\begin{bmatrix}
\mathbf{Y}_1 \\
\mathbf{Y}_2
\end{bmatrix},
\] então as distribuições marginais são

\[
\mathbf{Y}_1 \sim N_{n_1}(\boldsymbol{\mu}_1, \boldsymbol{\Sigma}_{11}),
\qquad
\mathbf{Y}_2 \sim N_{n_2}(\boldsymbol{\mu}_2, \boldsymbol{\Sigma}_{22}).
\]

Essa estabilidade marginal é consequência direta da definição da Normal
multivariada e pode ser verificada integrando-se a densidade conjunta ou
utilizando o resultado de que combinações lineares preservam
Normalidade.

\subsection{Partições e Independência entre
Blocos}\label{partiuxe7uxf5es-e-independuxeancia-entre-blocos}

Considere novamente a partição

\[
\mathbf{Y} =
\begin{bmatrix}
\mathbf{Y}_1 \\
\mathbf{Y}_2
\end{bmatrix}
\sim
N_n\!\left(
\begin{bmatrix}
\boldsymbol{\mu}_1 \\
\boldsymbol{\mu}_2
\end{bmatrix},
\begin{bmatrix}
\boldsymbol{\Sigma}_{11} & \boldsymbol{\Sigma}_{12} \\
\boldsymbol{\Sigma}_{21} & \boldsymbol{\Sigma}_{22}
\end{bmatrix}
\right).
\]

Então:

\[
\boldsymbol{\Sigma}_{12} = \mathbf{0}
\quad \Longleftrightarrow \quad
\mathbf{Y}_1 \text{ e } \mathbf{Y}_2 \text{ são independentes.}
\]

Isto é, na Normal multivariada, blocos são independentes se, e somente
se, sua matriz de covariância cruzada for nula.

Essa propriedade tem papel central na teoria da regressão linear
clássica. Quando se demonstra que duas quantidades são obtidas por
projeções ortogonais e que a matriz de covariância cruzada entre elas é
nula, a Normalidade garante automaticamente independência.

Essa combinação entre:

\begin{itemize}
\tightlist
\item
  ortogonalidade algébrica,
\item
  covariância nula,
\item
  estrutura Normal,
\end{itemize}

é o mecanismo matemático que sustenta a independência entre soma de
quadrados do modelo e soma de quadrados do erro, fundamento da
estatística \(F\).

\section{Papel da Distribuição Normal na fundamentação dos modelos de
regressão}\label{papel-da-distribuiuxe7uxe3o-normal-na-fundamentauxe7uxe3o-dos-modelos-de-regressuxe3o}

Os resultados apresentados neste apêndice fornecem uma base
probabilística razoável para a formulação e a análise dos modelos
clássicos de regressão linear. O objetivo aqui é explicitar as
estruturas matemáticas que a tornam analisável de forma rigorosa.

Em modelos de regressão linear com erros normalmente distribuídos,
considera-se que o vetor de respostas pode ser escrito como

\[
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon},
\qquad
\boldsymbol{\varepsilon} \sim N_n(\mathbf{0}, \sigma^2\mathbf{I}_n),
\]

o que implica diretamente que

\[
\mathbf{Y} \sim N_n(\mathbf{X}\boldsymbol{\beta}, \sigma^2\mathbf{I}_n).
\]

como discutido em Kutner et al. (2005). Essa especificação não define o
modelo de regressão em si, que pode ser formulado sob hipóteses mais
gerais, mas estabelece um \textbf{caso fundamental} no qual resultados
exatos de inferência podem ser obtidos em amostras finitas.

A partir dessa estrutura probabilística decorrem, de forma sistemática,
várias propriedades centrais da regressão linear clássica:

\begin{itemize}
\item
  os estimadores dos coeficientes surgem como \textbf{transformações
  lineares} do vetor aleatório \(\mathbf{Y}\);
\item
  os resíduos e as somas de quadrados associadas ao ajuste do modelo
  surgem como \textbf{expressões quadráticas} em \(\mathbf{Y}\);
\item
  as distribuições amostrais das estatísticas utilizadas para inferência
  são obtidas a partir das distribuições dessas transformações lineares
  e quadráticas.
\end{itemize}

Deste modo, estatísticas do tipo \(t\) e \(F\) não são introduzidas de
maneira ad hoc, mas emergem naturalmente da combinação entre a Normal
multivariada e as operações algébricas realizadas sobre o vetor de
respostas.

Outros apêndices exploram explicitamente essas estruturas, estudando as
distribuições associadas a transformações lineares e quadráticas de
vetores aleatórios conjuntamente normais. Esse desenvolvimento permitirá
compreender, de forma unificada, a origem das principais ferramentas
inferenciais utilizadas em modelos de regressão linear.

\chapter{Formas Lineares e Quadráticas na Normal
Multivariada}\label{formas-lineares-e-quadruxe1ticas-na-normal-multivariada}

Em regressão linear clássica, muitos estimadores e estatísticas
fundamentais são \textbf{formas lineares} e \textbf{formas quadráticas}
de um vetor aleatório Normal multivariado. Essa conexão não é meramente
técnica: ela é estrutural e explica por que distribuições exatas em
amostras finitas podem ser obtidas sob normalidade. A fundamentação
matricial e geométrica desses resultados pode ser encontrada, em
diferentes níveis de formalidade, em Harville (1997) e Anderson (2003).

\section{Preliminares: Normal multivariada e transformações
lineares}\label{preliminares-normal-multivariada-e-transformauxe7uxf5es-lineares}

Seja \[
\mathbf{Y}\sim N_n(\boldsymbol{\mu},\boldsymbol{\Sigma}),
\] com \(\boldsymbol{\Sigma}\) simétrica definida positiva.

Uma propriedade central da Normal multivariada é sua
\textbf{estabilidade por transformações lineares}: se \(\mathbf{A}\) é
uma matriz fixa \(m\times n\) e \(\mathbf{a}\in\mathbb{R}^m\), então \[
\mathbf{Z}=\mathbf{A}\mathbf{Y}+\mathbf{a}
\sim
N_m(\mathbf{A}\boldsymbol{\mu}+\mathbf{a},\, \mathbf{A}\boldsymbol{\Sigma}\mathbf{A}^\top).
\] Esse resultado é apresentado de forma sistemática em Anderson (2003)
e constitui a base probabilística da teoria da regressão linear.

Como caso particular, para um vetor fixo \(\mathbf{c}\in\mathbb{R}^n\),
\[
L=\mathbf{c}^\top\mathbf{Y}
\sim
N\!\left(\mathbf{c}^\top\boldsymbol{\mu},\, \mathbf{c}^\top\boldsymbol{\Sigma}\mathbf{c}\right).
\] A dedução segue diretamente da propriedade anterior e também pode ser
vista como aplicação do fato de que combinações lineares de vetores
Normais permanecem Normais, como discutido em Casella e Berger (2002).

Essa estrutura será aplicada diretamente a:

\begin{itemize}
\tightlist
\item
  \(\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{Y}\);
\item
  contrastes \(\mathbf{C}\hat{\boldsymbol{\beta}}\);
\item
  predições lineares e combinações de coeficientes.
\end{itemize}

\section{\texorpdfstring{Padronização multivariada e redução ao caso
\(\mathbf{I}\)}{Padronização multivariada e redução ao caso \textbackslash mathbf\{I\}}}\label{padronizauxe7uxe3o-multivariada-e-reduuxe7uxe3o-ao-caso-mathbfi}

Para estudar formas quadráticas, é útil reduzir o problema ao caso
esférico.

Como \(\boldsymbol{\Sigma}\) é definida positiva, existe uma matriz
simétrica definida positiva \(\boldsymbol{\Sigma}^{1/2}\) tal que \[
\boldsymbol{\Sigma}^{1/2}\boldsymbol{\Sigma}^{1/2}=\boldsymbol{\Sigma},
\qquad
(\boldsymbol{\Sigma}^{1/2})^{-1}=\boldsymbol{\Sigma}^{-1/2}.
\] A existência dessa raiz quadrada decorre da decomposição espectral de
matrizes simétricas definidas positivas, tratada em detalhe em Harville
(1997).

Defina \[
\mathbf{Z}=\boldsymbol{\Sigma}^{-1/2}(\mathbf{Y}-\boldsymbol{\mu}).
\]

Então \[
\mathbf{Z}\sim N_n(\mathbf{0},\mathbf{I}_n).
\]

Essa transformação separa de maneira conceitualmente limpa os papéis de
média e dispersão, reduzindo a análise probabilística ao caso esférico.
Além disso, ela mostra que a chamada distância de Mahalanobis \[
(\mathbf{Y}-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\mathbf{Y}-\boldsymbol{\mu})
\] é simplesmente a norma euclidiana ao quadrado de um vetor Normal
padrão: \[
(\mathbf{Y}-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\mathbf{Y}-\boldsymbol{\mu})
=
\mathbf{Z}^\top\mathbf{Z}.
\] Essa ponte entre geometria (normas e projeções) e distribuição
(Qui-quadrado) é central na teoria da inferência sob normalidade.

\section{Forma linear: distribuição, interpretação e conexão com
regressão}\label{forma-linear-distribuiuxe7uxe3o-interpretauxe7uxe3o-e-conexuxe3o-com-regressuxe3o}

Uma \textbf{forma linear} de um vetor aleatório é uma expressão do tipo
\[
L=\mathbf{c}^\top\mathbf{Y},
\] onde \(\mathbf{c}\) é fixo.

Se \(\mathbf{Y}\sim N_n(\boldsymbol{\mu},\boldsymbol{\Sigma})\), então:

\begin{itemize}
\tightlist
\item
  \(L\) é Normal;
\item
  \(\mathbb{E}(L)=\mathbf{c}^\top\boldsymbol{\mu}\);
\item
  \(\mathrm{Var}(L)=\mathbf{c}^\top\boldsymbol{\Sigma}\mathbf{c}\).
\end{itemize}

Na regressão linear, quando se assume normalidade dos erros, tanto
\(\hat{\boldsymbol{\beta}}\) quanto qualquer contraste
\(\mathbf{C}\hat{\boldsymbol{\beta}}\) são formas lineares em
\(\mathbf{Y}\). Por isso, possuem distribuição Normal exata em amostras
finitas.

\section{Forma quadrática: definição e
interpretação}\label{forma-quadruxe1tica-definiuxe7uxe3o-e-interpretauxe7uxe3o}

Uma \textbf{forma quadrática} em \(\mathbf{Y}\) é uma expressão do tipo
\[
Q=\mathbf{Y}^\top\mathbf{A}\mathbf{Y},
\] onde \(\mathbf{A}\) é uma matriz fixa \(n\times n\).

Primeira observação fundamental: apenas a parte simétrica de
\(\mathbf{A}\) influencia \(Q\). De fato, \[
\mathbf{Y}^\top\mathbf{A}\mathbf{Y}
=
\mathbf{Y}^\top\left(\frac{\mathbf{A}+\mathbf{A}^\top}{2}\right)\mathbf{Y}.
\] Logo, pode-se assumir \(\mathbf{A}\) simétrica sem perda de
generalidade, fato discutido na literatura matricial estatística como em
Harville (1997).

Em regressão, as somas de quadrados são precisamente formas quadráticas:
\[
\mathrm{SQReg}=\mathbf{Y}^\top\mathbf{H}\mathbf{Y},
\qquad
\mathrm{SQRes}=\mathbf{Y}^\top\mathbf{M}\mathbf{Y}.
\]

O comportamento probabilístico de \(Q\) depende criticamente das
propriedades estruturais de \(\mathbf{A}\). Quando \(\mathbf{A}\) é uma
projeção ortogonal (simétrica e idempotente), a forma quadrática se
reduz a uma soma de quadrados de componentes Normais padrão em um
subespaço.

\section{\texorpdfstring{O caso central: \(\mathbf{Z}^\top\mathbf{Z}\) e
a distribuição
\(\chi^2\)}{O caso central: \textbackslash mathbf\{Z\}\^{}\textbackslash top\textbackslash mathbf\{Z\} e a distribuição \textbackslash chi\^{}2}}\label{o-caso-central-mathbfztopmathbfz-e-a-distribuiuxe7uxe3o-chi2}

Se \(\mathbf{Z}\sim N_n(\mathbf{0},\mathbf{I}_n)\), então suas
componentes são independentes e \[
Z_i\sim N(0,1).
\]

Consequentemente, \[
\mathbf{Z}^\top\mathbf{Z}
=
\sum_{i=1}^n Z_i^2
\sim
\chi^2_n.
\]

Esse resultado é a base de todas as somas de quadrados na regressão sob
normalidade, como apresentado em textos de inferência como Casella e
Berger (2002).

\section{Projeções ortogonais e
Qui-quadrado}\label{projeuxe7uxf5es-ortogonais-e-qui-quadrado}

Seja \(\mathbf{Z}\sim N_n(\mathbf{0},\mathbf{I}_n)\) e seja
\(\mathbf{A}\) simétrica e idempotente: \[
\mathbf{A}^\top=\mathbf{A},
\qquad
\mathbf{A}^2=\mathbf{A}.
\]

Se \(r=\mathrm{rank}(\mathbf{A})=\mathrm{tr}(\mathbf{A})\), então \[
\mathbf{Z}^\top\mathbf{A}\mathbf{Z}\sim \chi^2_r.
\] Esse resultado pode ser deduzido via decomposição espectral de
\(\mathbf{A}\) e é tratado com rigor em Harville (1997) e Rencher e
Christensen (2012).

Geometricamente, \(\mathbf{A}\) projeta sobre um subespaço \(S\) de
dimensão \(r\). Assim, \(\mathbf{Z}^\top\mathbf{A}\mathbf{Z}\) mede o
comprimento ao quadrado da componente projetada de \(\mathbf{Z}\) em
\(S\), que se comporta como soma de quadrados de \(r\) Normais padrão.

Esse resultado aplica-se diretamente às matrizes \(\mathbf{H}\) (posto
\(p+1\)) e \(\mathbf{M}\) (posto \(n-p-1\)). Quando \(\mathbf{Z}\)
representa a versão padronizada do vetor de erros, as somas de quadrados
do modelo e do resíduo assumem distribuições Qui-quadrado com graus de
liberdade dados por esses postos.

\section{Independência via
ortogonalidade}\label{independuxeancia-via-ortogonalidade}

Se \(\mathbf{Z}\sim N_n(\mathbf{0},\mathbf{I}_n)\) e \(\mathbf{A}\),
\(\mathbf{B}\) são simétricas idempotentes tais que \[
\mathbf{A}\mathbf{B}=\mathbf{0},
\] então \[
\mathbf{Z}^\top\mathbf{A}\mathbf{Z}
\ \perp\
\mathbf{Z}^\top\mathbf{B}\mathbf{Z}.
\] A independência decorre da decomposição ortogonal do espaço combinada
com a simetria esférica da Normal padrão.

Critério análogo vale para independência entre forma linear
\(L=\mathbf{a}^\top\mathbf{Z}\) e forma quadrática
\(Q=\mathbf{Z}^\top\mathbf{A}\mathbf{Z}\): se \[
\mathbf{A}\mathbf{a}=\mathbf{0},
\] então \(L\) e \(Q\) são independentes.

\section{MRLM Normal}\label{mrlm-normal}

Considere o modelo de regressão linear múltipla sob normalidade e
homocedasticidade:

\[
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon},
\qquad
\boldsymbol{\varepsilon}\sim N_n(\mathbf{0},\sigma^2\mathbf{I}_n).
\]

Como consequência direta da estabilidade da Normal multivariada por
transformações lineares, tem-se

\[
\mathbf{Y}\sim N_n(\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I}_n).
\]

As quantidades centrais do modelo podem ser classificadas
estruturalmente da seguinte forma, destacando-se suas distribuições
quando há uma forma conhecida.

\subsection{\texorpdfstring{Estimador como forma linear em
\(\mathbf{Y}\)}{Estimador como forma linear em \textbackslash mathbf\{Y\}}}\label{estimador-como-forma-linear-em-mathbfy}

\[
\hat{\boldsymbol{\beta}}
=
(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{Y}.
\]

Trata-se de uma transformação linear do vetor aleatório \(\mathbf{Y}\).
Logo,

\[
\hat{\boldsymbol{\beta}}
\sim
N_{p+1}
\!\left(
\boldsymbol{\beta},
\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}
\right).
\]

Em particular, para cada componente \(\hat{\beta}_j\),

\[
\frac{\hat{\beta}_j-\beta_j}
{\sigma\sqrt{[(\mathbf{X}^\top\mathbf{X})^{-1}]_{jj}}}
\sim N(0,1).
\]

\subsection{Resíduos como transformação
linear}\label{resuxedduos-como-transformauxe7uxe3o-linear}

\[
\hat{\boldsymbol{\varepsilon}}
=
\mathbf{M}\mathbf{Y},
\qquad
\mathbf{M}=\mathbf{I}_n-\mathbf{H}.
\]

Como \(\hat{\boldsymbol{\varepsilon}}\) é transformação linear de um
vetor Normal multivariado, segue que

\[
\hat{\boldsymbol{\varepsilon}}
\sim
N_n\!\left(
\mathbf{M}\mathbf{X}\boldsymbol{\beta},
\,\sigma^2\mathbf{M}
\right).
\]

Como \(\mathbf{M}\mathbf{X}=\mathbf{0}\), obtém-se

\[
\hat{\boldsymbol{\varepsilon}}
\sim
N_n(\mathbf{0},\,\sigma^2\mathbf{M}).
\]

Assim, os resíduos possuem estrutura Normal degenerada em um subespaço
de dimensão \(n-p-1\).

\subsection{Soma de quadrados residual como forma
quadrática}\label{soma-de-quadrados-residual-como-forma-quadruxe1tica}

\[
\mathrm{SQRes}
=
\hat{\boldsymbol{\varepsilon}}^\top\hat{\boldsymbol{\varepsilon}}
=
\mathbf{Y}^\top\mathbf{M}\mathbf{Y}.
\]

Como \(\mathbf{M}\) é simétrica idempotente com

\[
\mathrm{rank}(\mathbf{M})=n-p-1,
\]

segue que, após padronização por \(\sigma^2\),

\[
\frac{\mathrm{SQRes}}{\sigma^2}
=
\frac{\mathbf{Y}^\top\mathbf{M}\mathbf{Y}}{\sigma^2}
\sim
\chi^2_{\,n-p-1}.
\]

Equivalentemente,

\[
\mathrm{SQRes}
\sim
\sigma^2\,\chi^2_{\,n-p-1}.
\]

Além disso, definindo

\[
\hat{\sigma}^2=\frac{\mathrm{SQRes}}{n-p-1},
\]

tem-se

\[
\frac{(n-p-1)\hat{\sigma}^2}{\sigma^2}
\sim
\chi^2_{\,n-p-1}.
\]

\subsection{Decomposição ortogonal da soma de quadrados
total}\label{decomposiuxe7uxe3o-ortogonal-da-soma-de-quadrados-total}

\[
\mathbf{Y}^\top\mathbf{Y}
=
\mathbf{Y}^\top\mathbf{H}\mathbf{Y}
+
\mathbf{Y}^\top\mathbf{M}\mathbf{Y}.
\]

Sob o modelo Normal, após padronização por \(\sigma^2\), as duas
parcelas tornam-se formas quadráticas associadas a projeções ortogonais
complementares.

Escrevendo \(\boldsymbol{\mu}=\mathbf{X}\boldsymbol{\beta}\), obtém-se:

\begin{itemize}
\tightlist
\item
  \textbf{Parte ajustada (forma quadrática não-central)}
\end{itemize}

\[
\frac{\mathbf{Y}^\top\mathbf{H}\mathbf{Y}}{\sigma^2}
\sim
\chi^2_{\,p+1}(\lambda),
\qquad
\lambda=\frac{\boldsymbol{\mu}^\top\boldsymbol{\mu}}{\sigma^2}.
\]

\begin{itemize}
\tightlist
\item
  \textbf{Parte residual (forma quadrática central)}
\end{itemize}

\[
\frac{\mathbf{Y}^\top\mathbf{M}\mathbf{Y}}{\sigma^2}
\sim
\chi^2_{\,n-p-1}.
\]

Como \(\mathbf{H}\mathbf{M}=\mathbf{0}\) e ambas são projeções
ortogonais, essas duas formas quadráticas são independentes:

\[
\frac{\mathbf{Y}^\top\mathbf{H}\mathbf{Y}}{\sigma^2}
\ \perp\
\frac{\mathbf{Y}^\top\mathbf{M}\mathbf{Y}}{\sigma^2}.
\]

\subsection{\texorpdfstring{Surgimento das distribuições \(t\) e
\(F\)}{Surgimento das distribuições t e F}}\label{surgimento-das-distribuiuxe7uxf5es-t-e-f}

A partir dessas distribuições, emergem naturalmente as estatísticas de
inferência.

Para cada coeficiente,

\[
T_j
=
\frac{\hat{\beta}_j-\beta_j}
{\hat{\sigma}\sqrt{[(\mathbf{X}^\top\mathbf{X})^{-1}]_{jj}}}
=
\frac{
\displaystyle
\frac{\hat{\beta}_j-\beta_j}
{\sigma\sqrt{[(\mathbf{X}^\top\mathbf{X})^{-1}]_{jj}}}
}
{
\sqrt{\frac{\mathrm{SQRes}/\sigma^2}{\,n-p-1\,}}
}
\sim t_{\,n-p-1}.
\]

A estatística \(t\) surge como razão entre uma Normal padrão e a raiz de
uma Qui-quadrado independente dividida por seus graus de liberdade.

De modo análogo, a estatística global de significância do modelo pode
ser escrita como

\[
F
=
\frac{
\left(\mathbf{Y}^\top\mathbf{H}\mathbf{Y}\right)/(p+1)
}{
\left(\mathbf{Y}^\top\mathbf{M}\mathbf{Y}\right)/(n-p-1)
}
\sim
F_{\,p+1,\;n-p-1},
\]

no caso central sob a hipótese nula apropriada.

Assim, as distribuições \(t\) e \(F\) não são introduzidas ad hoc: elas
emergem diretamente da estrutura geométrica das projeções ortogonais
combinada com a Normalidade multivariada do vetor de respostas.

\chapter{Tratamento de dados para regressão
(pré-modelagem)}\label{tratamento-de-dados-para-regressuxe3o-pruxe9-modelagem}

Este apêndice organiza, em forma de roteiro estruturado, os
procedimentos que devem ser realizados \textbf{antes} do ajuste de
qualquer modelo de regressão. O objetivo é preparar bases de dados
tabulares (em que linhas representam observações e colunas representam
variáveis) para que estejam \textbf{coerentes, consistentes, completas e
adequadas à modelagem estatística}.

Os métodos de regressão (lineares, generalizados ou penalizados)
\textbf{pressupõem} que a variabilidade observada nos dados represente o
fenômeno real sob estudo, e não erros de registro, inconsistências de
escala ou problemas de codificação. O tratamento pré-modelagem
constitui, portanto, o \textbf{primeiro passo do raciocínio estatístico
aplicado}: garantir que o modelo descreva o mundo observado e não
artefatos do processo de coleta ou organização dos dados.

\textbf{Princípios norteadores da preparação de dados}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Parcimônia:} trate o necessário, não o possível. Cada
  transformação modifica o significado estatístico das variáveis e pode
  alterar a interpretação dos resultados.
\item
  \textbf{Transparência:} todo procedimento deve ser reprodutível e
  documentado (log de alterações + dicionário de variáveis).
\item
  \textbf{Consistência:} mantenha coerência entre bases, períodos e
  versões (nomes de variáveis, unidades de medida, tipos e categorias).
\item
  \textbf{Domínio do contexto:} conheça o fenômeno estudado antes de
  decidir se determinado valor é erro, exceção legítima ou padrão
  relevante.
\item
  \textbf{Validação cruzada:} confirme decisões de tratamento por mais
  de uma ótica --- estatística e substantiva. (Iannone (2024))
\end{enumerate}

\textbf{Ferramentas e escopo}

\begin{itemize}
\tightlist
\item
  Trabalharemos com \textbf{R} (por exemplo, utilizando
  \texttt{tidyverse} ou \texttt{data.table}) e com \textbf{dados
  estruturados em formato tabular}.
\item
  \textbf{Não} abordaremos, neste apêndice, dados não estruturados ou
  semi-estruturados (texto livre, JSON heterogêneo, imagens, áudio ou
  bases documentais).
\item
  Quando utilizarmos expressões como ``detectar'', ``verificar'' ou
  ``avaliar'', estaremos nos referindo a \textbf{operações descritivas e
  exploratórias}, sem ajuste formal de modelos nesta etapa.
\end{itemize}

\textbf{Fluxo geral do processo analítico}

coleta → organização → tratamento → \textbf{(entregável: base tratada +
dicionário + log de decisões)} → modelagem → diagnóstico → interpretação

O tratamento de dados é, portanto, uma etapa intermediária e
estruturante, que conecta a coleta bruta à modelagem estatística.

\section{Exemplos do dia a dia e Checklist
rápido}\label{exemplos-do-dia-a-dia-e-checklist-ruxe1pido}

\textbf{Exemplos rápidos do dia a dia}

\begin{itemize}
\item
  \textbf{CSV que não abre ``corretamente''}: o arquivo foi salvo com
  \texttt{;} em vez de \texttt{,} como separador, e o decimal usa
  \texttt{,} (ex.: \texttt{3,14}). Resultado: números são interpretados
  como texto.\\
  \textbf{Tratamento}: especificar corretamente o delimitador e o
  símbolo decimal na leitura; padronizar separadores.
\item
  \textbf{Acentos e caracteres estranhos}: colunas aparecem como
  \texttt{ação}, \texttt{aÃ§Ã£o} ou nomes quebrados.\\
  \textbf{Tratamento}: ajustar o \texttt{encoding} (ex.: UTF-8 vs
  latin-1) e padronizar nomes de variáveis.
\item
  \textbf{Tipos incorretos}: idades lidas como texto (\texttt{"21"}),
  datas como strings (\texttt{"2025-10-24"}) e variáveis 0/1 lidas como
  numéricas quando deveriam ser categóricas. \textbf{Tratamento}:
  converter explicitamente para os tipos adequados (numérico, data/hora,
  fator) e validar o resultado.
\item
  \textbf{Códigos de ``faltante'' disfarçados}: valores como
  \texttt{999}, \texttt{-1}, \texttt{NA}, \texttt{""} representam
  ausência, mas estão misturados com dados válidos. (Wickham et al.
  (2024))\\
  \textbf{Tratamento}: recodificar para ausentes padronizados e decidir
  entre excluir, imputar ou manter com justificativa.
\item
  \textbf{Duplicatas e chaves quebradas}: a mesma unidade amostral
  aparece repetida; totais e médias ficam incorretos.\\
  \textbf{Tratamento}: identificar chaves únicas, remover ou conciliar
  duplicatas e documentar a decisão.
\item
  \textbf{Categorias inconsistentes}: \texttt{Masculino}, \texttt{M},
  \texttt{masc} e \texttt{male} aparecem como níveis distintos.\\
  \textbf{Tratamento}: unificar rótulos, definir categoria de referência
  substantiva e gerar dummies adequadamente.
\item
  \textbf{Datas em formatos mistos}: \texttt{31/12/2025} e
  \texttt{12-31-2025} na mesma coluna.\\
  \textbf{Tratamento}: normalizar formato, verificar timezone quando
  relevante e extrair componentes (ano/mês/dia) se necessário.
\item
  \textbf{Valores impossíveis}: \texttt{altura\ =\ -5},
  \texttt{proporção\ \textgreater{}\ 1}, \texttt{idade\ =\ 300}.\\
  \textbf{Tratamento}: definir faixas válidas segundo o domínio e
  corrigir ou descartar observações inconsistentes.
\item
  \textbf{Escalas heterogêneas}: receita em reais e custo em milhares de
  reais; área em m² e km².\\
  \textbf{Tratamento}: padronizar unidades ou aplicar
  reescala/padronização (ex.: z-score).
\item
  \textbf{Outliers evidentes}: uma observação muito superior às demais;
  zero estrutural inesperado.\\
  \textbf{Tratamento}: verificar plausibilidade no contexto, decidir
  entre correção, winsorização, transformação ou manutenção justificada.
\item
  \textbf{Resposta incompatível com o objetivo}: deseja-se regressão
  linear contínua, mas a variável resposta é binária ou contagem.\\
  \textbf{Tratamento}: verificar se o tipo de resposta é compatível com
  o modelo pretendido (linear, logístico, Poisson etc.).
\end{itemize}

Observe que, assim como a cozinha precisa estar organizada para a
receita ``dar certo'', a base precisa estar \textbf{coerente, completa e
padronizada} para que a modelagem produza resultados interpretáveis e
estatisticamente válidos.

\textbf{Checklist rápido: Faça antes de modelar}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Defina o objetivo e o tipo de resposta}: contínua, binária ou
  contagem; confirme que a variável resposta é compatível com o modelo
  pretendido.
\item
  \textbf{Garanta leitura correta}: verifique separador, encoding e
  símbolo decimal; confirme que variáveis numéricas não foram importadas
  como texto.
\item
  \textbf{Acerte tipos e unidades}: converta datas, inteiros e reais;
  padronize unidades e nomes de variáveis.
\item
  \textbf{Mapeie e trate dados faltantes}: identifique \texttt{NA},
  vazios e códigos artificiais (ex.: 999); decida entre excluir, imputar
  ou manter com justificativa. (Wickham et al. (2024))
\item
  \textbf{Elimine inconsistências}: remova ou una duplicatas, valide
  chaves, corrija valores impossíveis.
\item
  \textbf{Cuide de outliers}: identifique pontos extremos; corrija,
  transforme ou mantenha. Todas as ações com outliers devem ser
  realizadas com justificativa documentada.
\item
  \textbf{Codifique variáveis qualitativas}: unifique rótulos, defina
  categoria de referência e evite colinearidade perfeita na matriz de
  projeto.
\item
  \textbf{Cheque condições mínimas para regressão}:

  \begin{itemize}
  \tightlist
  \item
    Variabilidade das covariáveis;\\
  \item
    Ausência de colinearidade perfeita (\(X^\top X\) não singular);\\
  \item
    Número de observações maior que o número de parâmetros
    (\(n > p + 1\));\\
  \item
    Gere sumários e gráficos básicos;\\
  \item
    \textbf{Salve a versão tratada com dicionário de variáveis e log de
    decisões.}
  \end{itemize}
\end{enumerate}

\textbf{Cartões de decisão}

\begin{itemize}
\item
  \textbf{Faltantes:}\\
  Qual a proporção? A variável é essencial? Qual o mecanismo provável
  (MCAR/MAR/MNAR)? (Little e Rubin (2019))\\
  → excluir / imputar (média, mediana, por grupo, métodos múltiplos) /
  manter com justificativa. (Kuhn et al. (2024); Buuren e
  Groothuis-Oudshoorn (2024))
\item
  \textbf{Outliers:}\\
  Erro de digitação ou unidade? Valor plausível no domínio?\\
  → corrigir / winsorizar / transformar / manter e justificar.
\item
  \textbf{Categóricas:}\\
  Existem níveis raros (alta cardinalidade)?\\
  → agrupar em ``outros''; escolher referência substantiva; padronizar
  rótulos.
\item
  \textbf{Escalas:}\\
  Magnitudes muito diferentes entre covariáveis?\\
  → aplicar padronização (z-score) ou reescala; revisar unidades.
\item
  \textbf{Reprodutibilidade:}\\
  Toda decisão foi registrada no \textbf{log de tratamento}?
\end{itemize}

\#\#Leitura e organização dos dados

Ler corretamente um arquivo de dados (formato, delimitador, encoding,
símbolo decimal) é a primeira barreira contra erros que podem
comprometer toda a análise. Uma leitura incorreta pode fazer com que
números sejam interpretados como texto, datas como caracteres ou colunas
inteiras sejam deslocadas. Esses problemas, se não detectados no início,
propagam-se até a modelagem e podem invalidar inferências.

Em regressão, erros de tipagem e leitura afetam diretamente a construção
da matriz de projeto \(\mathbf{X}\) e da variável resposta
\(\mathbf{y}\). Portanto, esta etapa é estrutural, não meramente
operacional.

\textbf{Conexões com a literatura e boas práticas}

Autores como Charnet et al. (2008) e Sheather (2009) enfatizam que a
qualidade dos dados influencia diretamente as estimativas, testes e
intervalos de confiança. Essa etapa integra o que se denomina
\textbf{análise exploratória de dados (AED)}: conhecer o material
empírico antes de ajustar qualquer modelo.

Explorar não é modelar, é compreender a estrutura do dado.

\textbf{Funções úteis no R}

Pacotes e funções frequentemente utilizados nessa etapa incluem:

\begin{itemize}
\tightlist
\item
  \texttt{readr}: \texttt{read\_csv()}, \texttt{read\_delim()},
  \texttt{guess\_encoding()}, \texttt{locale()}
\item
  \texttt{readxl}: \texttt{read\_excel()}
\item
  \texttt{dplyr}: \texttt{glimpse()}, \texttt{summarise()},
  \texttt{count()}
\item
  \texttt{fs} e \texttt{here}: organização de diretórios e caminhos
  reproduzíveis
\item
  \texttt{vroom}: leitura rápida de arquivos grandes
\item
  \texttt{stringi}: diagnóstico e conversão de encoding
\end{itemize}

O objetivo não é ``usar funções'', mas garantir que a base esteja
corretamente interpretada pelo R antes de qualquer transformação.

\textbf{O que fazer}

\begin{itemize}
\tightlist
\item
  Mapear a \textbf{origem dos dados} (site, repositório, disciplina,
  experimento) e sua \textbf{licença de uso}.
\item
  Conferir o \textbf{formato do arquivo} (CSV, XLSX, SAV, DTA etc.) e o
  \textbf{delimitador} utilizado (\texttt{,}, \texttt{;}, tabulação).
\item
  Ajustar corretamente o \textbf{encoding} (UTF-8, latin-1) e o
  \textbf{símbolo decimal} (\texttt{,} ou \texttt{.}).
\item
  Definir um \textbf{esquema de pastas do projeto} (por exemplo:
  \texttt{dados\_brutos/}, \texttt{dados\_interinos/},
  \texttt{dados\_tratados/}) e convenções padronizadas de nomes.
\end{itemize}

\textbf{Boas práticas}

\begin{itemize}
\tightlist
\item
  Manter a pasta \texttt{dados\_brutos/} \textbf{imutável}. Nunca
  sobrescrever dados originais.
\item
  Realizar todas as modificações em cópias armazenadas em
  \texttt{dados\_tratados/}.
\item
  Padronizar nomes de colunas: minúsculas, sem acento, em formato
  \texttt{snake\_case}.
\item
  Criar um arquivo \texttt{README\_dados.md} com:

  \begin{itemize}
  \tightlist
  \item
    Fonte dos dados
  \item
    Data de download
  \item
    Responsável
  \item
    Descrição geral das variáveis
  \end{itemize}
\end{itemize}

\textbf{Erros comuns (e como evitar)}

\begin{itemize}
\tightlist
\item
  Arquivo CSV com \texttt{;} como separador lido como se fosse
  \texttt{,} → verificar explicitamente o delimitador.
\item
  Valores como \texttt{1,23} interpretados como texto → ajustar
  \texttt{locale(decimal\_mark\ =\ ",")} ou converter adequadamente.
\item
  Datas ambíguas (ex.: \texttt{01/02/2020}) → padronizar formato
  (preferencialmente ISO 8601: \texttt{2020-02-01}).
\item
  Colunas numéricas importadas como \texttt{character} → converter
  explicitamente e validar.
\end{itemize}

\textbf{Checklist rápido}

Antes de seguir para qualquer transformação ou modelagem, verifique:

\begin{itemize}
\tightlist
\item[$\square$]
  Colunas numéricas estão realmente no tipo numérico?
\item[$\square$]
  Datas foram reconhecidas como datas?
\item[$\square$]
  Há duplicatas segundo a chave da base?
\item[$\square$]
  Nomes de colunas estão padronizados?
\item[$\square$]
  A leitura preservou o número esperado de linhas e colunas?
\end{itemize}

Se qualquer resposta for negativa, o tratamento ainda não começou e a
leitura ainda não terminou.

\section{Estrutura e tipagem das
variáveis}\label{estrutura-e-tipagem-das-variuxe1veis}

A estrutura e o tipo das variáveis determinam como a matriz de projeto
\(\mathbf{X}\) será construída. Uma tipagem incorreta não é apenas um
erro técnico: ela altera o significado estatístico do modelo, pode
introduzir colinearidade artificial e comprometer estimativas, testes e
interpretações.

De acordo com Sheather (2009), a correta identificação do tipo de cada
variável é condição essencial para que o modelo represente adequadamente
a relação entre preditores e resposta. Uma variável categórica tratada
como numérica impõe uma estrutura inexistente; uma variável numérica
tratada como categórica multiplica desnecessariamente parâmetros.

Em regressão, a tipagem define como cada coluna entra em \(\mathbf{X}\):
como valor contínuo, como conjunto de dummies ou como transformação
temporal.

\textbf{Ferramentas R úteis}

Funções base do R:

\begin{itemize}
\tightlist
\item
  \texttt{str()} --- inspeciona a estrutura da base\\
\item
  \texttt{class()} --- verifica o tipo de objeto\\
\item
  \texttt{as.numeric()}, \texttt{as.integer()} --- conversões
  numéricas\\
\item
  \texttt{as.Date()} --- conversão de datas
\end{itemize}

Pacotes auxiliares:

\begin{itemize}
\tightlist
\item
  \texttt{lubridate}: \texttt{ymd()}, \texttt{dmy()},
  \texttt{ymd\_hms()}\\
\item
  \texttt{dplyr}: \texttt{mutate()}, \texttt{across()}\\
\item
  \texttt{forcats}: manipulação de fatores
\end{itemize}

Para validação: - \texttt{is.na()} --- valores ausentes\\
- \texttt{is.finite()} --- valores numéricos válidos\\
- Verificações de faixa (ex.: idade ≥ 0)

O objetivo não é apenas converter, mas \textbf{verificar se a conversão
preserva o significado substantivo da variável}.

\textbf{Classifique as variáveis}

Antes de qualquer modelagem, identifique claramente:

\begin{itemize}
\tightlist
\item
  Numéricas \textbf{contínuas} (salário, temperatura, peso)\\
\item
  Numéricas \textbf{discretas} (número de visitas, contagem de
  eventos)\\
\item
  \textbf{Binárias} (0/1, sim/não)\\
\item
  \textbf{Categóricas nominais} (sexo, região)\\
\item
  \textbf{Categóricas ordinais} (baixo, médio, alto)\\
\item
  \textbf{Temporais} (datas, horários)\\
\item
  \textbf{Identificadores (IDs)} que não devem entrar como preditores
  numéricos
\end{itemize}

Identificadores numéricos (ex.: matrícula, CPF, código do lote)
\textbf{não são variáveis quantitativas} e não devem ser tratados como
tal na regressão.

\textbf{Ajustes típicos}

\begin{itemize}
\tightlist
\item
  Converter strings numéricas para número de forma explícita.
\item
  Converter datas para classe apropriada e, quando necessário, extrair
  componentes (ano, mês).
\item
  Transformar variáveis 0/1 em fator quando a interpretação for
  categórica.
\item
  Definir fatores com níveis claros e ordenados quando houver estrutura
  ordinal.
\item
  Padronizar rótulos inconsistentes (ex.: \texttt{M},
  \texttt{Masculino}, \texttt{male} → um único padrão).
\end{itemize}

Após os ajustes, reavalie a estrutura da base e confirme que cada coluna
está no tipo esperado antes de avançar para o tratamento de faltantes ou
criação de dummies.

\section{Tratamento de dados faltantes
(missing)}\label{tratamento-de-dados-faltantes-missing}

Dados faltantes alteram o tamanho efetivo da amostra, modificam a
estrutura da matriz de projeto \(\mathbf{X}\) e podem introduzir viés
nas estimativas quando o mecanismo de ausência não é completamente
aleatório (MCAR). Em regressão, a presença de faltantes pode funcionar
como um \textbf{mecanismo implícito de seleção amostral}, afetando tanto
a estimação quanto a interpretação dos coeficientes.

Excluir observações com valores ausentes equivale, muitas vezes, a
analisar uma subamostra potencialmente não representativa.

\textbf{Erros clássicos}

\begin{itemize}
\tightlist
\item
  Imputar zero indiscriminadamente (por exemplo, via
  \texttt{replace\_na}) sem considerar o significado substantivo.
\item
  Realizar junções (\emph{joins}) entre tabelas sem validar chaves,
  criando duplicações artificiais.
\item
  Remover colunas inteiras apenas por conterem \texttt{NA}, sem avaliar
  sua relevância analítica.
\item
  Imputar a variável resposta sem justificativa metodológica.
\end{itemize}

\textbf{Sugestões da literatura}

Segundo Little e Rubin (2019), o tratamento adequado depende do
mecanismo de ausência:

\begin{itemize}
\tightlist
\item
  \textbf{MCAR (Missing Completely At Random)}: ausência independente de
  variáveis observadas e não observadas.
\item
  \textbf{MAR (Missing At Random)}: ausência depende apenas de variáveis
  observadas.
\item
  \textbf{MNAR (Missing Not At Random)}: ausência depende de informação
  não observada.
\end{itemize}

Em situações simples e com baixa proporção de faltantes, imputações por
média ou mediana podem ser aceitáveis como aproximação. Em contextos
mais complexos, recomenda-se imputação múltipla ou métodos baseados em
modelos, preservando a incerteza associada ao processo.

\textbf{Como minimizar problemas}

\begin{itemize}
\tightlist
\item
  Mapear sistematicamente os valores ausentes com \texttt{is.na()} e
  quantificar proporções por variável.
\item
  Identificar códigos artificiais de ausência (\texttt{999},
  \texttt{-1}, \texttt{""}) e recodificá-los para \texttt{NA}.
\item
  Avaliar a importância substantiva da variável antes de decidir pela
  exclusão.
\item
  Documentar cada remoção ou imputação realizada.
\item
  Para duplicatas associadas a junções, utilizar verificações de chave e
  funções como \texttt{duplicated()} ou \texttt{distinct()}.
\end{itemize}

A decisão deve ser estatística \textbf{e} substantiva.

\textbf{Boas práticas}

\begin{itemize}
\tightlist
\item
  Manter um log explícito das decisões tomadas.
\item
  Comparar estatísticas descritivas antes e depois da imputação.
\item
  Evitar alterar a distribuição da variável de forma não justificada.
\item
  Não imputar automaticamente a variável resposta nesta etapa, salvo sob
  estratégia metodológica claramente definida.
\end{itemize}

\textbf{Procedimento recomendado}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calcular a taxa de faltantes por coluna.
\item
  Identificar padrões estruturais de ausência.
\item
  Classificar o possível mecanismo (MCAR/MAR/MNAR).
\item
  Definir estratégia:

  \begin{itemize}
  \tightlist
  \item
    excluir observações,\\
  \item
    imputar (média, mediana, por grupo ou múltipla),\\
  \item
    manter e modelar posteriormente o mecanismo de ausência.
  \end{itemize}
\item
  Registrar todas as decisões no log de tratamento.
\end{enumerate}

Tratamento de dados faltantes não é apenas limpeza --- é uma decisão
inferencial que pode alterar os resultados do modelo.

\section{Detecção de valores extremos e
inconsistências}\label{detecuxe7uxe3o-de-valores-extremos-e-inconsistuxeancias}

Valores extremos (outliers) podem ser resultado de erro de digitação,
inconsistência de unidade ou representar fenômenos reais raros. Em
regressão, esses pontos podem influenciar de forma desproporcional os
estimadores \(\widehat{\boldsymbol{\beta}}\), os resíduos e os
diagnósticos de ajuste.

Nem todo valor extremo é um erro, mas todo valor extremo exige
investigação.

\textbf{Erros clássicos}

\begin{itemize}
\tightlist
\item
  Remover automaticamente qualquer ponto extremo sem verificar sua
  origem.
\item
  Estabelecer cortes arbitrários sem registrar critérios e
  justificativas.
\item
  Transformar a variável resposta sem considerar a nova interpretação
  dos coeficientes.
\item
  Ignorar a possibilidade de que o ponto seja informativo para o
  fenômeno estudado.
\end{itemize}

Segundo Charnet et al. (2008) e Sheather (2009), valores extremos podem
distorcer estimativas, ampliar variâncias e comprometer testes de
hipóteses.

Barnett e Lewis (1994) oferecem fundamentos formais para detecção de
outliers em análises univariadas e multivariadas, distinguindo entre:

\begin{itemize}
\tightlist
\item
  Observações extremas na distribuição marginal;
\item
  Pontos de alta alavancagem (leverage);
\item
  Observações influentes (que alteram substancialmente o ajuste do
  modelo).
\end{itemize}

Embora diagnósticos formais de influência sejam discutidos em outros
capítulos, a identificação preliminar já deve ocorrer nesta etapa.

\textbf{Ferramentas R}

Para inspeção inicial:

\begin{itemize}
\tightlist
\item
  \texttt{quantile()} --- limites interquartílicos\\
\item
  \texttt{sd()} e \texttt{scale()} --- padronização e z-score\\
\item
  \texttt{summary()} --- inspeção geral
\end{itemize}

Visualizações:

\begin{itemize}
\tightlist
\item
  \texttt{ggplot2::geom\_boxplot()}\strut \\
\item
  \texttt{ggplot2::geom\_histogram()}\strut \\
\item
  \texttt{ggplot2::geom\_point()}
\end{itemize}

A visualização frequentemente revela padrões que estatísticas isoladas
não mostram.

\textbf{Como resolver ou minimizar efeitos}

\begin{itemize}
\tightlist
\item
  Confirmar se o valor resulta de erro de digitação ou unidade (ex.:
  centímetros vs metros).
\item
  Corrigir inconsistências quando verificadas documentalmente.
\item
  Aplicar winsorização (\texttt{DescTools::Winsorize}) quando a
  estratégia for limitar extremos mantendo observações.
\item
  Utilizar transformações (log, raiz quadrada) quando houver forte
  assimetria.
\item
  Manter a observação quando for substantivamente plausível e documentar
  a decisão.
\end{itemize}

A remoção deve ser exceção, não regra.

\textbf{Observação importante}

Eliminar valores extremos altera a distribuição da variável, o tamanho
da amostra e potencialmente a matriz \(\mathbf{X}\). Toda decisão deve
ser registrada no log de tratamento.

\section{Padronização e transformações
numéricas}\label{padronizauxe7uxe3o-e-transformauxe7uxf5es-numuxe9ricas}

Padronizar variáveis numéricas torna os preditores comparáveis em escala
e pode melhorar a estabilidade numérica de procedimentos de estimação.
Transformações adequadas, por sua vez, reduzem assimetria, estabilizam
variância e facilitam interpretações coerentes com o fenômeno estudado.

Em regressão linear clássica, a padronização não altera o ajuste global
do modelo nem o \(R^2\), mas modifica a escala dos coeficientes e sua
interpretação. Já em métodos penalizados (Ridge e LASSO), a padronização
é praticamente indispensável.

Charnet et al. (2008) ressaltam que variáveis em escalas muito distintas
podem gerar instabilidade numérica e dificultar a comparação entre
efeitos.

Sheather (2009) destaca que reescalar variáveis pode facilitar a
interpretação de coeficientes em regressões múltiplas, especialmente
quando as unidades originais são muito grandes ou muito pequenas.

Montgomery, Peck, e Vining (2021) enfatizam que diferenças extremas de
magnitude entre covariáveis podem afetar diagnósticos e procedimentos
computacionais.

\textbf{Ferramentas R}

\begin{itemize}
\tightlist
\item
  \texttt{scale()} --- padronização pelo z-score (média zero e desvio
  padrão um).
\item
  Reescala min--max --- pode ser feita manualmente via transformações
  aritméticas.
\item
  \texttt{MASS::boxcox()} --- identificação de transformações do tipo
  Box--Cox.
\item
  \texttt{log()} ou \texttt{log1p()} --- transformações logarítmicas
  (úteis para assimetria positiva).
\item
  Transformações via \texttt{dplyr::mutate()} para aplicação
  sistemática.
\end{itemize}

A escolha da transformação deve ser guiada pelo comportamento empírico
da variável e pelo contexto substantivo.

\textbf{Soluções recomendadas na literatura}

\begin{itemize}
\tightlist
\item
  Para variáveis altamente assimétricas, aplicar transformações
  logarítmicas pode aproximar a normalidade e reduzir
  heterocedasticidade.
\item
  Para contagens moderadas, Paula (2004) sugere transformação por raiz
  quadrada como alternativa simples.
\item
  Para distribuições fortemente assimétricas, considerar Box--Cox quando
  apropriado.
\item
  Evitar misturar variáveis em escalas radicalmente diferentes sem
  padronização prévia.
\end{itemize}

\textbf{Quando padronizar}

\begin{itemize}
\tightlist
\item
  Para comparar magnitudes relativas entre preditores.
\item
  Para métodos penalizados (Ridge, LASSO), em que a penalização depende
  da escala.
\item
  Para algoritmos baseados em distância ou otimização numérica.
\item
  Quando unidades originais dificultam interpretação direta.
\end{itemize}

Padronizar não é obrigação universal; é uma decisão metodológica que
deve preservar a interpretabilidade do modelo.

\section{Codificação de variáveis categóricas
(dummies)}\label{codificauxe7uxe3o-de-variuxe1veis-categuxf3ricas-dummies}

Uma variável categórica com \(k\) níveis não pode entrar diretamente
como coluna numérica em \(\mathbf{X}\). É necessário convertê-la em
variáveis indicadoras (dummies), usualmente em número \(k-1\), para
evitar colinearidade perfeita.

A escolha da codificação determina a interpretação dos coeficientes
estimados.

\textbf{Erros comuns}

\begin{itemize}
\tightlist
\item
  Criar \(k\) dummies para \(k\) categorias (armadilha da variável
  dummy), gerando singularidade em \(\mathbf{X}^\top \mathbf{X}\).
\item
  Escolher categoria de referência sem critério substantivo.
\item
  Manter níveis raros, produzindo colunas quase vazias e estimativas
  instáveis.
\item
  Usar rótulos inconsistentes (acentos, abreviações,
  maiúsculas/minúsculas misturadas).
\item
  Tratar variável ordinal como nominal sem refletir sobre a estrutura de
  ordem.
\end{itemize}

\textbf{Conexão com a modelagem}

Quando uma variável categórica é convertida corretamente em \(k-1\)
dummies, cada coeficiente estimado representa a diferença média entre
aquela categoria e a categoria de referência, mantendo os demais
preditores constantes.

Se todas as \(k\) dummies forem incluídas juntamente com o intercepto,
ocorre dependência linear exata, tornando \(\mathbf{X}^\top \mathbf{X}\)
não invertível no caso clássico de MRLM.

Portanto, a codificação correta não é apenas conveniência computacional,
é condição para a existência do estimador de mínimos quadrados.

\textbf{Fundamentação teórica}

Charnet et al. (2008) destacam a importância da escolha da categoria de
referência para interpretação dos coeficientes.

James et al. (2013) discutem como alta cardinalidade pode gerar modelos
instáveis e sobreajustados.

Para variáveis ordinais, a estrutura de ordenação pode ser incorporada
explicitamente, evitando perda de informação.

\textbf{Ferramentas R}

\begin{itemize}
\tightlist
\item
  \texttt{model.matrix()} --- gera automaticamente a matriz de projeto
  com codificação apropriada.
\item
  \texttt{fastDummies} --- criação explícita de variáveis indicadoras.
\item
  \texttt{recipes::step\_dummy()} --- codificação sistemática em
  pipelines.
\item
  \texttt{forcats} --- manipulação e reorganização de níveis.
\item
  Fatores ordenados (\texttt{ordered}) para variáveis com hierarquia
  natural.
\end{itemize}

O R, por padrão, utiliza codificação por tratamento (\emph{treatment
contrasts}), mas outras codificações podem ser especificadas conforme
necessidade.

\textbf{Boas práticas}

\begin{itemize}
\tightlist
\item
  Definir categoria de referência com base em critério substantivo
  (grupo controle, baseline, padrão).
\item
  Agrupar níveis raros quando apropriado.
\item
  Manter um dicionário de variáveis documentando níveis e significados.
\item
  Padronizar rótulos antes da geração de dummies.
\item
  Verificar o número final de parâmetros gerados após codificação.
\end{itemize}

\textbf{Atenção à alta cardinalidade}

Variáveis com muitos níveis distintos podem gerar dezenas ou centenas de
colunas em \(\mathbf{X}\), aumentando dimensionalidade e risco de
sobreajuste.

Nesses casos, considere:

\begin{itemize}
\tightlist
\item
  Agrupamento por regras de negócio;
\item
  Seleção de variáveis;
\item
  Métodos penalizados (quando apropriado na etapa seguinte).
\end{itemize}

Codificar corretamente é garantir que a estrutura qualitativa do
fenômeno seja traduzida adequadamente para o modelo quantitativo.

\section{Verificação de condições para modelagem
linear}\label{verificauxe7uxe3o-de-condiuxe7uxf5es-para-modelagem-linear}

Cumprir condições mínimas como variabilidade das covariáveis, ausência
de colinearidade perfeita e tamanho amostral adequado (\(n > p + 1\)) é
o que permite aplicar os resultados teóricos da regressão linear e
múltipla com segurança.

No modelo linear clássico, a existência do estimador de mínimos
quadrados depende de \(\mathbf{X}^\top \mathbf{X}\) ser invertível, o
que exige que a matriz de projeto \(\mathbf{X}\) tenha posto completo.
Assim, esta verificação não é opcional --- é estrutural. (Searle (2016))

Essas checagens antecedem os diagnósticos formais e reduzem problemas
posteriores na estimação e interpretação.

\textbf{Ferramentas R úteis}

\begin{itemize}
\tightlist
\item
  \texttt{cor()} ou pacote \texttt{corrr} --- inspeção de associações
  entre preditores.
\item
  \texttt{qr()} ou \texttt{Matrix::rankMatrix()} --- verificação do
  posto da matriz de projeto.
\item
  \texttt{car::vif()} --- cálculo do fator de inflação da variância
  (VIF). (Fox e Weisberg (2024))
\item
  \texttt{summary()} e inspeção de variância --- identificação de
  variáveis constantes.
\end{itemize}

Essas ferramentas ajudam a identificar:

\begin{itemize}
\tightlist
\item
  Colinearidade perfeita ou quase perfeita;
\item
  Variáveis constantes ou quase constantes;
\item
  Relações lineares redundantes entre preditores.
\end{itemize}

\textbf{Como mitigar problemas}

\begin{itemize}
\tightlist
\item
  Remover ou combinar variáveis altamente correlacionadas.
\item
  Revisar a codificação de dummies para evitar dependência linear.
\item
  Padronizar variáveis quando necessário.
\item
  Eliminar duplicatas estruturais na base.
\item
  Reduzir dimensionalidade quando \(p\) se aproxima de \(n\).
\end{itemize}

Toda verificação deve ser documentada no relatório de tratamento.

\textbf{Compatibilidade da variável resposta}

Antes da modelagem, é essencial verificar se o tipo da variável resposta
é compatível com o modelo pretendido:

\begin{itemize}
\tightlist
\item
  \textbf{Contínua}: variável em escala intervalar ou razão.
\item
  \textbf{Binária}: 0/1 ou fator com dois níveis.
\item
  \textbf{Contagem}: inteiros não negativos.
\item
  \textbf{Proporção}: valores no intervalo \([0,1]\).
\end{itemize}

Escolher modelo inadequado ao tipo de resposta gera inferências
inválidas.

\textbf{Tabela-guia: tipo de resposta e cuidados}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Tipo de resposta
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Exemplo
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Tratamento pré-modelo
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Modelo típico
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Contínua & Preço, temperatura & Verificar outliers, padronização e
unidades & MRLS, MRLM \\
Binária & Sucesso/fracasso & Conferir codificação 0/1, balanceamento e
faltantes & Logístico, Probit \\
Contagem & Nº de eventos & Avaliar zeros estruturais e dispersão &
Poisson, Binomial Negativa \\
Proporção & Taxa, share & Verificar limites 0/1 e denominadores & Beta,
Quasi-binomial \\
\end{longtable}

Modelar sem verificar essas condições equivale a aplicar teoria sob
premissas não verificadas. O tratamento adequado garante que a transição
para a modelagem seja matemática e estatisticamente legítima.

\section{Sumários e visualizações
exploratórias}\label{sumuxe1rios-e-visualizauxe7uxf5es-exploratuxf3rias}

Explorar os dados antes da modelagem permite identificar padrões,
inconsistências e relações estruturais que orientam decisões de limpeza,
transformação e especificação do modelo.

A visualização não substitui a modelagem, ela antecipa problemas e
revela estruturas que podem afetar a construção de \(\mathbf{X}\) e a
escolha do modelo. (Tufte (2001))

\textbf{Ferramentas R}

Funções e pacotes úteis nesta etapa:

\begin{itemize}
\tightlist
\item
  \texttt{summary()} --- estatísticas descritivas básicas.
\item
  \texttt{table()} ou \texttt{dplyr::count()} --- frequências de
  variáveis categóricas.
\item
  \texttt{ggplot2} --- histogramas, boxplots, gráficos de dispersão.
\item
  \texttt{GGally::ggpairs()} --- matriz gráfica de dispersão.
\item
  \texttt{corrplot} ou \texttt{ggcorrplot} --- visualização de matrizes
  de correlação.
\item
  \texttt{plotly} --- visualizações interativas (opcional).
\end{itemize}

O objetivo é compreender estrutura, dispersão, assimetria e possíveis
relações lineares preliminares.

\textbf{Como resolver dificuldades comuns}

\begin{itemize}
\tightlist
\item
  Distribuições muito assimétricas → aplicar transformações (log, raiz
  quadrada) e reavaliar.
\item
  Categorias vazias ou raras → reclassificar níveis ou agrupar.
\item
  Escalas muito distintas → padronizar antes de comparar magnitudes.
\item
  Correlações elevadas entre preditores → revisar especificação do
  modelo.
\end{itemize}

Visualizar é diagnosticar antes do diagnóstico formal.

\textbf{Produtos esperados}

Ao final da etapa exploratória, espera-se:

\begin{itemize}
\tightlist
\item
  Tabela de estatísticas descritivas por variável numérica (média,
  desvio padrão, p5, p50, p95).
\item
  Frequência absoluta e relativa por variável categórica.
\item
  Histogramas e boxplots para avaliar distribuição.
\item
  Gráficos de dispersão entre \(Y\) e cada \(X\).
\item
  Matriz de correlação entre preditores numéricos.
\end{itemize}

Esses produtos funcionam como evidência documental do entendimento da
base antes da modelagem.

\textbf{Perguntas-guia}

\begin{itemize}
\tightlist
\item
  Alguma variável apresenta assimetria extrema?
\item
  Existem valores fora de faixa plausível?
\item
  Há categorias quase vazias?
\item
  Quais pares de \(X\) apresentam correlação elevada?
\item
  A relação entre \(Y\) e \(X\) parece aproximadamente linear?
\end{itemize}

Responder a essas perguntas reduz erros na etapa seguinte.

\textbf{Mapa de funções em R (resumo operacional)}

\begin{itemize}
\tightlist
\item
  \textbf{Leitura}: \texttt{read\_csv}, \texttt{read\_delim},
  \texttt{read\_excel}.
\item
  \textbf{Inspeção}: \texttt{glimpse}, \texttt{summary}, \texttt{count}.
\item
  \textbf{Transformação}: \texttt{mutate}, \texttt{across},
  \texttt{scale}.
\item
  \textbf{Tipagem}: \texttt{as.numeric}, \texttt{as.Date},
  \texttt{factor}.
\item
  \textbf{Correlação}: \texttt{cor}.
\item
  \textbf{Matriz de projeto}: \texttt{model.matrix}.
\item
  \textbf{Diagnóstico estrutural}: \texttt{qr}, \texttt{rankMatrix},
  \texttt{vif}.
\end{itemize}

A exploração sistemática consolida a transição entre tratamento de dados
e modelagem estatística.

\section{Salvamento e documentação da base
tratada}\label{salvamento-e-documentauxe7uxe3o-da-base-tratada}

A documentação do tratamento (log + dicionário) e o versionamento
adequado garantem reprodutibilidade, transparência metodológica e
facilitam revisão por pares.

Em regressão, a qualidade das inferências depende não apenas do modelo
ajustado, mas da rastreabilidade das decisões tomadas antes da
modelagem.

\textbf{O que entregar ao final do tratamento}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Base final tratada (formato padrão, por exemplo: CSV).
\item
  Dicionário de variáveis contendo:

  \begin{itemize}
  \tightlist
  \item
    Nome da variável
  \item
    Tipo
  \item
    Unidade (quando aplicável)
  \item
    Níveis (para categóricas)
  \item
    Origem ou transformação realizada
  \end{itemize}
\item
  Log de decisões documentando:

  \begin{itemize}
  \tightlist
  \item
    O que foi modificado
  \item
    Por que foi modificado
  \item
    Quando foi modificado
  \end{itemize}
\item
  Script reproduzível contendo todas as etapas do tratamento (opcional,
  mas altamente recomendado).
\end{enumerate}

O objetivo é que qualquer outro pesquisador consiga reconstruir
exatamente a base utilizada na modelagem.

\textbf{Convenções úteis}

\begin{itemize}
\tightlist
\item
  Utilizar nome de arquivo com carimbo de data, por exemplo:\\
  \texttt{base\_tratada\_2025-10-26.csv}
\item
  Manter script de preparo versionado e comentado.
\item
  Utilizar estrutura organizada de pastas:

  \begin{itemize}
  \tightlist
  \item
    \texttt{dados\_brutos/}
  \item
    \texttt{dados\_interinos/}
  \item
    \texttt{dados\_tratados/}
  \end{itemize}
\end{itemize}

Nunca sobrescrever dados brutos.

\textbf{Pipelines modernos e reprodutibilidade}

Em aplicações contemporâneas, o tratamento de dados é frequentemente
organizado em \textbf{pipelines}: sequências estruturadas de leitura →
transformação → validação → saída tratada → modelagem.

Esse fluxo reduz erros humanos, aumenta consistência e fortalece a
reprodutibilidade científica. No ecossistema R, essa abordagem é
facilitada por ferramentas como \texttt{tidyverse}, \texttt{recipes} e
estruturas de modelagem integradas.

Mesmo quando não formalizado em código automatizado, o processo deve ser
concebido como um fluxo explícito, sequencial e documentado.

Modelar é o passo visível; documentar é o passo que garante
credibilidade.

\section{Checklist técnico: Condições mínimas para seguir à
modelagem}\label{checklist-tuxe9cnico-condiuxe7uxf5es-muxednimas-para-seguir-uxe0-modelagem}

Este checklist consolida os critérios estruturais que devem ser
verificados ao final do tratamento de dados, antes do ajuste de qualquer
modelo de regressão.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Variabilidade das covariáveis:} nenhuma variável explicativa
  deve ser constante ou quase constante.
\item
  \textbf{Ausência de colinearidade perfeita:} a matriz \(\mathbf{X}\)
  deve ter posto completo; logo, \(\mathbf{X}^\top \mathbf{X}\) deve ser
  não singular.
\item
  \textbf{Tamanho amostral adequado:} \(n > p + 1\) no caso do modelo
  linear com intercepto; caso contrário, considere reduzir
  dimensionalidade ou ampliar a amostra.
\item
  \textbf{Escalas compatíveis:} quando necessário, variáveis reescaladas
  ou padronizadas para evitar instabilidade numérica.
\item
  \textbf{Tipos de dados conferidos:} variáveis numéricas, categóricas e
  temporais corretamente tipadas.
\item
  \textbf{Ausência de erros estruturais:} duplicatas removidas, chaves
  validadas e inconsistências corrigidas.
\item
  \textbf{Base salva e documentada:} versão final armazenada com data,
  autor e dicionário de variáveis.
\item
  \textbf{Pronta para modelagem:} a base pode ser utilizada diretamente
  em MRLS, MRLM, MLG ou métodos penalizados sem retrabalho estrutural.
\end{enumerate}

O não atendimento a qualquer desses critérios compromete a legitimidade
estatística do modelo. A qualidade de toda inferência estatística
depende dessa distinção, e ela começa antes do ajuste do primeiro
modelo.

\section{Bases para prática}\label{bases-para-pruxe1tica}

Ao finalizar o tratamento de uma base, tente \textbf{ajustar um modelo
simples apenas para validar tipos e dummies} (sem discutir resultados).
Se rodar sem erros e os sumários fizerem sentido, a base está pronta
para a próxima unidade.

\textbf{Escada de dificuldade das bases} - \textbf{Nível 1
(aquecimento):} Online Shoppers: tipagem + dummies + sumários.\\
- \textbf{Nível 2 (intermediário):} Ames/House Prices: faltantes
moderados + reescala + dicionário.\\
- \textbf{Nível 3 (avançado):} Student Failure (messy):
chaves/duplicatas + integração + plano de imputação.\\
- \textbf{Extra (discreta):} Bike Sharing: temporais + zeros estruturais
+ outliers climáticos.

Esta seção apresenta \textbf{bases públicas e didáticas} para exercícios
de tratamento pré‑modelagem. Cada item traz link de acesso, o que a base
representa e \textbf{situações‑problema} que motivam o tratamento. Ao
final de cada base há um bloco \textbf{Tarefas sugeridas} para orientar
o estudo.

\subsection{House Prices: Advanced Regression Techniques
(Kaggle)}\label{house-prices-advanced-regression-techniques-kaggle}

\begin{itemize}
\tightlist
\item
  \textbf{Tipo de resposta:} Contínua (\texttt{SalePrice}).
\item
  \textbf{Link:}
  \url{https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques}
\item
  \textbf{Sobre a base:} preços de casas em Ames (Iowa, EUA), com
  \textasciitilde79 variáveis numéricas e categóricas.
\item
  \textbf{Situação:}

  \begin{itemize}
  \tightlist
  \item
    Muitas colunas com valores ausentes (por ex.: \texttt{LotFrontage},
    \texttt{Alley}, \texttt{PoolQC}).
  \item
    Variáveis categóricas com codificação inconsistente e níveis raros.
  \item
    Outliers em preço e área; unidades e escalas heterogêneas.
  \end{itemize}
\item
  Excelente caso para um \textbf{pipeline completo} de limpeza (missing,
  tipagem, codificação, reescala) antes da regressão contínua.
\item
  \textbf{Tarefas sugeridas:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Mapear porcentagens de faltantes por coluna e decidir estratégia
    (excluir, imputar, manter).
  \item
    Padronizar nomes e unidades; verificar outliers em
    \texttt{SalePrice} e \texttt{GrLivArea}.
  \item
    Unificar níveis categóricos raros e definir dummies com categoria de
    referência.
  \item
    Salvar uma versão tratada e documentar as decisões.
  \end{enumerate}
\end{itemize}

\subsection{Ames Housing Dataset}\label{ames-housing-dataset}

\begin{itemize}
\tightlist
\item
  \textbf{Tipo de resposta:} Contínua (\texttt{SalePrice}).
\item
  \textbf{Link:}
  \url{https://github.com/data-doctors/kaggle-house-prices-advanced-regression-techniques}
\item
  \textbf{Sobre a base:} variação/derivação do problema de habitação de
  Ames, amplamente usada em cursos.
\item
  \textbf{Situação:}

  \begin{itemize}
  \tightlist
  \item
    Mistura de tipos (numéricos + categóricos), com valores ausentes e
    níveis raros.
  \item
    Recomendação frequente de transformação logarítmica da resposta.
  \item
    Outliers estruturais (ex.: casas muito acima da média).
  \end{itemize}
\item
  Reforça o \textbf{contraste de estratégias} de tratamento em relação à
  10.1.
\item
  \textbf{Tarefas sugeridas:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Comparar duas abordagens de tratamento de faltantes (ex.: imputação
    por mediana vs.~KNN) e registrar impactos em sumários.
  \item
    Testar padronização vs.~não padronização nas variáveis contínuas.
  \item
    Produzir um dicionário de variáveis claro.
  \end{enumerate}
\end{itemize}

\subsection{Online Shoppers Purchasing Intention
(UCI)}\label{online-shoppers-purchasing-intention-uci}

\begin{itemize}
\tightlist
\item
  \textbf{Tipo de resposta:} Binária (\texttt{Revenue}: sim/não).
\item
  \textbf{Link:}
  \url{https://archive.ics.uci.edu/ml/datasets/Online\%2BShoppers\%2BPurchasing\%2BIntention\%2BDataset}
\item
  \textbf{Sobre a base:} sessões de navegação em e‑commerce; objetivo é
  prever se a sessão termina em compra.
\item
  \textbf{Situação:}

  \begin{itemize}
  \tightlist
  \item
    Variáveis categóricas e temporais misturadas às numéricas;
    necessidade de codificação.
  \item
    Possível desbalanceamento da classe \texttt{Revenue}.
  \item
    Datas/temporais como texto exigindo normalização e extração de
    componentes.
  \end{itemize}
\item
  Bom \textbf{caso de tratamento moderado}, contrastando com bases mais
  ``sujas''.
\item
  \textbf{Tarefas sugeridas:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Verificar distribuição de \texttt{Revenue} (balanceamento).
  \item
    Definir dummies consistentes e padronizar escalas numéricas.
  \item
    Criar variáveis derivadas temporais (mês, dia da semana) de forma
    reproducível.
  \end{enumerate}
\end{itemize}

\subsection{Student Failure (Messy) Dataset
(Kaggle)}\label{student-failure-messy-dataset-kaggle}

\begin{itemize}
\tightlist
\item
  \textbf{Tipo de resposta:} Binária (\texttt{fail} = 1 se o aluno
  reprova/sai; 0 caso contrário).
\item
  \textbf{Link:}
  \url{https://www.kaggle.com/code/sashatarakanova/student-failure-modelling-with-a-messy-dataset}
\item
  \textbf{Sobre a base:} dados educacionais com múltiplas tabelas
  heterogêneas para prever reprovação.
\item
  \textbf{Situação:}

  \begin{itemize}
  \tightlist
  \item
    Muitos valores ausentes; tabelas com chaves não padronizadas;
    duplicatas.
  \item
    Categorias inconsistentes para o mesmo conceito (ex.: formas
    distintas de escrever ``curso'').
  \item
    Necessidade de unificação/integração de fontes (join/merge) com
    validação.
  \end{itemize}
\item
  Ótimo para treinar \textbf{integração e saneamento} antes de qualquer
  modelagem binária.
\item
  \textbf{Tarefas sugeridas:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Reconstruir uma chave única estável e eliminar duplicatas.
  \item
    Mapear e recodificar categorias equivalentes.
  \item
    Documentar um plano de imputação apropriado por variável.
  \end{enumerate}
\end{itemize}

\subsection{Bike Sharing Dataset (UCI)}\label{bike-sharing-dataset-uci}

\begin{itemize}
\tightlist
\item
  \textbf{Tipo de resposta:} Discreta (contagem de bicicletas alugadas
  por hora/dia).
\item
  \textbf{Link:}
  \url{https://archive.ics.uci.edu/dataset/275/bike\%2Bsharing\%2Bdataset}
\item
  \textbf{Sobre a base:} uso de bicicletas compartilhadas, com variáveis
  meteorológicas, feriados, sazonalidade e efeitos de hora do dia.
\item
  \textbf{Situação:}

  \begin{itemize}
  \tightlist
  \item
    Contagens com muitos zeros em horários de baixa demanda e picos em
    horários de pico; necessidade de identificar \textbf{zeros
    estruturais}.
  \item
    Variáveis temporais em formato de texto exigindo conversão e
    extração (hora, dia da semana, feriado).
  \item
    Possíveis outliers (eventos climáticos extremos) e variabilidade
    alta da resposta.
  \item
    Padronização de escalas e codificação consistente de
    feriados/sazonalidade.
  \item
    Prepara para o tratamento de \textbf{resposta discreta (contagem)},
    anterior à escolha de modelos como Poisson ou Binomial Negativa.
  \end{itemize}
\item
  \textbf{Tarefas sugeridas:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Normalizar as variáveis temporais e criar indicadores (feriado, fim
    de semana, hora do rush).
  \item
    Caracterizar zeros estruturais vs.~esparsidade aleatória e discutir
    implicações para a modelagem.
  \item
    Detectar outliers climáticos e decidir estratégia (transformação,
    winsorização ou justificativa de manutenção).
  \item
    Entregar uma versão tratada com dicionário e log de decisões.
  \end{enumerate}
\end{itemize}

\section{Glossário}\label{glossario}

\textbf{Encoding (codificação de caracteres)}\\
Como o computador guarda letras com acentos e símbolos. Exemplos:
\textbf{UTF‑8}, \textbf{latin‑1}. Se o encoding está errado, aparecem
``�'' ou letras quebradas.

\textbf{Delimitador / Separador}\\
Símbolo que separa colunas em arquivos de texto. Exemplos: vírgula
\texttt{,}, ponto e vírgula \texttt{;}, tab .

\textbf{Decimal (símbolo decimal)}\\
O símbolo que separa parte inteira e fracionária. Em pt‑BR, vírgula
(ex.: \texttt{3,14}); em en‑US, ponto (\texttt{3.14}).

\textbf{CSV, XLSX, SAV, DTA}\\
Formatos de planilha/tabela. \textbf{CSV}: texto simples; \textbf{XLSX}:
Excel; \textbf{SAV}: SPSS; \textbf{DTA}: Stata.

\textbf{Licença (de uso dos dados)}\\
Condições legais de uso/compartilhamento do dataset (ex.: CC‑BY). Leia
antes de usar.

\textbf{Faltante / Dados faltantes}\\
Informação ausente em uma célula. Pode aparecer como \textbf{NA},
\textbf{NaN}, vazio \texttt{""} ou códigos como \texttt{999}.

\textbf{MCAR, MAR, MNAR}\\
Tipos de mecanismo de ausência: \textbf{MCAR} (ausência completamente ao
acaso), \textbf{MAR} (ao acaso condicional a outras variáveis),
\textbf{MNAR} (não ao acaso; depende do próprio valor ausente).

\textbf{Duplicatas e chaves}\\
\textbf{Duplicata}: linha repetida. \textbf{Chave}: coluna (ou
combinação) que identifica exclusivamente cada linha (ex.: \texttt{id}).

\textbf{Log de duplicatas / Log de tratamento}\\
Registro simples do que foi removido/alterado e por quê. Ajuda na
reprodutibilidade.

\textbf{Outlier}\\
Valor muito fora do padrão do conjunto. Pode ser erro, evento raro ou
caso especial.

\textbf{Winsorização (winsorize)}\\
Técnica que \textbf{limita} valores extremos a um limite (ex.: truncar
no percentil 1\% e 99\%) para reduzir impacto de outliers.

\textbf{Reescala / Padronização (z‑score)}\\
Colocar variáveis em escala comparável. \textbf{z‑score}: subtrai a
média e divide pelo desvio‑padrão (fica média 0 e dp 1).
\textbf{min--max}: leva para {[}0,1{]} pela fórmula
\texttt{(x−min)/(max−min)}.

\textbf{Cardinalidade (de categorias)}\\
Número de níveis distintos de uma variável categórica. Alta
cardinalidade = muitos níveis.

\textbf{Padronizar rótulos}\\
Escrever categorias de forma consistente (ex.: tudo minúsculo, sem
acento, sem espaços extras), unificando sinônimos.

\textbf{Dummies (one‑hot)}\\
Transformar uma variável categórica em colunas 0/1 (uma coluna a menos
que o número de categorias, para evitar colinearidade perfeita).

\textbf{Variável ordinal}\\
Categorias que \textbf{têm ordem} (ex.: fundamental \textless{} médio
\textless{} superior).

\textbf{Zeros estruturais}\\
Zeros esperados por construção (ex.: aluguel de bicicletas à 03h pode
ser zero). Diferem de ``zeros por acaso''.

\textbf{Regularização (Ridge, LASSO)}\\
Técnicas que penalizam coeficientes para lidar com muitas variáveis e
reduzir sobreajuste; exigem atenção à \textbf{escala} dos preditores.

\textbf{JSON}\\
Formato de texto para dados estruturados em pares chave:valor (não será
foco aqui; usamos tabelas).

\textbf{Pipeline}\\
Sequência organizada de passos de tratamento: leitura → limpeza →
transformação → verificação → saída tratada.

\bookmarksetup{startatroot}

\chapter*{Referências}\label{referuxeancias}
\addcontentsline{toc}{chapter}{Referências}

\markboth{Referências}{Referências}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-abadir2005}
Abadir, Karim M., e Jan R. Magnus. 2005. \emph{Matrix Algebra}.
Cambridge: Cambridge University Press.

\bibitem[\citeproctext]{ref-akaike1974}
Akaike, Hirotugu. 1974. {``A new look at the statistical model
identification''}. \emph{IEEE Transactions on Automatic Control} 19 (6):
716--23.

\bibitem[\citeproctext]{ref-anderson2003}
Anderson, T. W. 2003. \emph{An Introduction to Multivariate Statistical
Analysis}. 3º ed. New York: Wiley.

\bibitem[\citeproctext]{ref-barnett1994}
Barnett, Vic, e Toby Lewis. 1994. \emph{Outliers in Statistical Data}.
3º ed. Chichester: Wiley.

\bibitem[\citeproctext]{ref-belsley1980}
Belsley, David A., Edwin Kuh, e Roy E. Welsch. 1980. \emph{Regression
Diagnostics: Identifying Influential Data and Sources of Collinearity}.
New York: John Wiley \& Sons.

\bibitem[\citeproctext]{ref-burnham2002}
Burnham, Kenneth P., e David R. Anderson. 2002. \emph{Model Selection
and Multimodel Inference: A Practical Information-Theoretic Approach}.
2º ed. New York: Springer.

\bibitem[\citeproctext]{ref-mice_cran}
Buuren, Stef van, e Karin Groothuis-Oudshoorn. 2024. \emph{mice:
Multivariate Imputation by Chained Equations}. CRAN.
\url{https://cran.r-project.org/package=mice}.

\bibitem[\citeproctext]{ref-casella2002}
Casella, George, e Roger L. Berger. 2002. \emph{Statistical Inference}.
2º ed. Pacific Grove: Duxbury.

\bibitem[\citeproctext]{ref-charnet2008}
Charnet, Reinaldo, Carlos Alberto Freire, Eliane M. R. Charnet, e Helio
Bonvino. 2008. \emph{Análise de Modelos de Regressão Linear com
Aplicações}. 2º ed. Campinas: EDUNICAMP.

\bibitem[\citeproctext]{ref-draper1998}
Draper, Norman R., e Harry Smith. 1998. \emph{Applied Regression
Analysis}. 3º ed. New York: John Wiley \& Sons.

\bibitem[\citeproctext]{ref-car_vif}
Fox, John, e Sanford Weisberg. 2024. \emph{vif: Variance Inflation
Factors}. car package documentation.
\url{https://search.r-project.org/CRAN/refmans/car/html/vif.html}.

\bibitem[\citeproctext]{ref-galton1886b}
Galton, Francis. 1886a. {``Family likeness in stature''}.
\emph{Proceedings of the Royal Society of London} 40: 42--72.

\bibitem[\citeproctext]{ref-galton1886a}
---------. 1886b. {``Regression towards mediocrity in hereditary
stature''}. \emph{Journal of the Anthropological Institute} 15: 246--63.

\bibitem[\citeproctext]{ref-gelman2014}
Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki
Vehtari, e Donald B. Rubin. 2014. \emph{Bayesian Data Analysis}. 3º ed.
CRC Press.

\bibitem[\citeproctext]{ref-giordano2013}
Giordano, Frank R., William P. Fox, e Steven B. Horton. 2013. \emph{A
First Course in Mathematical Modeling}. Cengage Learning.

\bibitem[\citeproctext]{ref-golub2013}
Golub, Gene H., e Charles F. Van Loan. 2013. \emph{Matrix Computations}.
4º ed. Baltimore: Johns Hopkins University Press.

\bibitem[\citeproctext]{ref-gujarati2006}
Gujarati, Damodar N. 2006. \emph{Econometria Básica}. 4º ed. Rio de
Janeiro: Elsevier Campus.

\bibitem[\citeproctext]{ref-harville1997}
Harville, David A. 1997. \emph{Matrix Algebra From a Statistician's
Perspective}. New York: Springer.

\bibitem[\citeproctext]{ref-harville2000}
---------. 2000. \emph{Matrix Algebra from a Statistician's
Perspective}. New York: Springer.

\bibitem[\citeproctext]{ref-hoffmann2006}
Hoffmann, Rodolfo. 2006. \emph{Análise de Regressão: Uma Introdução à
Econometria}. 2º ed. São Paulo: Hucitec.

\bibitem[\citeproctext]{ref-hoffmann2016}
---------. 2016. \emph{Análise de Regressão: Uma Introdução à
Econometria}. 5º ed. Portal de Livros Abertos da USP.
\url{https://doi.org/10.11606/9788592105709}.

\bibitem[\citeproctext]{ref-pointblank_pkg}
Iannone, Richard. 2024. \emph{pointblank: Data Validation}. R package
documentation. \url{https://rstudio.github.io/pointblank/}.

\bibitem[\citeproctext]{ref-james2013}
James, Gareth, Daniela Witten, Trevor Hastie, e Robert Tibshirani. 2013.
\emph{An Introduction to Statistical Learning with Applications in R}.
New York: Springer.

\bibitem[\citeproctext]{ref-recipes_impute}
Kuhn, Max et al. 2024. \emph{step\_impute\_mean: Imputation Steps}.
recipes package documentation.
\url{https://recipes.tidymodels.org/reference/step_impute_mean.html}.

\bibitem[\citeproctext]{ref-kutner2005}
Kutner, Michael H., Christopher J. Nachtsheim, John Neter, e William Li.
2005. \emph{Applied Linear Statistical Models}. 5º ed. New York:
McGraw-Hill.

\bibitem[\citeproctext]{ref-little2019}
Little, Roderick J. A., e Donald B. Rubin. 2019. \emph{Statistical
Analysis with Missing Data}. 3º ed. Hoboken: Wiley.

\bibitem[\citeproctext]{ref-meerschaert2013}
Meerschaert, Mark M. 2013. \emph{Mathematical Modeling}. 4º ed. Academic
Press.

\bibitem[\citeproctext]{ref-montgomery2021}
Montgomery, Douglas C., Elizabeth A. Peck, e G. Geoffrey Vining. 2021.
\emph{Introduction to Linear Regression Analysis}. 6º ed. Hoboken: John
Wiley \& Sons.

\bibitem[\citeproctext]{ref-paula2004}
Paula, Gilberto A. 2004. \emph{Modelos de Regressão com Apoio
Computacional}. São Paulo: IME-USP.

\bibitem[\citeproctext]{ref-pearson1903}
Pearson, Karl, e Alice Lee. 1903. {``On the laws of inheritance''}.
\emph{Biometrika} 2: 357--462.

\bibitem[\citeproctext]{ref-rencher2012}
Rencher, Alvin C., e William F. Christensen. 2012. \emph{Methods of
Multivariate Analysis}. 3º ed. Hoboken: Wiley.

\bibitem[\citeproctext]{ref-schwarz1978}
Schwarz, Gideon. 1978. {``Estimating the Dimension of a Model''}.
\emph{The Annals of Statistics} 6 (2): 461--64.

\bibitem[\citeproctext]{ref-searle2016}
Searle, Shayle R. 2016. \emph{Matrix Algebra Useful for Statistics}. 2º
ed. Hoboken: Wiley.

\bibitem[\citeproctext]{ref-sheather2009}
Sheather, Simon J. 2009. \emph{A Modern Approach to Regression with R}.
New York: Springer.

\bibitem[\citeproctext]{ref-tufte2001}
Tufte, Edward R. 2001. \emph{The Visual Display of Quantitative
Information}. 2º ed. Cheshire: Graphics Press.

\bibitem[\citeproctext]{ref-weisberg2005}
Weisberg, Sanford. 2005. \emph{Applied Linear Regression}. New York:
Wiley.

\bibitem[\citeproctext]{ref-tidyr_missing}
Wickham, Hadley et al. 2024. \emph{Missing values}. tidyr package
documentation.
\url{https://tidyr.tidyverse.org/articles/missing-values.html}.

\end{CSLReferences}




\end{document}
